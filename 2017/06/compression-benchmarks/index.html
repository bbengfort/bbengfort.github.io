<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Compression Benchmarks | Libelli</title>
<meta name=keywords content><meta name=description content="One of the projects I&rsquo;m currently working on is the ingestion of RSS feeds into a Mongo database. It&rsquo;s been running for the past year, and as of this post has collected 1,575,987 posts for 373 feeds after 8,126 jobs. This equates to about 585GB of raw data, and a firm requirement for compression in order to exchange data.
Recently, @ojedatony1616 downloaded the compressed zip file (53GB) onto a 1TB external hard disk and attempted to decompress it. After three days, he tried to cancel it and ended up restarting his computer because it wouldn&rsquo;t cancel. His approach was simply to double click the file on OS X, but that got me to thinking &ndash; it shouldn&rsquo;t have taken that long; why did it choke? Inspecting the export logs on the server, I noted that it took 137 minutes to compress the directory; shouldn&rsquo;t it take that long to decompress as well?"><meta name=author content="Benjamin Bengfort"><link rel=canonical href=https://bbengfort.github.io/2017/06/compression-benchmarks/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://bbengfort.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bbengfort.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://bbengfort.github.io/icon.png><link rel=apple-touch-icon href=https://bbengfort.github.io/apple-touch-icon-precomposed.png><link rel=mask-icon href=https://bbengfort.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://bbengfort.github.io/2017/06/compression-benchmarks/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-D3BE7EHHVP"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D3BE7EHHVP")}</script><meta property="og:title" content="Compression Benchmarks"><meta property="og:description" content="One of the projects I&rsquo;m currently working on is the ingestion of RSS feeds into a Mongo database. It&rsquo;s been running for the past year, and as of this post has collected 1,575,987 posts for 373 feeds after 8,126 jobs. This equates to about 585GB of raw data, and a firm requirement for compression in order to exchange data.
Recently, @ojedatony1616 downloaded the compressed zip file (53GB) onto a 1TB external hard disk and attempted to decompress it. After three days, he tried to cancel it and ended up restarting his computer because it wouldn&rsquo;t cancel. His approach was simply to double click the file on OS X, but that got me to thinking &ndash; it shouldn&rsquo;t have taken that long; why did it choke? Inspecting the export logs on the server, I noted that it took 137 minutes to compress the directory; shouldn&rsquo;t it take that long to decompress as well?"><meta property="og:type" content="article"><meta property="og:url" content="https://bbengfort.github.io/2017/06/compression-benchmarks/"><meta property="og:image" content="https://bbengfort.github.io/bear.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-06-07T10:45:35+00:00"><meta property="article:modified_time" content="2017-06-07T10:45:35+00:00"><meta property="og:site_name" content="Libelli"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bbengfort.github.io/bear.png"><meta name=twitter:title content="Compression Benchmarks"><meta name=twitter:description content="One of the projects I&rsquo;m currently working on is the ingestion of RSS feeds into a Mongo database. It&rsquo;s been running for the past year, and as of this post has collected 1,575,987 posts for 373 feeds after 8,126 jobs. This equates to about 585GB of raw data, and a firm requirement for compression in order to exchange data.
Recently, @ojedatony1616 downloaded the compressed zip file (53GB) onto a 1TB external hard disk and attempted to decompress it. After three days, he tried to cancel it and ended up restarting his computer because it wouldn&rsquo;t cancel. His approach was simply to double click the file on OS X, but that got me to thinking &ndash; it shouldn&rsquo;t have taken that long; why did it choke? Inspecting the export logs on the server, I noted that it took 137 minutes to compress the directory; shouldn&rsquo;t it take that long to decompress as well?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://bbengfort.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Compression Benchmarks","item":"https://bbengfort.github.io/2017/06/compression-benchmarks/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Compression Benchmarks","name":"Compression Benchmarks","description":"One of the projects I\u0026rsquo;m currently working on is the ingestion of RSS feeds into a Mongo database. It\u0026rsquo;s been running for the past year, and as of this post has collected 1,575,987 posts for 373 feeds after 8,126 jobs. This equates to about 585GB of raw data, and a firm requirement for compression in order to exchange data.\nRecently, @ojedatony1616 downloaded the compressed zip file (53GB) onto a 1TB external hard disk and attempted to decompress it. After three days, he tried to cancel it and ended up restarting his computer because it wouldn\u0026rsquo;t cancel. His approach was simply to double click the file on OS X, but that got me to thinking \u0026ndash; it shouldn\u0026rsquo;t have taken that long; why did it choke? Inspecting the export logs on the server, I noted that it took 137 minutes to compress the directory; shouldn\u0026rsquo;t it take that long to decompress as well?\n","keywords":[],"articleBody":"One of the projects I’m currently working on is the ingestion of RSS feeds into a Mongo database. It’s been running for the past year, and as of this post has collected 1,575,987 posts for 373 feeds after 8,126 jobs. This equates to about 585GB of raw data, and a firm requirement for compression in order to exchange data.\nRecently, @ojedatony1616 downloaded the compressed zip file (53GB) onto a 1TB external hard disk and attempted to decompress it. After three days, he tried to cancel it and ended up restarting his computer because it wouldn’t cancel. His approach was simply to double click the file on OS X, but that got me to thinking – it shouldn’t have taken that long; why did it choke? Inspecting the export logs on the server, I noted that it took 137 minutes to compress the directory; shouldn’t it take that long to decompress as well?\nA quick Google revealed A Quick Benchmark: Gzip vs. Bzip2 vs. LZMA, written in 2005 to explore the performance of Gzip, Bzip2, and LZMA. This post cited Gzip as having the largest final compression size, but the fastest compression speed. Being 12 years ago, however, I wanted to get more modern numbers for the compression of a directory of many intermediately sized files. Hopefully this will help us make better decisions about data management and compression in the future.\nIn particular, these observations explore the compression ratio and speed of Tar Gzip, Tar Bzip2, and Zip on directories containing many intermediate sized files from 1MB to 10MB.\nResults The following results were recorded on the following platform:\n2.8GHz Intel Core i7 Macbook Pro 16GB DDR3 Memory and 750GB Flash Storage Disk OS X El Capitan Version 10.11.6 bsdtar 2.8.3 - libarchive 2.8.3 Apple gzip 251 bzip2 version 1.0.6, 6-Sept-2010 Zip 3.0 (July 5th 2008), by Info-ZIP As always, performance measurements are determined by a number of factors, use these results as a guide rather than as strict truth!\nIn the first chart we explore the amount of time it takes to compress a large directory. There is linear relationship between the size of the directory and the amount of time it takes to compress it, which makes sense. BZip2 takes the longest, and Zip and GZip are comparable in terms of the overall amount of time.\nWe get a similar result for extraction time, though clearly extraction is much faster than compression. BZip2 is once again the slowest, but although Zip and GZip are still comparable at lower file sizes, GZip appears to be taking an advantage at the larger archives. We’ll have to explore this more with much larger archives.\nCompression to extraction times appear to have a nearly linear relationship. When plotted against each other, we can see that indeed the slope of Zip is slightly larger than that of GZip and in fact there will be a measurable difference for larger file sizes!\nThe above graph simply shows both the compression and extraction times and their relationship to each other.\nLooking at how much we’ve compressed, we can compute the compression ratio: plotting the size of the original data to the archive size. This is a log-log scale, and we can see that BZip2 creates smaller archives at the cost of the time performance hit. BZip2 appears to be parallel with GZip, but GZip appears to have a slightly larger slope than Zip, doing better at smaller archive sizes and may eventually do even better at much larger file sizes.\nAll compression algorithms of course reduce huge amounts of dataset space when reducing text, around 80% reductions for Zip and GZip and over 90% reduction for BZip2.\nBecause of this result, it’s clear that instead of compressing the entire directory, we should instead compress each individual file, extracting them only as necessary as we need to read them in.\nMethod The goal of this benchmark was to explore compression and extraction of a directory containing many small files (similar to the corpus dataset we are dealing with). The files in question are text, json, or html, which compress pretty well. Therefore I created a dataset generation script that used the lorem package to create random text files of various sizes (1MiB and 2MiB files to start).\nEach directory contained 8 subdirectories with n files in each directory, which determines the total size of the dataset. For example, the 64MiB dataset of 1MiB files contained 8 files per subdirectory. The benchmark script first walked the data directory to get an exact file size, then compressed it using the specified tool. It computed the archive size to get the percent compression, then extracted the file to a temporary directory. Both compression and extraction was timed.\nFor more details, please see the script used to generate test data sets and run benchmarks on Gist: zipbench.py.\nFor future work I’d like to build this up to much larger corpus sizes, but that will probably require AWS or some dedicated hardware other than my MacBook pro, and a lot more time!\n","wordCount":"841","inLanguage":"en","image":"https://bbengfort.github.io/bear.png","datePublished":"2017-06-07T10:45:35Z","dateModified":"2017-06-07T10:45:35Z","author":{"@type":"Person","name":"Benjamin Bengfort"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bbengfort.github.io/2017/06/compression-benchmarks/"},"publisher":{"@type":"Organization","name":"Libelli","logo":{"@type":"ImageObject","url":"https://bbengfort.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://bbengfort.github.io/ accesskey=h title="Libelli (Alt + H)"><img src=https://bbengfort.github.io/icon.png alt aria-label=logo height=35>Libelli</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://bbengfort.github.io/archive/ title=archive><span>archive</span></a></li><li><a href=https://bbengfort.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://bbengfort.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://bbengfort.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://bbengfort.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://bbengfort.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Compression Benchmarks</h1><div class=post-meta><span title='2017-06-07 10:45:35 +0000 UTC'>June 7, 2017</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;841 words&nbsp;·&nbsp;Benjamin Bengfort&nbsp;|&nbsp;<a href=https://github.com/bbengfort/bbengfort.github.io/tree/main/content/posts/2017-06-07-compression-benchmarks.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>One of the projects I&rsquo;m currently working on is the <a href=http://baleen.districtdatalabs.com/>ingestion of RSS feeds into a Mongo database</a>. It&rsquo;s been running for the past year, and as of this post has collected 1,575,987 posts for 373 feeds after 8,126 jobs. This equates to about 585GB of raw data, and a firm requirement for compression in order to exchange data.</p><p>Recently, <a href=https://github.com/ojedatony1616>@ojedatony1616</a> downloaded the compressed zip file (53GB) onto a 1TB external hard disk and attempted to decompress it. After three days, he tried to cancel it and ended up restarting his computer because it wouldn&rsquo;t cancel. His approach was simply to double click the file on OS X, but that got me to thinking &ndash; it shouldn&rsquo;t have taken that long; why did it choke? Inspecting the export logs on the server, I noted that it took 137 minutes to compress the directory; shouldn&rsquo;t it take that long to decompress as well?</p><p>A quick Google revealed <a href=https://tukaani.org/lzma/benchmarks.html>A Quick Benchmark: Gzip vs. Bzip2 vs. LZMA</a>, written in 2005 to explore the performance of Gzip, Bzip2, and LZMA. This post cited Gzip as having the largest final compression size, but the fastest compression speed. Being 12 years ago, however, I wanted to get more modern numbers for the compression of a <em>directory</em> of many intermediately sized files. Hopefully this will help us make better decisions about data management and compression in the future.</p><p>In particular, these observations explore the compression ratio and speed of <a href=https://en.wikipedia.org/wiki/Gzip>Tar Gzip</a>, <a href=https://en.wikipedia.org/wiki/Bzip2>Tar Bzip2</a>, and <a href=https://en.wikipedia.org/wiki/Zip_(file_format)>Zip</a> on directories containing many intermediate sized files from 1MB to 10MB.</p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>The following results were recorded on the following platform:</p><ul><li>2.8GHz Intel Core i7 Macbook Pro</li><li>16GB DDR3 Memory and 750GB Flash Storage Disk</li><li>OS X El Capitan Version 10.11.6</li><li>bsdtar 2.8.3 - libarchive 2.8.3</li><li>Apple gzip 251</li><li>bzip2 version 1.0.6, 6-Sept-2010</li><li>Zip 3.0 (July 5th 2008), by Info-ZIP</li></ul><p>As always, performance measurements are determined by a number of factors, use these results as a guide rather than as strict truth!</p><p><a href=/images/2017-06-07-compress-time.png><img loading=lazy src=/images/2017-06-07-compress-time.png alt="Compression Time by Original Size"></a></p><p>In the first chart we explore the amount of time it takes to compress a large directory. There is linear relationship between the size of the directory and the amount of time it takes to compress it, which makes sense. BZip2 takes the longest, and Zip and GZip are comparable in terms of the overall amount of time.</p><p><a href=/images/2017-06-07-extract-time.png><img loading=lazy src=/images/2017-06-07-extract-time.png alt="Extraction Time by Original Size"></a></p><p>We get a similar result for extraction time, though clearly extraction is much faster than compression. BZip2 is once again the slowest, but although Zip and GZip are still comparable at lower file sizes, GZip appears to be taking an advantage at the larger archives. We&rsquo;ll have to explore this more with much larger archives.</p><p><a href=/images/2017-06-07-compress-to-extract-ratio.png><img loading=lazy src=/images/2017-06-07-compress-to-extract-ratio.png alt="Compression to Extraction Time Ratio"></a></p><p>Compression to extraction times appear to have a nearly linear relationship. When plotted against each other, we can see that indeed the slope of Zip is slightly larger than that of GZip and in fact there will be a measurable difference for larger file sizes!</p><p><a href=/images/2017-06-07-compress-vs-extract.png><img loading=lazy src=/images/2017-06-07-compress-vs-extract.png alt="Compression vs. Extraction Time by Original Size"></a></p><p>The above graph simply shows both the compression and extraction times and their relationship to each other.</p><p><a href=/images/2017-06-07-compression-ratio.png><img loading=lazy src=/images/2017-06-07-compression-ratio.png alt="Compression Ratio"></a></p><p>Looking at how much we&rsquo;ve compressed, we can compute the compression ratio: plotting the size of the original data to the archive size. This is a log-log scale, and we can see that BZip2 creates smaller archives at the cost of the time performance hit. BZip2 appears to be parallel with GZip, but GZip appears to have a slightly larger slope than Zip, doing better at smaller archive sizes and may eventually do even better at much larger file sizes.</p><p><a href=/images/2017-06-07-reduction-percent.png><img loading=lazy src=/images/2017-06-07-reduction-percent.png alt="Percentage Reduction by Algorithm"></a></p><p>All compression algorithms of course reduce huge amounts of dataset space when reducing text, around 80% reductions for Zip and GZip and over 90% reduction for BZip2.</p><p>Because of this result, it&rsquo;s clear that instead of compressing the entire directory, we should instead compress each individual file, extracting them only as necessary as we need to read them in.</p><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><p>The goal of this benchmark was to explore compression and extraction of a directory containing many small files (similar to the corpus dataset we are dealing with). The files in question are text, json, or html, which compress pretty well. Therefore I created a dataset generation script that used the <a href=https://pypi.python.org/pypi/lorem>lorem</a> package to create random text files of various sizes (1MiB and 2MiB files to start).</p><p>Each directory contained 8 subdirectories with <code>n</code> files in each directory, which determines the total size of the dataset. For example, the 64MiB dataset of 1MiB files contained 8 files per subdirectory. The benchmark script first walked the data directory to get an exact file size, then compressed it using the specified tool. It computed the archive size to get the percent compression, then extracted the file to a temporary directory. Both compression and extraction was timed.</p><p>For more details, please see the script used to generate test data sets and run benchmarks on Gist: <a href=https://gist.github.com/bbengfort/9ca2821d66e2a0f1316f3986fbcef8e5>zipbench.py</a>.</p><p>For future work I&rsquo;d like to build this up to much larger corpus sizes, but that will probably require AWS or some dedicated hardware other than my MacBook pro, and a lot more time!</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://bbengfort.github.io/2017/06/append-json-results/><span class=title>« Prev</span><br><span>Appending Results to a File</span>
</a><a class=next href=https://bbengfort.github.io/2017/05/test-decorators/><span class=title>Next »</span><br><span>Decorating Nose Tests</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://bbengfort.github.io/>Libelli</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>