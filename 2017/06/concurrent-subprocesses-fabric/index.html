<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Concurrent Subprocesses and Fabric | Libelli</title><meta name=keywords content><meta name=description content="I&rsquo;ve ben using Fabric to concurrently start multiple processes on several machines. These processes have to run at the same time (since they are experimental processes and are interacting with each other) and shut down at more or less the same time so that I can collect results and immediately execute the next sample in the experiment. However, I was having a some difficulties directly using Fabric:
 Fabric can parallelize one task across multiple hosts accordint to roles."><meta name=author content="Benjamin Bengfort"><link rel=canonical href=https://bbengfort.github.io/2017/06/concurrent-subprocesses-fabric/><link crossorigin=anonymous href=/assets/css/stylesheet.min.d0c0348c2d0cff14148d0e347258519d8df2ce53ce5ac32c7bd9a549182cb8ae.css integrity="sha256-0MA0jC0M/xQUjQ40clhRnY3yzlPOWsMse9mlSRgsuK4=" rel="preload stylesheet" as=style><link rel=preload href=/icon.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://bbengfort.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bbengfort.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://bbengfort.github.io/icon.png><link rel=apple-touch-icon href=https://bbengfort.github.io/apple-touch-icon-precomposed.png><link rel=mask-icon href=https://bbengfort.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.79.0"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-8096804-11','auto');ga('send','pageview');}</script><meta property="og:title" content="Concurrent Subprocesses and Fabric"><meta property="og:description" content="I&rsquo;ve ben using Fabric to concurrently start multiple processes on several machines. These processes have to run at the same time (since they are experimental processes and are interacting with each other) and shut down at more or less the same time so that I can collect results and immediately execute the next sample in the experiment. However, I was having a some difficulties directly using Fabric:
 Fabric can parallelize one task across multiple hosts accordint to roles."><meta property="og:type" content="article"><meta property="og:url" content="https://bbengfort.github.io/2017/06/concurrent-subprocesses-fabric/"><meta property="og:image" content="https://bbengfort.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-06-14T15:56:24+00:00"><meta property="article:modified_time" content="2017-06-14T15:56:24+00:00"><meta property="og:site_name" content="Libelli"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bbengfort.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Concurrent Subprocesses and Fabric"><meta name=twitter:description content="I&rsquo;ve ben using Fabric to concurrently start multiple processes on several machines. These processes have to run at the same time (since they are experimental processes and are interacting with each other) and shut down at more or less the same time so that I can collect results and immediately execute the next sample in the experiment. However, I was having a some difficulties directly using Fabric:
 Fabric can parallelize one task across multiple hosts accordint to roles."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://bbengfort.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Concurrent Subprocesses and Fabric","item":"https://bbengfort.github.io/2017/06/concurrent-subprocesses-fabric/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Concurrent Subprocesses and Fabric","name":"Concurrent Subprocesses and Fabric","description":"I\u0026rsquo;ve ben using Fabric to concurrently start multiple processes on several machines. These processes have to run at the same time (since they are experimental processes and are interacting with each other) and shut down at more or less the same time so that I can collect results and immediately execute the next sample in the experiment. However, I was having a some difficulties directly using Fabric:\n Fabric can parallelize one task across multiple hosts accordint to roles.","keywords":[],"articleBody":"I’ve ben using Fabric to concurrently start multiple processes on several machines. These processes have to run at the same time (since they are experimental processes and are interacting with each other) and shut down at more or less the same time so that I can collect results and immediately execute the next sample in the experiment. However, I was having a some difficulties directly using Fabric:\n Fabric can parallelize one task across multiple hosts accordint to roles. Fabric can be hacked to run multiple tasks on multiple hosts by setting env.dedupe_hosts = False Fabric can only parallelize one type of task, not multiple types Fabric can’t handle large numbers of SSH connections  In this post we’ll explore my approach with Fabric and my current solution.\nFabric Consider the following problem: I want to run a Honu replica server on four different hosts. This is pretty easy using fabric as follows:\nfrom itertools import count from fabric.api import env, parallel, run # assign unique pids to servers counter = count(1,1) # Set the hosts environment env.hosts = ['user@hostA:22', 'user@hostB:22', 'user@hostC:22', 'user@hostD:22'] @parallel def serve(pid=None): pid = pid or next(counter) run(\"honu serve -i {}\".format(pid)) Note that this uses a global variable, counter to assign a unique id to each process (more on this later). What if I want to run four replica processes on four hosts? We can hack that as follows:\nfrom fabric.api import execute, settings def multiexecute(task, n, host, *args, **kwargs): \"\"\" Execute the task n times on the specified host. If the task is parallel then this will be parallel as well. All other args are passed to execute. \"\"\" # Do nothing if n is zero or less if n  1: return # Return one execution of the task with the given host if n == 1: return execute(task, host=host, *args, **kwargs) # Otherwise create a lists of hosts, don't dedupe them, and execute hosts = [host]*n with settings(dedupe_hosts=False): execute(task, hosts=hosts, *args, **kwargs) # Note the removal of the decorator def serve(pid=None): pid = pid or next(counter) run(\"honu serve -i {}\".format(pid)) @parallel def serveall(): multiexecute(serve, 4, env.host) Here, we create a multiexecute() function that temporarily sets dedupe_hosts=False using the settings context manager, then creates a host list that duplicates the original host n times, executing the task in parallel. By parallelizing the serveall task, each host is passed into the task once, then branched out 4 times by multiexecute.\nNow, what if I want to run 4 serve() and 4 work() tasks with different arguments to each in parallel? Well, here’s where things fall apart, it can’t be done. If we write:\n@parallel def serveall(): multiexecute(serve, 4, env.host) multiexecute(work, 4, env.host) Then the second multiexecute() will happen sequentially after the first multiexecute(). Unfortunately there seems to be no solution. Moreover, each of the additional tasks opens up a new SSH connection and many SSH connections quickly become untenable as you reach file descriptor limits in Python.\nConcurrent Subprocess Ok, so let’s step back - Fabric is great for one task to one host, let’s continue to use that to our advantage. What can we put on each host that will be able to spawn multiple processes of different types? My first thought was a custom script, but after a tiny bit of research I found a StackOverflow question: Python subprocess in parallel.\nThe long and short of this is that creating a list of subprocess.Popen objects allows them to run concurrently. By polling them to see if they’re done and using select to buffer IO across multiple processes, you can collect stdout on demand, managing the execution of multiple subprocesses.\nSo now the plan is:\n Fabric sends a list of commands per host to pproc pproc coordinates the execution of processes per host pproc sends Fabric serialized stdout Fabric quits when pproc exits  I’ve created a command line script called pproc.py that wraps this and takes any number of commands and their arguments (so long as they are surrounded by quotes) and executes the pproc functionality described above. Consider the following “child process”:\n#!/usr/bin/env python3 import os import sys import time import random import argparse def fprint(s): \"\"\" Performs a flush after print and prepends the pid. \"\"\" msg = \"proc {}: {}\".format(os.getpid(), s) print(msg) sys.stdout.flush() if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument(\"-l\", \"--limit\", type=int, default=5) args = parser.parse_args() for idx in range(5): worked = random.random() * args.limit time.sleep(worked) fprint(\"task {} lasted {:0.2f} seconds\".format(idx, worked)) This script is just simulating work by sleeping, but crucially, takes an argument on the command line. If we run proc as follows:\n$ pproc \"./child.py -l 5\" \"./child.py -l 6\" \"./child.py -l 4\" Then we get the following serialized output:\nproc 46145: task 0 lasted 2.68 seconds proc 46146: task 0 lasted 3.13 seconds proc 46145: task 1 lasted 0.95 seconds proc 46144: task 0 lasted 3.70 seconds proc 46144: task 1 lasted 0.15 seconds proc 46146: task 1 lasted 1.12 seconds proc 46145: task 2 lasted 2.90 seconds proc 46146: task 2 lasted 2.80 seconds proc 46144: task 2 lasted 3.67 seconds proc 46146: task 3 lasted 0.59 seconds proc 46144: task 3 lasted 2.30 seconds proc 46146: task 4 lasted 2.23 seconds proc 46145: task 3 lasted 4.65 seconds proc 46144: task 4 lasted 3.06 seconds proc 46145: task 4 lasted 4.05 seconds Sweet! Things are happening concurrently and we can specify any arbitrary commands with their arguments on the command line! Win! The complete listing of the pproc script is as follows:\n Experiments So what was this all for? Well, I’m running distributed systems experiments, and it’s very tricky to coordinate everything and get results. A datapoint for an experiment runs the entire system with a specific workload and a specific configuration for a fixed amount of time, then dumps the numbers to disk.\nProblem: For a single datapoint I need to concurrently startup 48 processes: 24 replicas and 24 workload generators on 4 machines. Each process requires a slightly different configuration. An experiment is composed of multiple data points, usually between 40-200 individual runs of samples that take approximately 45 - 480 seconds each.\nThe solutions I had proposed were as follows:\nSolution 1 (by hand): open up 48 terminals and type simultaneously into them using iTerm. Each configuration is handled by the environment of each terminal session. Experiments take about 4-5 hours using this method and is prone to user error.\nSolution 2 (ssh push): use fabric to parallelize the opening of 48 ssh sessions and run a command on the remote host. Experiment run times go down to about 1.5 hours, but each script has to be written by hand and am also noticing SSH failures for too many connections at the higher levels, it’s also pretty hacky.\nSolution 3 (amqp pull): write a daemon on all machines that listens to an amqp service (AWS SQS is $0.40 for 1M requests) and starts up processes on the local machine. This would solve the coordination issue and could even aggregate results, but would require extra coding and involve another process running on the machines.\nThe solution described in this post would hopefully modify Solution 2 (ssh push) to actually make it tenable.\n","wordCount":"1198","inLanguage":"en","datePublished":"2017-06-14T15:56:24Z","dateModified":"2017-06-14T15:56:24Z","author":{"@type":"Person","name":"Benjamin Bengfort"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bbengfort.github.io/2017/06/concurrent-subprocesses-fabric/"},"publisher":{"@type":"Organization","name":"Libelli","logo":{"@type":"ImageObject","url":"https://bbengfort.github.io/favicon.ico"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: #1d1e20;--entry: #2e2e33;--primary: rgba(255, 255, 255, 0.84);--secondary: rgba(255, 255, 255, 0.56);--tertiary: rgba(255, 255, 255, 0.16);--content: rgba(255, 255, 255, 0.74);--hljs-bg: #2e2e33;--code-bg: #37383e;--border: #333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://bbengfort.github.io accesskey=h title="Libelli (Alt + H)"><img src=/icon.png alt=logo aria-label=logo height=35>Libelli</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://bbengfort.github.io/archive/ title=archive><span>archive</span></a></li><li><a href=https://bbengfort.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://bbengfort.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://bbengfort.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Concurrent Subprocesses and Fabric</h1><div class=post-meta>June 14, 2017&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Benjamin Bengfort</div></header><div class=post-content><p>I&rsquo;ve ben using <a href=http://docs.fabfile.org/>Fabric</a> to concurrently start multiple processes on several machines. These processes have to run at the same time (since they are experimental processes and are interacting with each other) and shut down at more or less the same time so that I can collect results and immediately execute the next sample in the experiment. However, I was having a some difficulties directly using Fabric:</p><ol><li>Fabric can parallelize one task across multiple hosts accordint to roles.</li><li>Fabric can be hacked to run multiple tasks on multiple hosts by setting <code>env.dedupe_hosts = False</code></li><li>Fabric can only parallelize one type of task, not multiple types</li><li>Fabric can&rsquo;t handle large numbers of SSH connections</li></ol><p>In this post we&rsquo;ll explore my approach with Fabric and my current solution.</p><h2 id=fabric>Fabric<a hidden class=anchor aria-hidden=true href=#fabric>#</a></h2><p>Consider the following problem: I want to run a Honu replica server on four different hosts. This is pretty easy using fabric as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> itertools <span style=color:#f92672>import</span> count
<span style=color:#f92672>from</span> fabric.api <span style=color:#f92672>import</span> env, parallel, run

<span style=color:#75715e># assign unique pids to servers</span>
counter <span style=color:#f92672>=</span> count(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>)

<span style=color:#75715e># Set the hosts environment</span>
env<span style=color:#f92672>.</span>hosts <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;user@hostA:22&#39;</span>, <span style=color:#e6db74>&#39;user@hostB:22&#39;</span>, <span style=color:#e6db74>&#39;user@hostC:22&#39;</span>, <span style=color:#e6db74>&#39;user@hostD:22&#39;</span>]

<span style=color:#a6e22e>@parallel</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>serve</span>(pid<span style=color:#f92672>=</span>None):
    pid <span style=color:#f92672>=</span> pid <span style=color:#f92672>or</span> next(counter)
    run(<span style=color:#e6db74>&#34;honu serve -i {}&#34;</span><span style=color:#f92672>.</span>format(pid))
</code></pre></div><p>Note that this uses a global variable, <code>counter</code> to assign a unique id to each process (more on this later). What if I want to run four replica processes on four hosts? We can hack that as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> fabric.api <span style=color:#f92672>import</span> execute, settings


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>multiexecute</span>(task, n, host, <span style=color:#f92672>*</span>args, <span style=color:#f92672>**</span>kwargs):
    <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>    Execute the task n times on the specified host. If the task is parallel
</span><span style=color:#e6db74>    then this will be parallel as well. All other args are passed to execute.
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
    <span style=color:#75715e># Do nothing if n is zero or less</span>
    <span style=color:#66d9ef>if</span> n <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1</span>: <span style=color:#66d9ef>return</span>

    <span style=color:#75715e># Return one execution of the task with the given host</span>
    <span style=color:#66d9ef>if</span> n <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:
        <span style=color:#66d9ef>return</span> execute(task, host<span style=color:#f92672>=</span>host, <span style=color:#f92672>*</span>args, <span style=color:#f92672>**</span>kwargs)

    <span style=color:#75715e># Otherwise create a lists of hosts, don&#39;t dedupe them, and execute</span>
    hosts <span style=color:#f92672>=</span> [host]<span style=color:#f92672>*</span>n
    <span style=color:#66d9ef>with</span> settings(dedupe_hosts<span style=color:#f92672>=</span>False):
        execute(task, hosts<span style=color:#f92672>=</span>hosts, <span style=color:#f92672>*</span>args, <span style=color:#f92672>**</span>kwargs)


<span style=color:#75715e># Note the removal of the decorator</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>serve</span>(pid<span style=color:#f92672>=</span>None):
    pid <span style=color:#f92672>=</span> pid <span style=color:#f92672>or</span> next(counter)
    run(<span style=color:#e6db74>&#34;honu serve -i {}&#34;</span><span style=color:#f92672>.</span>format(pid))


<span style=color:#a6e22e>@parallel</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>serveall</span>():
    multiexecute(serve, <span style=color:#ae81ff>4</span>, env<span style=color:#f92672>.</span>host)
</code></pre></div><p>Here, we create a <code>multiexecute()</code> function that temporarily sets <code>dedupe_hosts=False</code> using the <code>settings</code> context manager, then creates a host list that duplicates the original host <code>n</code> times, executing the task in parallel. By parallelizing the <code>serveall</code> task, each host is passed into the task once, then branched out 4 times by multiexecute.</p><p>Now, what if I want to run 4 <code>serve()</code> and 4 <code>work()</code> tasks with different arguments to each in parallel? Well, here&rsquo;s where things fall apart, it can&rsquo;t be done. If we write:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a6e22e>@parallel</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>serveall</span>():
    multiexecute(serve, <span style=color:#ae81ff>4</span>, env<span style=color:#f92672>.</span>host)
    multiexecute(work, <span style=color:#ae81ff>4</span>, env<span style=color:#f92672>.</span>host)
</code></pre></div><p>Then the second <code>multiexecute()</code> will happen sequentially after the first <code>multiexecute()</code>. Unfortunately there seems to be no solution. Moreover, each of the additional tasks opens up a new SSH connection and many SSH connections quickly become untenable as you reach file descriptor limits in Python.</p><h2 id=concurrent-subprocess>Concurrent Subprocess<a hidden class=anchor aria-hidden=true href=#concurrent-subprocess>#</a></h2><p>Ok, so let&rsquo;s step back - Fabric is great for one task to one host, let&rsquo;s continue to use that to our advantage. What can we put on each host that will be able to spawn multiple processes of different types? My first thought was a custom script, but after a tiny bit of research I found a StackOverflow question: <a href=https://stackoverflow.com/questions/9743838/python-subprocess-in-parallel>Python subprocess in parallel</a>.</p><p>The long and short of this is that creating a list of <code>subprocess.Popen</code> objects allows them to run concurrently. By polling them to see if they&rsquo;re done and using <code>select</code> to buffer IO across multiple processes, you can collect stdout on demand, managing the execution of multiple subprocesses.</p><p>So now the plan is:</p><ol><li>Fabric sends a list of commands per host to pproc</li><li>pproc coordinates the execution of processes per host</li><li>pproc sends Fabric serialized stdout</li><li>Fabric quits when pproc exits</li></ol><p>I&rsquo;ve created a <a href=https://gist.github.com/bbengfort/6b66fceb73dff58edd21e49967c0a07f>command line script called pproc.py</a> that wraps this and takes any number of commands and their arguments (so long as they are surrounded by quotes) and executes the <code>pproc</code> functionality described above. Consider the following &ldquo;child process&rdquo;:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>#!/usr/bin/env python3</span>

<span style=color:#f92672>import</span> os
<span style=color:#f92672>import</span> sys
<span style=color:#f92672>import</span> time
<span style=color:#f92672>import</span> random
<span style=color:#f92672>import</span> argparse

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fprint</span>(s):
    <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>    Performs a flush after print and prepends the pid.
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
    msg <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;proc {}: {}&#34;</span><span style=color:#f92672>.</span>format(os<span style=color:#f92672>.</span>getpid(), s)
    <span style=color:#66d9ef>print</span>(msg)
    sys<span style=color:#f92672>.</span>stdout<span style=color:#f92672>.</span>flush()


<span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
    parser <span style=color:#f92672>=</span> argparse<span style=color:#f92672>.</span>ArgumentParser()
    parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#34;-l&#34;</span>, <span style=color:#e6db74>&#34;--limit&#34;</span>, type<span style=color:#f92672>=</span>int, default<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)
    args <span style=color:#f92672>=</span> parser<span style=color:#f92672>.</span>parse_args()

    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>5</span>):
        worked <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>random() <span style=color:#f92672>*</span> args<span style=color:#f92672>.</span>limit
        time<span style=color:#f92672>.</span>sleep(worked)
        fprint(<span style=color:#e6db74>&#34;task {} lasted {:0.2f} seconds&#34;</span><span style=color:#f92672>.</span>format(idx, worked))

</code></pre></div><p>This script is just simulating work by sleeping, but crucially, takes an argument on the command line. If we run <code>proc</code> as follows:</p><pre><code>$ pproc &quot;./child.py -l 5&quot; &quot;./child.py -l 6&quot; &quot;./child.py -l 4&quot;
</code></pre><p>Then we get the following serialized output:</p><pre><code>proc 46145: task 0 lasted 2.68 seconds
proc 46146: task 0 lasted 3.13 seconds
proc 46145: task 1 lasted 0.95 seconds
proc 46144: task 0 lasted 3.70 seconds
proc 46144: task 1 lasted 0.15 seconds
proc 46146: task 1 lasted 1.12 seconds
proc 46145: task 2 lasted 2.90 seconds
proc 46146: task 2 lasted 2.80 seconds
proc 46144: task 2 lasted 3.67 seconds
proc 46146: task 3 lasted 0.59 seconds
proc 46144: task 3 lasted 2.30 seconds
proc 46146: task 4 lasted 2.23 seconds
proc 46145: task 3 lasted 4.65 seconds
proc 46144: task 4 lasted 3.06 seconds
proc 46145: task 4 lasted 4.05 seconds
</code></pre><p>Sweet! Things are happening concurrently and we can specify any arbitrary commands with their arguments on the command line! Win! The complete listing of the pproc script is as follows:</p><script type=application/javascript src=https://gist.github.com/bbengfort/6b66fceb73dff58edd21e49967c0a07f.js></script><h2 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h2><p>So what was this all for? Well, I&rsquo;m running distributed systems experiments, and it&rsquo;s very tricky to coordinate everything and get results. A datapoint for an experiment runs the entire system with a specific workload and a specific configuration for a fixed amount of time, then dumps the numbers to disk.</p><p>Problem: For a single datapoint I need to concurrently startup 48 processes: 24 replicas and 24 workload generators on 4 machines. Each process requires a slightly different configuration. An experiment is composed of multiple data points, usually between 40-200 individual runs of samples that take approximately 45 - 480 seconds each.</p><p>The solutions I had proposed were as follows:</p><p><em>Solution 1 (by hand)</em>: open up 48 terminals and type simultaneously into them using iTerm. Each configuration is handled by the environment of each terminal session. Experiments take about 4-5 hours using this method and is prone to user error.</p><p><em>Solution 2 (ssh push)</em>: use fabric to parallelize the opening of 48 ssh sessions and run a command on the remote host. Experiment run times go down to about 1.5 hours, but each script has to be written by hand and am also noticing SSH failures for too many connections at the higher levels, it&rsquo;s also pretty hacky.</p><p><em>Solution 3 (amqp pull)</em>: write a daemon on all machines that listens to an amqp service (AWS SQS is $0.40 for 1M requests) and starts up processes on the local machine. This would solve the coordination issue and could even aggregate results, but would require extra coding and involve another process running on the machines.</p><p>The solution described in this post would hopefully modify Solution 2 (ssh push) to actually make it tenable.</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2021 <a href=https://bbengfort.github.io>Libelli</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu')
menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>