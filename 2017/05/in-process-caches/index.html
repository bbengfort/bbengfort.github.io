<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>In Process Cacheing | Libelli</title><meta name=keywords content><meta name=description content="I have had some recent discussions regarding cacheing to improve application performance that I wanted to share. Most of the time those conversations go something like this: “have you heard of Redis?” I&rsquo;m fascinated by the fact that an independent, distributed key-value store has won the market to this degree. However, as I&rsquo;ve pointed out in these conversations, cacheing is a hierarchy (heck, even the processor has varying levels of cacheing)."><meta name=author content="Benjamin Bengfort"><link rel=canonical href=https://bbengfort.github.io/2017/05/in-process-caches/><link crossorigin=anonymous href=/assets/css/stylesheet.min.d0c0348c2d0cff14148d0e347258519d8df2ce53ce5ac32c7bd9a549182cb8ae.css integrity="sha256-0MA0jC0M/xQUjQ40clhRnY3yzlPOWsMse9mlSRgsuK4=" rel="preload stylesheet" as=style><link rel=preload href=/icon.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://bbengfort.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bbengfort.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://bbengfort.github.io/icon.png><link rel=apple-touch-icon href=https://bbengfort.github.io/apple-touch-icon-precomposed.png><link rel=mask-icon href=https://bbengfort.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.79.0"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-8096804-11','auto');ga('send','pageview');}</script><meta property="og:title" content="In Process Cacheing"><meta property="og:description" content="I have had some recent discussions regarding cacheing to improve application performance that I wanted to share. Most of the time those conversations go something like this: “have you heard of Redis?” I&rsquo;m fascinated by the fact that an independent, distributed key-value store has won the market to this degree. However, as I&rsquo;ve pointed out in these conversations, cacheing is a hierarchy (heck, even the processor has varying levels of cacheing)."><meta property="og:type" content="article"><meta property="og:url" content="https://bbengfort.github.io/2017/05/in-process-caches/"><meta property="og:image" content="https://bbengfort.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-05-17T08:16:34+00:00"><meta property="article:modified_time" content="2017-05-17T08:16:34+00:00"><meta property="og:site_name" content="Libelli"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bbengfort.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="In Process Cacheing"><meta name=twitter:description content="I have had some recent discussions regarding cacheing to improve application performance that I wanted to share. Most of the time those conversations go something like this: “have you heard of Redis?” I&rsquo;m fascinated by the fact that an independent, distributed key-value store has won the market to this degree. However, as I&rsquo;ve pointed out in these conversations, cacheing is a hierarchy (heck, even the processor has varying levels of cacheing)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://bbengfort.github.io/posts/"},{"@type":"ListItem","position":3,"name":"In Process Cacheing","item":"https://bbengfort.github.io/2017/05/in-process-caches/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"In Process Cacheing","name":"In Process Cacheing","description":"I have had some recent discussions regarding cacheing to improve application performance that I wanted to share. Most of the time those conversations go something like this: “have you heard of Redis?” I\u0026rsquo;m fascinated by the fact that an independent, distributed key-value store has won the market to this degree. However, as I\u0026rsquo;ve pointed out in these conversations, cacheing is a hierarchy (heck, even the processor has varying levels of cacheing).","keywords":[],"articleBody":"I have had some recent discussions regarding cacheing to improve application performance that I wanted to share. Most of the time those conversations go something like this: “have you heard of Redis?” I’m fascinated by the fact that an independent, distributed key-value store has won the market to this degree. However, as I’ve pointed out in these conversations, cacheing is a hierarchy (heck, even the processor has varying levels of cacheing). Especially when considering micro-service architectures that require extremely low latency responses, cacheing should be a critical part of the design, not just a bolt-on after thought!\nSo here are the tools I consider when implementing cacheing, in a hierarchy from single process to distributed processes:\n\nIn a later post, I may review embedded, multi-threaded, or external multi-process cacheing. In this post, however, I’m focused on component based single thread cacheing. But before we discuss that let’s review why cacheing is important. A definition:\n Cacheing: storing a computed value in a quickly readable data structure (usually in memory) to reduce the amount of time to respond to API calls usually by minimizing the need for repeated computation.\n The idea is that computing a value takes a measurable amount of time, either from processor cycles or I/O from the disk or another data source. By storing the computed value, repeated calls with a similar input can benefit from fast lookups in memory. Let’s look at a simple example of this, a process called memoization:\nfrom functool import wraps def memoized(fget): attr_name = '_{0}'.format(fget.__name__) @wraps(fget) def fget_memoized(self): if not hasattr(self, attr_name): setattr(self, attr_name, fget(self)) return getattr(self, attr_name) return property(fget_memoized) This snippet of code is so common that it is seen in a utils module in almost every larger piece of software I write. The memoized function is a method decorator (for classes) that acts similarly to the @property decorator. When a class attribute is accessed, its value corresponds to the return value of the fget function. When used with fget_memoized, however, the fget function is called, stored on the object, and instead of calling fget repeatedly, the cached value is returned. For example:\nclass Thing(object): @memoized def prime(self): print(\"long running computation!\") return 31 The print statement will only occur once, on the first access to thing.prime. After that, all calls will return the value of thing._prime. To force a recomputation you can simply del thing._prime.\nThis is great, and extremely commonly used — but what if you want to cache the response by input or timeout the cache after a fixed period? The answer to the first is the lru_cache, which caches values, discarding the “least recently used” first. Therefore if you have a function that accepts an argument:\nfrom functools import lru_cache @lru_cache(maxsize=256) def fib(n): if n  2: return n return fib(n-1) + fib(n-2) Then the cache will store values for that argument until maxsize is reached, at which point values used least recently will be discarded. Note that it is best to use a maxsize value that is a power of 2 for best performance. You can also inspect the cache as follows:\n fib(31) 1346269  fib.cache_info() CacheInfo(hits=29, misses=32, maxsize=256, currsize=32) To expire a value after a specific amount of time, I recommend using an ExpiringDict as follows:\nfrom expiringdict import ExpiringDict cache = ExpiringDict(max_len=256, max_age_seconds=2) You can now get and put values into the cache:\nimport time cache[\"foo\"] = \"bar\" cache.get(\"foo\") # bar time.sleep(2) cache.get(\"foo\") # None On get, the ExpiringDict checks the number of seconds since the value was inserted into the dictionary. If it is longer than the max_age the value is deleted and None is returned. Note that the cache is only managed on access, therefore without a max_length, they can grow to infinite size if not cleaned up. One way to manage this is with a routine garbage collection thread that just performs a get on all values, locking the dictionary as it does.\nNeither of these types of caches are persistent. In order to persist a cache to disk, you can simply pickle the object to disk. However, a better option might be the Python shelve module.\nA “shelf” is a persistent, dictionary-like object that stores Python objects to disk. By itself, it is not a cache per-se, but with the writeback flag set to True, it can be used as a durable cache. In this case, entries are cached and accessed in memory, and only snapshotted to disk on sync and close.\nfrom shelve import shelve class DurableCache(object): def __init__(self, path): self.db = shelve.open(path, writeback=True) def put(self, key, val): self.db[key] = val def get(self, key): return self.db[key] def close(self): self.db.close() def sync(self): self.db.sync() With a little creativity, these caches can be extremely effective local durable storage. However note that the shelf does not know when an object has been mutated, which means it can consume a lot of memory or take a long time to sync or close. Advanced in-memory caches that use the shelve module add logic to detect these things and background routines to clean up and periodically checkpoint to disk for recovery.\nThere is still a long way to go with cacheing options, including embedded and in-memory databases as well as external caches for multi-process or distributed cacheing. I may discuss these in another post.\n","wordCount":"877","inLanguage":"en","datePublished":"2017-05-17T08:16:34Z","dateModified":"2017-05-17T08:16:34Z","author":{"@type":"Person","name":"Benjamin Bengfort"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bbengfort.github.io/2017/05/in-process-caches/"},"publisher":{"@type":"Organization","name":"Libelli","logo":{"@type":"ImageObject","url":"https://bbengfort.github.io/favicon.ico"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: #1d1e20;--entry: #2e2e33;--primary: rgba(255, 255, 255, 0.84);--secondary: rgba(255, 255, 255, 0.56);--tertiary: rgba(255, 255, 255, 0.16);--content: rgba(255, 255, 255, 0.74);--hljs-bg: #2e2e33;--code-bg: #37383e;--border: #333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://bbengfort.github.io accesskey=h title="Libelli (Alt + H)"><img src=/icon.png alt=logo aria-label=logo height=35>Libelli</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://bbengfort.github.io/archive/ title=archive><span>archive</span></a></li><li><a href=https://bbengfort.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://bbengfort.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://bbengfort.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>In Process Cacheing</h1><div class=post-meta>May 17, 2017&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Benjamin Bengfort</div></header><div class=post-content><p>I have had some recent discussions regarding cacheing to improve application performance that I wanted to share. Most of the time those conversations go something like this: “have you heard of Redis?” I&rsquo;m fascinated by the fact that an independent, distributed key-value store has won the market to this degree. However, as I&rsquo;ve pointed out in these conversations, cacheing is a hierarchy (heck, even the processor has varying levels of cacheing). Especially when considering micro-service architectures that require extremely low latency responses, cacheing should be a critical part of the design, not just a bolt-on after thought!</p><p>So here are the tools I consider when implementing cacheing, in a hierarchy from single process to distributed processes:</p><p><a href=/images/2017-05-17-cacheing-hierarchy.png><img loading=lazy src=/images/2017-05-17-cacheing-hierarchy.png alt="Cacheing Hierarchy"></a></p><p>In a later post, I may review embedded, multi-threaded, or external multi-process cacheing. In this post, however, I&rsquo;m focused on component based single thread cacheing. But before we discuss that let’s review why cacheing is important. A definition:</p><blockquote><p><strong>Cacheing</strong>: storing a computed value in a quickly readable data structure (usually in memory) to reduce the amount of time to respond to API calls usually by minimizing the need for repeated computation.</p></blockquote><p>The idea is that computing a value takes a measurable amount of time, either from processor cycles or I/O from the disk or another data source. By storing the computed value, repeated calls with a similar input can benefit from fast lookups in memory. Let&rsquo;s look at a simple example of this, a process called <a href=https://en.wikipedia.org/wiki/Memoization><em>memoization</em></a>:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> functool <span style=color:#f92672>import</span> wraps

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>memoized</span>(fget):
    attr_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;_{0}&#39;</span><span style=color:#f92672>.</span>format(fget<span style=color:#f92672>.</span>__name__)

    <span style=color:#a6e22e>@wraps</span>(fget)
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fget_memoized</span>(self):
        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> hasattr(self, attr_name):
            setattr(self, attr_name, fget(self))
        <span style=color:#66d9ef>return</span> getattr(self, attr_name)

    <span style=color:#66d9ef>return</span> property(fget_memoized)
</code></pre></div><p>This snippet of code is so common that it is seen in a <code>utils</code> module in almost every larger piece of software I write. The <code>memoized</code> function is a method decorator (for classes) that acts similarly to the <a href=https://docs.python.org/3/library/functions.html#property><code>@property</code></a> decorator. When a class attribute is accessed, its value corresponds to the return value of the <code>fget</code> function. When used with <code>fget_memoized</code>, however, the <code>fget</code> function is called, stored on the object, and instead of calling <code>fget</code> repeatedly, the cached value is returned. For example:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Thing</span>(object):

    <span style=color:#a6e22e>@memoized</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>prime</span>(self):
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;long running computation!&#34;</span>)
        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>31</span>
</code></pre></div><p>The print statement will only occur once, on the first access to <code>thing.prime</code>. After that, all calls will return the value of <code>thing._prime</code>. To force a recomputation you can simply <code>del thing._prime</code>.</p><p>This is great, and extremely commonly used — but what if you want to cache the response by input or timeout the cache after a fixed period? The answer to the first is the <a href=https://docs.python.org/3/library/functools.html#functools.lru_cache><code>lru_cache</code></a>, which caches values, discarding the <a href=https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_Recently_Used_.28LRU.29>&ldquo;least recently used&rdquo;</a> first. Therefore if you have a function that accepts an argument:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> functools <span style=color:#f92672>import</span> lru_cache

<span style=color:#a6e22e>@lru_cache</span>(maxsize<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>)
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fib</span>(n):
    <span style=color:#66d9ef>if</span> n <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>2</span>:
        <span style=color:#66d9ef>return</span> n
    <span style=color:#66d9ef>return</span> fib(n<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>+</span> fib(n<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>)
</code></pre></div><p>Then the cache will store values for that argument until <code>maxsize</code> is reached, at which point values used least recently will be discarded. <em>Note that it is best to use a <code>maxsize</code> value that is a power of 2 for best performance.</em> You can also inspect the cache as follows:</p><pre><code>&gt;&gt;&gt; fib(31)
1346269
&gt;&gt;&gt; fib.cache_info()
CacheInfo(hits=29, misses=32, maxsize=256, currsize=32)
</code></pre><p>To expire a value after a specific amount of time, I recommend using an <a href=https://pypi.python.org/pypi/expiringdict><code>ExpiringDict</code></a> as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> expiringdict <span style=color:#f92672>import</span> ExpiringDict
cache <span style=color:#f92672>=</span> ExpiringDict(max_len<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>, max_age_seconds<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</code></pre></div><p>You can now get and put values into the cache:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> time

cache[<span style=color:#e6db74>&#34;foo&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;bar&#34;</span>
cache<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;foo&#34;</span>) <span style=color:#75715e># bar</span>
time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>2</span>)
cache<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;foo&#34;</span>) <span style=color:#75715e># None</span>
</code></pre></div><p>On <code>get</code>, the <code>ExpiringDict</code> checks the number of seconds since the value was inserted into the dictionary. If it is longer than the <code>max_age</code> the value is deleted and None is returned. Note that the cache is only managed on access, therefore without a <code>max_length</code>, they can grow to infinite size if not cleaned up. One way to manage this is with a routine garbage collection thread that just performs a <code>get</code> on all values, locking the dictionary as it does.</p><p>Neither of these types of caches are persistent. In order to persist a cache to disk, you can simply <code>pickle</code> the object to disk. However, a better option might be the Python <a href=https://docs.python.org/3/library/shelve.html><code>shelve</code></a> module.</p><p>A &ldquo;shelf&rdquo; is a persistent, dictionary-like object that stores Python objects to disk. By itself, it is not a cache per-se, but with the <code>writeback</code> flag set to <code>True</code>, it can be used as a durable cache. In this case, entries are cached and accessed in memory, and only snapshotted to disk on <code>sync</code> and <code>close</code>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> shelve <span style=color:#f92672>import</span> shelve

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DurableCache</span>(object):

    <span style=color:#66d9ef>def</span> __init__(self, path):
        self<span style=color:#f92672>.</span>db <span style=color:#f92672>=</span> shelve<span style=color:#f92672>.</span>open(path, writeback<span style=color:#f92672>=</span>True)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>put</span>(self, key, val):
        self<span style=color:#f92672>.</span>db[key] <span style=color:#f92672>=</span> val

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get</span>(self, key):
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>db[key]

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>close</span>(self):
        self<span style=color:#f92672>.</span>db<span style=color:#f92672>.</span>close()

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sync</span>(self):
        self<span style=color:#f92672>.</span>db<span style=color:#f92672>.</span>sync()
</code></pre></div><p>With a little creativity, these caches can be extremely effective local durable storage. However note that the shelf does not know when an object has been mutated, which means it can consume a lot of memory or take a long time to sync or close. Advanced in-memory caches that use the shelve module add logic to detect these things and background routines to clean up and periodically checkpoint to disk for recovery.</p><p>There is still a long way to go with cacheing options, including embedded and in-memory databases as well as external caches for multi-process or distributed cacheing. I may discuss these in another post.</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2021 <a href=https://bbengfort.github.io>Libelli</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu')
menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>