<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Libelli</title>
    <link>https://bbengfort.github.io/posts/</link>
    <description>Recent content in Posts on Libelli</description>
    <image>
      <title>Libelli</title>
      <url>https://bbengfort.github.io/bear.png</url>
      <link>https://bbengfort.github.io/bear.png</link>
    </image>
    <generator>Hugo -- 0.135.0</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 11:35:00 -0700</lastBuildDate>
    <atom:link href="https://bbengfort.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Privacy and Security in the Age of Generative AI</title>
      <link>https://bbengfort.github.io/2024/10/privacy-security-generative-ai/</link>
      <pubDate>Wed, 30 Oct 2024 11:35:00 -0700</pubDate>
      <guid>https://bbengfort.github.io/2024/10/privacy-security-generative-ai/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://odsc.com/speakers/privacy-and-security-in-the-age-of-generative-ai/&#34;&gt;Privacy and Security in the Age of Generative AI&lt;/a&gt; is a talk that I gave at ODSC West 2024 in Burlingame, California. The slides of the talk are below:&lt;/p&gt;
&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/272867721?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;From sensitive data leakage to prompt injection and zero-click worms, LLMs and generative models are the new cyber battleground for hackers. As more AI models are deployed in production, data scientists and ML engineers can&amp;rsquo;t ignore these problems. The good news is that we can influence privacy and security in the machine learning lifecycle using data specific techniques. In this talk, we&amp;rsquo;ll review some of the newest security concerns affecting LLMs and deep learning models and learn how to embed privacy into model training with ACLs and differential privacy, secure text generation and function-calling interfaces, and even leverage models to defend other models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Smart Global Replication Using Reinforcement Learning</title>
      <link>https://bbengfort.github.io/2023/11/smart-global-replication-using-reinforcement-learning/</link>
      <pubDate>Tue, 07 Nov 2023 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2023/11/smart-global-replication-using-reinforcement-learning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://kccncna2023.sched.com/event/21d4640540a0961019d201de8ec2fd5e&#34;&gt;Smart Global Replication using Reinforcement Learning&lt;/a&gt; is a talk that I gave at KubeCon + CloudNative North America 2023 in Chicago, IL. The video of the talk is below:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/YTF2dXNhFzI?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;There are many great reasons to replicate data across Kubernetes clusters in different geographic regions: e.g. for disaster recovery and to ensure the best possible user experiences. Unfortunately, global replication is not easy; not just because of the difficulty in consistency reasoning that it introduces, but also due to the increased cost of provisioning multiple volumes that exponentially duplicate ingress and egress. Wouldn&amp;rsquo;t it be great if our systems could learn the optimal placement of storage blocks so that total replication was not necessary? Wouldn&amp;rsquo;t it be even better if our replication messaging was reduced ensuring communication only between the minimally necessary set of storage nodes? We show a system that uses multi-armed bandits to perform such an optimization; dynamically adjusting how data is replicated based on usage. We demonstrate the savings achieved and system performance using a real world system: the TRISA Global Travel Rule Compliance Directory.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DIY Consensus: Crafting Your Own Distributed Code (with Benjamin Bengfort)</title>
      <link>https://bbengfort.github.io/2023/08/diy-consensus-crafting-your-own-distributed-code/</link>
      <pubDate>Wed, 30 Aug 2023 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2023/08/diy-consensus-crafting-your-own-distributed-code/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pod.link/developer-voices/episode/938292a4e4b2c1ca82a66d4674dd8d97&#34;&gt;DIY Consensus: Crafting Your Own Distributed Code (with Benjamin Bengfort)&lt;/a&gt;&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/Ij_PBvocf5c?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;How do distributed systems work? If you&amp;rsquo;ve got a database spread over three servers, how do they elect a leader? How does that change when we spread those machines out across data centers, situated around the globe? Do we even need to understand how it works, or can we relegate those problems to an off the shelf tool like Zookeeper? Joining me this week is Distributed Systems Doctor—Benjamin Bengfort—for a deep dive into consensus algorithms. We start off by discussing how much of &amp;ldquo;the clustering problem&amp;rdquo; is your problem, and how much can be handled by a library. We go through many of the constraints and tradeoffs that you need to understand either way. And we eventually reach Benjamin&amp;rsquo;s surprising message - maybe the time is ripe to roll your own. Should we be writing our own bespoke Raft implementations? And if so, how hard would that be? What guidance can he offer us?  Somewhere in the recording of this episode, I decided I want to sit down and try to implement a leader election protocol. Maybe you will too. And if not, you&amp;rsquo;ll at least have a better appreciation for what it takes. Distributed systems used to be rocket science, but they&amp;rsquo;re becoming deployment as usual. This episode should help us all to keep up!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Faster Protocol Buffer Serialization</title>
      <link>https://bbengfort.github.io/2023/05/faster-protocol-buffer-serialization/</link>
      <pubDate>Wed, 03 May 2023 10:30:16 -0500</pubDate>
      <guid>https://bbengfort.github.io/2023/05/faster-protocol-buffer-serialization/</guid>
      <description>&lt;p&gt;Performance is key when building streaming gRPC services. When you&amp;rsquo;re trying to maximize throughput (e.g. messages per second) benchmarking is essential to understanding where the bottlenecks in your application are.&lt;/p&gt;
&lt;p&gt;However, as a start, you can pretty much guarantee that one bottleneck is going to be the serialization (marshaling) and deserialization (unmarshaling) of protocol buffer messages.&lt;/p&gt;
&lt;p&gt;We have a use case where the server does not need all of the information in the message in order to process the message. E.g. we have header information such as IDs and client information that the server does need to update as part of processing. The other part of the message is data that needs to be saved to disk and does not have to be unmarshaled until it&amp;rsquo;s read. However, our protocol buffer schema right now is &amp;ldquo;flat&amp;rdquo; — meaning that all fields whether they are required for processing or not are defined by a single protocol buffer message.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Atomic vs Mutex</title>
      <link>https://bbengfort.github.io/2022/11/atomic-vs-mutex/</link>
      <pubDate>Sat, 26 Nov 2022 12:24:25 -0600</pubDate>
      <guid>https://bbengfort.github.io/2022/11/atomic-vs-mutex/</guid>
      <description>&lt;p&gt;When implementing Go code, I find myself chasing increased concurrency performance by trying to reduce the number of locks in my code. Often I wonder if using the &lt;code&gt;sync/atomic&lt;/code&gt; package is a better choice because I know (as proved by this blog post) that atomics have far more performance than mutexes. The issue is that reading on the internet, including the &lt;a href=&#34;https://pkg.go.dev/sync/atomic&#34;&gt;package documentation&lt;/a&gt; itself strongly recommends relying on channels, then mutexes, and finally atomics &lt;em&gt;only if you know what you&amp;rsquo;re doing&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Nonlinear Workflow for Planning Software Projects</title>
      <link>https://bbengfort.github.io/2021/03/nonlinear-workflow-planning-software-projects/</link>
      <pubDate>Sun, 14 Mar 2021 09:53:49 -0400</pubDate>
      <guid>https://bbengfort.github.io/2021/03/nonlinear-workflow-planning-software-projects/</guid>
      <description>&lt;p&gt;Good software development achieves complexity by describing the interactions between simpler components. Although we tend to think of software processes as step-by-step &amp;ldquo;wizards&amp;rdquo;, design and decoupling of components often means that the interactions are non-linear. So why should our software project planning be defined in a linear progression of steps with time estimates? Can we plan projects using a non-linear workflow that mirrors how we think about component design?&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bbengfort.github.io/images/2021-03-14-blocks-dependencies.png&#34; alt=&#34;Blocks and Dependencies&#34;  /&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Go Closures &amp; Interfaces</title>
      <link>https://bbengfort.github.io/2021/02/go-closures-interfaces/</link>
      <pubDate>Tue, 23 Feb 2021 08:28:22 -0500</pubDate>
      <guid>https://bbengfort.github.io/2021/02/go-closures-interfaces/</guid>
      <description>&lt;p&gt;Strict typing in the Go programming language provides safety and performance that is valuable even if it does increase the verbosity of code. If there is a drawback to be found with strict typing, it is usually felt by library developers who require flexibility to cover different use cases, and most often appears as a suite of type-named functions such as &lt;code&gt;lib.HandleString&lt;/code&gt;, &lt;code&gt;lib.HandleUint64&lt;/code&gt;, &lt;code&gt;lib.HandleBool&lt;/code&gt; and so on. Go does provide two important language tools that do provide a lot of flexibility in library development: &lt;em&gt;closures&lt;/em&gt; and &lt;em&gt;interfaces&lt;/em&gt;, which we will explore in this post.&lt;/p&gt;</description>
    </item>
    <item>
      <title>New Hugo Theme</title>
      <link>https://bbengfort.github.io/2021/01/new-hugo-theme/</link>
      <pubDate>Sun, 24 Jan 2021 17:12:16 -0500</pubDate>
      <guid>https://bbengfort.github.io/2021/01/new-hugo-theme/</guid>
      <description>&lt;p&gt;A facelift for Libelli today! I moved from &lt;a href=&#34;https://jekyllrb.com/&#34;&gt;Jekyll&lt;/a&gt; to &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; for static site generation, a move that has been long overdue — and I&amp;rsquo;m very happy I&amp;rsquo;ve done it. Not only can I take advantage of a new theme with extra functionality (&lt;a href=&#34;https://themes.gohugo.io/hugo-papermod/&#34;&gt;PaperMod&lt;/a&gt; in this case) but also because Hugo is written in Go, I feel like I have more control over how the site gets generated.&lt;/p&gt;
&lt;p&gt;A lot has been said on this topic, if you&amp;rsquo;re thinking about migrating from Jekyll to Hugo, I recommend &lt;a href=&#34;https://www.sarasoueidan.com/blog/jekyll-ghpages-to-hugo-netlify/&#34;&gt;Sara Soueidan&amp;rsquo;s blog post&lt;/a&gt; — the notes here are Libelli specific and are listed here more as notes than anything else.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Documenting a gRPC API with OpenAPI</title>
      <link>https://bbengfort.github.io/2021/01/grpc-openapi-docs/</link>
      <pubDate>Thu, 21 Jan 2021 17:45:35 +0000</pubDate>
      <guid>https://bbengfort.github.io/2021/01/grpc-openapi-docs/</guid>
      <description>&lt;p&gt;gRPC makes the specification and implementation of networked APIs a snap. But what is the simplest way to &lt;em&gt;document&lt;/em&gt; a gRPC API? There seem to be some hosted providers by Google, e.g. &lt;a href=&#34;https://cloud.google.com/endpoints/docs/grpc/dev-portal-update-ref-docs&#34;&gt;SmartDocs&lt;/a&gt;, but I have yet to find a gRPC-specific tool. For REST API frameworks, documentation is commonly generated along with live examples using &lt;a href=&#34;https://swagger.io/resources/open-api/&#34;&gt;OpenAPI (formerly swagger)&lt;/a&gt;. By using &lt;a href=&#34;https://github.com/grpc-ecosystem/grpc-gateway&#34;&gt;grpc-gateway&lt;/a&gt; it appears to be pretty straight forward to generate a REST/gRPC API combo from protocol buffers and then hook into the OpenAPI specification.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self Signed CA</title>
      <link>https://bbengfort.github.io/2020/12/self-signed-ca/</link>
      <pubDate>Wed, 30 Dec 2020 15:51:06 +0000</pubDate>
      <guid>https://bbengfort.github.io/2020/12/self-signed-ca/</guid>
      <description>&lt;p&gt;I went on a brief adventure looking into creating a lightweight certificate authority (CA) in Go to issue certificates for mTLS connections between peers in a network. The CA was a simple command line program and the idea was that the certificate would initialize its own self-generated certs whose public key would be included in the code base of the peer-to-peer servers, then it could generate TLS x.509 key pairs signed by the CA. Of course you could do this with &lt;code&gt;openssl&lt;/code&gt;, but I wanted to keep a self-coded Go version around for posterity.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OS X Cleanup</title>
      <link>https://bbengfort.github.io/2020/11/mac-cleanup/</link>
      <pubDate>Tue, 24 Nov 2020 14:26:25 +0000</pubDate>
      <guid>https://bbengfort.github.io/2020/11/mac-cleanup/</guid>
      <description>&lt;p&gt;Developer computers often get a lot of cruft built up in non-standard places because of compiled binaries, assets, packages, and other tools that we install over time then forget about as we move onto other projects. In general, I like to reinstall my OS and wipe my disk every year or so to prevent crud from accumulating. As an interemediate step, this post compiles several maintenance caommands that I run fairly routinely.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Managing Multi-Errors in Go</title>
      <link>https://bbengfort.github.io/2020/10/go-multiple-errors/</link>
      <pubDate>Thu, 22 Oct 2020 11:45:41 +0000</pubDate>
      <guid>https://bbengfort.github.io/2020/10/go-multiple-errors/</guid>
      <description>&lt;p&gt;This post is a response to &lt;a href=&#34;https://medium.com/a-journey-with-go/go-multiple-errors-management-a67477628cf1&#34;&gt;Go: Multiple Errors Management&lt;/a&gt;. I&amp;rsquo;ve dealt with a multiple error contexts in a few places in my Go code but never created a subpackage for it in &lt;code&gt;github.com/bbengfort/x&lt;/code&gt; and so I thought this post was a good motivation to explore it in slightly more detail. I&amp;rsquo;d also like to make error contexts for routine cancellation a part of my standard programming practice, so this post also investigates multiple error handling in a single routine or multiple routines like the original post.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Writing JSON into a Zip file with Python</title>
      <link>https://bbengfort.github.io/2020/08/zipfiles-json/</link>
      <pubDate>Thu, 20 Aug 2020 11:41:14 +0000</pubDate>
      <guid>https://bbengfort.github.io/2020/08/zipfiles-json/</guid>
      <description>&lt;p&gt;For scientific reproducibility, it has become common for me to output experimental results as zip files that contain both configurations and inputs as well as one or more output results files. This is similar to .epub or .docx formats which are just specialized zip files - and allows me to easily rerun experiments for comparison purposes. Recently I tried to dump some json data into a zip file using Python 3.8 and was surprised when the code errored as it seemed pretty standard. This is the story of the crazy loophole that I had to go into as a result.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Read mprofile Output into Pandas</title>
      <link>https://bbengfort.github.io/2020/07/read-mprofile-into-pandas/</link>
      <pubDate>Mon, 27 Jul 2020 18:16:50 +0000</pubDate>
      <guid>https://bbengfort.github.io/2020/07/read-mprofile-into-pandas/</guid>
      <description>&lt;p&gt;When benchmarking Python programs, it is very common for me to use &lt;a href=&#34;https://pypi.org/project/memory-profiler/&#34;&gt;&lt;code&gt;memory_profiler&lt;/code&gt;&lt;/a&gt; from the command line - e.g. &lt;code&gt;mprof run python myscript.py&lt;/code&gt;. This creates a .dat file in the current working directory which you can view with &lt;code&gt;mprof show&lt;/code&gt;. More often than not, though I want to compare two different runs for their memory profiles or do things like annotate the graphs with different timing benchmarks. This requires generating my own figures, which requires loading the memory profiler data myself.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Basic Python Profiling</title>
      <link>https://bbengfort.github.io/2020/07/basic-python-profiling/</link>
      <pubDate>Tue, 14 Jul 2020 18:01:08 +0000</pubDate>
      <guid>https://bbengfort.github.io/2020/07/basic-python-profiling/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m getting started on some projects that will make use of extensive Python performance profiling, unfortunately Python doesn&amp;rsquo;t focus on performance and so doesn&amp;rsquo;t have benchmark tools like I might find in Go. I&amp;rsquo;ve noticed that the two most important usages I&amp;rsquo;m looking at when profiling are speed and memory usage. For the latter, I simply use &lt;a href=&#34;https://pypi.org/project/memory-profiler/&#34;&gt;&lt;code&gt;memory_profiler&lt;/code&gt;&lt;/a&gt; from the command line - which is pretty straight forward. However for speed usage, I did find a snippet that I thought would be useful to include and update depending on how my usage changes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Launching a JupyterHub Instance</title>
      <link>https://bbengfort.github.io/2019/10/launch-jupyterhub-server/</link>
      <pubDate>Wed, 09 Oct 2019 16:40:08 +0000</pubDate>
      <guid>https://bbengfort.github.io/2019/10/launch-jupyterhub-server/</guid>
      <description>&lt;p&gt;In this post I walk through the steps of creating a multi-user JupyterHub sever running on an AWS Ubuntu 18.04 instance. There are many ways of setting up JupyterHub including using Docker and Kubernetes - but this is a pretty staight forward mechanism that doesn&amp;rsquo;t have too many moving parts such as TLS termination proxies etc. I think of this as the baseline setup.&lt;/p&gt;
&lt;p&gt;Note that this setup has a few pros or cons depending on how you look at them.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mount an EBS volume</title>
      <link>https://bbengfort.github.io/2019/02/mount-ebs-volume/</link>
      <pubDate>Tue, 05 Feb 2019 12:48:18 +0000</pubDate>
      <guid>https://bbengfort.github.io/2019/02/mount-ebs-volume/</guid>
      <description>&lt;p&gt;Once the EBS volume has been created and attached to the instance, ssh into the instance and list the available disks:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0         7:0    0 86.9M  1 loop /snap/core/4917
loop1         7:1    0 12.6M  1 loop /snap/amazon-ssm-agent/295
loop2         7:2    0   91M  1 loop /snap/core/6350
loop3         7:3    0   18M  1 loop /snap/amazon-ssm-agent/930
nvme0n1     259:0    0  300G  0 disk
nvme1n1     259:1    0    8G  0 disk
└─nvme1n1p1 259:2    0    8G  0 part /
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the above case we want to attach nvme0n1 - a 300GB gp2 EBS volume. Check if the volume already has data in it (e.g. created from a snapshot or being attached to a new instance):&lt;/p&gt;</description>
    </item>
    <item>
      <title>Visual Diagnostics for More Effective Machine Learning</title>
      <link>https://bbengfort.github.io/2019/01/visual-diagnostics-for-more-effective-machine-learning/</link>
      <pubDate>Thu, 10 Jan 2019 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2019/01/visual-diagnostics-for-more-effective-machine-learning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pydata.org/miami2019/schedule/presentation/8/&#34;&gt;Visual Diagnostics for More Effective Machine Learning&lt;/a&gt;&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/2kZ38ysHDzM?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/127690054?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Modeling is often treated as a search activity: find some combination of features, algorithm, and hyperparameters that yields the best score after cross-validation. In this talk, we will explore how to steer the model selection process with visual diagnostics and the Yellowbrick library, leading to more effective and more interpretable results and faster experimental workflows.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Blast Throughput</title>
      <link>https://bbengfort.github.io/2018/09/blast-throughput/</link>
      <pubDate>Wed, 26 Sep 2018 17:06:24 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/09/blast-throughput/</guid>
      <description>&lt;p&gt;Blast throughput is what we call a throughput measurement such that N requests are simultaneously sent to the server and the duration to receive responses for all N requests is recorded. The throughput is computed as &lt;code&gt;N/duration&lt;/code&gt; where duration is in seconds. This is the typical and potentially correct way to measure throughput from a client to a server, however issues do arise in distributed systems land:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the requests must all originate from a single client&lt;/li&gt;
&lt;li&gt;high latency response outliers can skew results&lt;/li&gt;
&lt;li&gt;you must be confident that N is big enough to max out the server&lt;/li&gt;
&lt;li&gt;N mustn&amp;rsquo;t be so big as to create non-server related bottlenecks.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post I&amp;rsquo;ll discuss my implementation of the blast workload as well as an issue that came up with many concurrent connections in gRPC. This led me down the path to use one connection to do blast throughput testing, which led to other issues, which I&amp;rsquo;ll discuss later.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Go Testing Notes</title>
      <link>https://bbengfort.github.io/2018/09/go-testing-notes/</link>
      <pubDate>Sat, 22 Sep 2018 09:58:12 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/09/go-testing-notes/</guid>
      <description>&lt;p&gt;In this post I&amp;rsquo;m just going to maintain a list of notes for Go testing that I seem to commonly need to reference. It will also serve as an index for the posts related to testing that I have to commonly look up as well. Here is a quick listing of the table of contents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basics&#34;&gt;Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#table-driven-tests&#34;&gt;Table Driven Tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fixtures&#34;&gt;Fixtures&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#golden-files&#34;&gt;Golden Files&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#frameworks&#34;&gt;Frameworks&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#no-framework&#34;&gt;No Framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ginkgo--gomega&#34;&gt;Ginkgo &amp;amp; Gomega&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#helpers&#34;&gt;Helpers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#temporary-directories&#34;&gt;Temporary Directories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sources-and-references&#34;&gt;Sources and References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;p&gt;Just a quick reminder of how to write tests, benchmarks, and examples. A test is written as follows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming Remote Throughput</title>
      <link>https://bbengfort.github.io/2018/09/streaming-remote-throughput/</link>
      <pubDate>Tue, 11 Sep 2018 15:19:17 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/09/streaming-remote-throughput/</guid>
      <description>&lt;p&gt;In order to improve the performance of asynchronous message passing in Alia, I&amp;rsquo;m using gRPC bidirectional streaming to create the peer to peer connections. When the replica is initialized it creates a remote connection to each of its peers that lives in its own go routine; any other thread can send messages by passing them to that go routine through a channel, replies are then dispatched via another channel, directed to the thread via an actor dispatching model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Future Date Script</title>
      <link>https://bbengfort.github.io/2018/09/future-date/</link>
      <pubDate>Wed, 05 Sep 2018 17:42:50 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/09/future-date/</guid>
      <description>&lt;p&gt;This is kind of a dumb post, but it&amp;rsquo;s something I&amp;rsquo;m sure I&amp;rsquo;ll look up in the future. I have a lot of emails where I have to send a date that&amp;rsquo;s sometime in the future, e.g. six weeks from the end of a class to specify a deadline … I&amp;rsquo;ve just been opening a Python terminal and importing &lt;code&gt;datetime&lt;/code&gt; and &lt;code&gt;timedelta&lt;/code&gt; but I figured this quick script on the command line would make my life a bit easier:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Aggregating Reads from a Go Channel</title>
      <link>https://bbengfort.github.io/2018/08/aggregating-go-channels/</link>
      <pubDate>Sat, 25 Aug 2018 08:28:59 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/08/aggregating-go-channels/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s the scenario: we have a buffered channel that&amp;rsquo;s being read by a single Go routine and is written to by multiple go routines. For simplicity, we&amp;rsquo;ll say that the channel accepts events and that the other routines generate events of specific types, &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt;. If there are more of one type of event generator (or some producers are faster than others) we may end up in the situation where there are a series of the same events on the buffered channel. What we would like to do is read &lt;em&gt;all&lt;/em&gt; of the same type of event that is on the buffered channel at once, handling them all simultaneously; e.g. aggregating the read of our events.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Actor Model</title>
      <link>https://bbengfort.github.io/2018/08/actor-model/</link>
      <pubDate>Fri, 03 Aug 2018 07:27:36 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/08/actor-model/</guid>
      <description>&lt;p&gt;Building correct concurrent programs in a distributed system with multiple threads and processes can quickly become very complex to reason about. For performance, we want each thread in a single process to operate as independently as possible; however anytime the shared state of the system is modified synchronization is required. Primitives like mutexes can [ensure structs are thread-safe]({% post_url 2017-02-21-synchronizing-structs %}), however in Go, the strong preference for &lt;a href=&#34;https://blog.golang.org/share-memory-by-communicating&#34;&gt;synchronization is communication&lt;/a&gt;. In either case Go programs can quickly become locks upon locks or morasses of channels, incurring performance penalties at each synchronization point.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Syntax Parsing with CoreNLP and NLTK</title>
      <link>https://bbengfort.github.io/2018/06/corenlp-nltk-parses/</link>
      <pubDate>Fri, 22 Jun 2018 14:38:21 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/06/corenlp-nltk-parses/</guid>
      <description>&lt;p&gt;Syntactic parsing is a technique by which segmented, tokenized, and part-of-speech tagged text is assigned a structure that reveals the relationships between tokens governed by syntax rules, e.g. by grammars. Consider the sentence:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The factory employs 12.8 percent of Bradford County.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A syntax parse produces a tree that might help us understand that the subject of the sentence is &amp;ldquo;the factory&amp;rdquo;, the predicate is &amp;ldquo;employs&amp;rdquo;, and the target is &amp;ldquo;12.8 percent&amp;rdquo;, which in turn is modified by &amp;ldquo;Bradford County&amp;rdquo;. Syntax parses are often a first step toward deep information extraction or semantic understanding of text. Note however, that syntax parsing methods suffer from &lt;em&gt;structural ambiguity&lt;/em&gt;, that is the possibility that there exists more than one correct parse for a given sentence. Attempting to select the most likely parse for a sentence is incredibly difficult.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding Machine Learning Through Visualizations with Benjamin Bengfort and Rebecca Bilbro - Episode 166</title>
      <link>https://bbengfort.github.io/2018/06/understanding-machine-learning-through-visualizations-with-benjamin-bengfort-and-rebecca-bilbro-episode-166/</link>
      <pubDate>Sun, 17 Jun 2018 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2018/06/understanding-machine-learning-through-visualizations-with-benjamin-bengfort-and-rebecca-bilbro-episode-166/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.pythonpodcast.com/yellowbrick-with-bejnamin-bengfort-and-rebecca-bilbro-episode-166/&#34;&gt;Understanding Machine Learning Through Visualizations with Benjamin Bengfort and Rebecca Bilbro - Episode 166&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Machine learning models are often inscrutable and it can be difficult to know whether you are making progress. To improve feedback and speed up iteration&lt;/p&gt;</description>
    </item>
    <item>
      <title>Continuing Outer Loops with for/else</title>
      <link>https://bbengfort.github.io/2018/05/continuing-outer-loops-for-else/</link>
      <pubDate>Thu, 17 May 2018 09:02:43 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/05/continuing-outer-loops-for-else/</guid>
      <description>&lt;p&gt;When you have an outer and an inner loop, how do you continue the outer loop from a condition inside the inner loop? Consider the following code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# break out of inner loop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# continue outer loop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# don&amp;#39;t print unless inner loop completes,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# e.g. outer loop is not continued&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;inner complete!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here, we want to print for all &lt;code&gt;i&lt;/code&gt; ∈ &lt;code&gt;[0,10)&lt;/code&gt; all numbers &lt;code&gt;j&lt;/code&gt; ∈ &lt;code&gt;[0,9)&lt;/code&gt; that are less than or equal to i and we want to print complete once we&amp;rsquo;ve found an entire list of &lt;code&gt;j&lt;/code&gt; that meets the criteria. While this seems like a fairly contrived example, I&amp;rsquo;ve actually encountered this exact situation in several places in code this week, and I&amp;rsquo;ll provide a real example in a bit.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Predicted Class Balance</title>
      <link>https://bbengfort.github.io/2018/03/prediction-balance/</link>
      <pubDate>Thu, 08 Mar 2018 09:18:37 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/03/prediction-balance/</guid>
      <description>&lt;p&gt;This is a follow on to the [prediction distribution]({{ site.base_url }}{% link _posts/2018-02-28-prediction-distribution.md %}) visualization presented in the last post. This visualization shows a bar chart with the number of predicted and number of actual values for each class, e.g. a class balance chart with predicted balance as well.&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bbengfort.github.io/images/2018-03-08-cb-preds.png&#34; alt=&#34;Class Balance of Actual vs. Predictions&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;This visualization actually came before the prior visualization, but I was more excited about that one because it showed where error was occurring similar to a classification report or confusion matrix. I&amp;rsquo;ve recently been using this chart for initial spot checking more however, since it gives me a general feel for how balanced both the class and the classifier is with respect to each other. It has also helped diagnose what is being displayed in the heat map chart of the other post.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Class Balance Prediction Distribution</title>
      <link>https://bbengfort.github.io/2018/02/prediction-distribution/</link>
      <pubDate>Wed, 28 Feb 2018 12:52:11 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/02/prediction-distribution/</guid>
      <description>&lt;p&gt;In this quick snippet I present an alternative to the &lt;a href=&#34;http://www.scikit-yb.org/en/latest/api/classifier/confusion_matrix.html&#34;&gt;confusion matrix&lt;/a&gt; or &lt;a href=&#34;http://www.scikit-yb.org/en/latest/api/classifier/classification_report.html&#34;&gt;classification report&lt;/a&gt; visualizations in order to judge the efficacy of multi-class classifiers:&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bbengfort.github.io/images/2018-02-28-cb-preds-dist.png&#34; alt=&#34;Class Balance of Actual vs. Predictions&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;The base of the visualization is a class balance chart, the x-axis is the actual (or true class) and the height of the bar chart is the number of instances that match that class in the dataset. The difference here is that each bar is a stacked chart representing the percentage of the predicted class given the actual value. If the predicted color matches the actual color then the classifier was correct, otherwise it was wrong.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Synchronization in Write Throughput</title>
      <link>https://bbengfort.github.io/2018/02/sync-write-throughput/</link>
      <pubDate>Tue, 13 Feb 2018 07:10:06 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/02/sync-write-throughput/</guid>
      <description>&lt;p&gt;This post serves as a reminder of how to perform benchmarks when accounting for synchronized writing in Go. The normal benchmarking process involves running a command a large number of times and determining the average amount of time that operation took. When threads come into play, we consider &lt;em&gt;throughput&lt;/em&gt; - that is the number of operations that can be conducted per second. However, in order to successfully measure this without duplicating time, the throughput must be measured from the server&amp;rsquo;s perspective.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Thread and Non-Thread Safe Go Set</title>
      <link>https://bbengfort.github.io/2018/01/go-set/</link>
      <pubDate>Fri, 26 Jan 2018 09:15:13 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/01/go-set/</guid>
      <description>&lt;p&gt;I came across this now archived project that implements a &lt;a href=&#34;https://github.com/fatih/set&#34;&gt;set data structure in Go&lt;/a&gt; and was intrigued by the implementation of both thread-safe and non-thread-safe implementations of the same data structure. Recently I&amp;rsquo;ve been attempting to get rid of locks in my code in favor of one master data structure that does all of the synchronization, having multiple options for thread safety is useful. Previously I did this by having a lower-case method name (a private method) that was non-thread-safe and an upper-case method name (public) that did implement thread-safety. However, as I&amp;rsquo;ve started to reorganize my packages this no longer works.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Git-Style File Editing in CLI</title>
      <link>https://bbengfort.github.io/2018/01/cli-editor-app/</link>
      <pubDate>Sat, 06 Jan 2018 09:30:58 +0000</pubDate>
      <guid>https://bbengfort.github.io/2018/01/cli-editor-app/</guid>
      <description>&lt;p&gt;A recent application I was working on required the management of several configuration and list files that needed to be validated. Rather than have the user find and edit these files directly, I wanted to create an editing  workflow similar to &lt;code&gt;crontab -e&lt;/code&gt; or &lt;code&gt;git commit&lt;/code&gt; — the user would call the application, which would redirect to a text editor like vim, then when editing was complete, the application would take over again.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transaction Handling with Psycopg2</title>
      <link>https://bbengfort.github.io/2017/12/psycopg2-transactions/</link>
      <pubDate>Wed, 06 Dec 2017 13:58:16 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/12/psycopg2-transactions/</guid>
      <description>&lt;p&gt;Databases are essential to most applications, however most database interaction is often overlooked by Python developers who use higher level libraries like Django or SQLAlchemy. We use and love PostgreSQL with Psycopg2, but I recently realized that I didn&amp;rsquo;t have a good grasp on how exactly psycopg2 implemented core database concepts: particularly transaction isolation and thread safety.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s what the documentation says regarding transactions:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Transactions are handled by the connection class. By default, the first time a command is sent to the database (using one of the cursors created by the connection), a new transaction is created. The following database commands will be executed in the context of the same transaction – not only the commands issued by the first cursor, but the ones issued by all the cursors created by the same connection. Should any command fail, the transaction will be aborted and no further command will be executed until a call to the rollback() method.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lock Diagnostics in Go</title>
      <link>https://bbengfort.github.io/2017/09/lock-diagnostics/</link>
      <pubDate>Thu, 28 Sep 2017 10:44:30 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/09/lock-diagnostics/</guid>
      <description>&lt;p&gt;By now it&amp;rsquo;s pretty clear that I&amp;rsquo;ve just had a bear of a time with locks and synchronization inside of multi-threaded environments with Go. Probably most gophers would simply tell me that I should share memory by communicating rather than to communication by sharing memory — and frankly I&amp;rsquo;m in that camp too. The issue is that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mutexes can be more expressive than channels&lt;/li&gt;
&lt;li&gt;Channels are fairly heavyweight&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So to be honest, there are situations where a mutex is a better choice than a channel. I believe that one of those situations is when dealing with replicated state machines … which is what I&amp;rsquo;ve been working on the past few months. The issue is that the state of the replica has to be consistent across a variety of events: timers and remote messages. The problem is that the timers and network traffic are all go routines, and there can be a lot of them running in the system at a time.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lock Queuing in Go</title>
      <link>https://bbengfort.github.io/2017/09/lock-queueing/</link>
      <pubDate>Fri, 08 Sep 2017 11:31:19 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/09/lock-queueing/</guid>
      <description>&lt;p&gt;In Go, you can use &lt;code&gt;sync.Mutex&lt;/code&gt; and &lt;code&gt;sync.RWMutex&lt;/code&gt; objects to create thread-safe data structures in memory as discussed in [“Synchronizing Structs for Safe Concurrency in Go”]({% post_url 2017-02-21-synchronizing-structs %}). When using the &lt;code&gt;sync.RWMutex&lt;/code&gt; in Go, there are two kinds of locks: read locks and write locks. The basic difference is that many read locks can be acquired at the same time, but only one write lock can be acquired at at time.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Messaging Throughput gRPC vs. ZMQ</title>
      <link>https://bbengfort.github.io/2017/09/message-throughput/</link>
      <pubDate>Mon, 04 Sep 2017 17:20:06 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/09/message-throughput/</guid>
      <description>&lt;p&gt;Building distributed systems in Go requires an RPC or message framework of some sort. In the systems I build I prefer to pass messages serialized with &lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;protocol buffers&lt;/a&gt; therefore a natural choice for me is &lt;a href=&#34;https://grpc.io/&#34;&gt;grpc&lt;/a&gt;. The grpc library uses HTTP2 as a transport layer and provides a code generator based on the protocol buffer syntax making it very simple to use.&lt;/p&gt;
&lt;p&gt;For more detailed control, the &lt;a href=&#34;http://zeromq.org/&#34;&gt;ZMQ&lt;/a&gt; library is an excellent, low latency socket framework. ZMQ provides several communication patterns from basic REQ/REP (request/reply) to PUB/SUB (publish/subscribe). ZMQ is used at a lower level though, so more infrastructure per app needs to be built.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Online Distribution</title>
      <link>https://bbengfort.github.io/2017/08/online-distribution/</link>
      <pubDate>Mon, 28 Aug 2017 12:49:46 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/08/online-distribution/</guid>
      <description>&lt;p&gt;This post started out as a discussion of a &lt;code&gt;struct&lt;/code&gt; in Go that could keep track of online statistics without keeping an array of values. It ended up being a lesson on over-engineering for concurrency.&lt;/p&gt;
&lt;p&gt;The spec of the routine was to build a data structure that could keep track of internal statistics of values over time in a space-saving fashion. The primary interface was a method, &lt;code&gt;Update(sample float64)&lt;/code&gt;, so that a new sample could be passed to the structure, updating internal parameters. At conclusion, the structure should be able to describe the mean, variance, and range of all values passed to the update method. I created two versions:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rapid FS Walks with ErrGroup</title>
      <link>https://bbengfort.github.io/2017/08/rapid-fs-walk/</link>
      <pubDate>Fri, 18 Aug 2017 15:33:35 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/08/rapid-fs-walk/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been looking for a way to quickly scan a file system and gather information about the files in directories contained within. I had been doing this with multiprocessing in Python, but figured Go could speed up my performance by a lot. What I discovered when I went down this path was the &lt;a href=&#34;https://godoc.org/golang.org/x/sync/errgroup&#34;&gt;&lt;code&gt;sync.ErrGroup&lt;/code&gt;&lt;/a&gt;, an extension of the &lt;code&gt;sync.WaitGroup&lt;/code&gt; that helps manage the complexity of multiple go routines but also includes error handling!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Buffered Write Performance</title>
      <link>https://bbengfort.github.io/2017/08/buffered-writes/</link>
      <pubDate>Thu, 03 Aug 2017 09:48:19 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/08/buffered-writes/</guid>
      <description>&lt;p&gt;This is just a quick note on the performance of writing to a file on disk using Go, and reveals a question about a common programming paradigm that I am now suspicious of.  I discovered that when I wrapped the open file object with a &lt;a href=&#34;https://golang.org/pkg/bufio/#Writer&#34;&gt;&lt;code&gt;bufio.Writer&lt;/code&gt;&lt;/a&gt; that the performance of my writes to disk significantly increased. Ok, so this isn&amp;rsquo;t about simple file writing to disk, this is about a complex writer that does some seeking in the file writing to different positions and maintains the overall state of what&amp;rsquo;s on disk in memory, however the question remains:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Event Dispatcher in Go</title>
      <link>https://bbengfort.github.io/2017/07/event-dispatcher/</link>
      <pubDate>Fri, 21 Jul 2017 06:28:45 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/07/event-dispatcher/</guid>
      <description>&lt;p&gt;The event dispatcher pattern is extremely common in software design, particularly in languages like JavaScript that are primarily used for user interface work. The dispatcher is an object (usually a mixin to other objects) that can &lt;em&gt;register&lt;/em&gt; callback functions for particular events. Then when a &lt;em&gt;dispatch&lt;/em&gt; method is called with an event, the dispatcher calls each callback function in order of their registration and passes them a copy of the event. In fact, I&amp;rsquo;ve already written a version of this pattern in Python: [Implementing Observers with Events]({% post_url 2016-02-16-observer-pattern %})  In this snippet, I&amp;rsquo;m presenting a version in Go that has been incredibly stable and useful in my code.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lazy Pirate Client</title>
      <link>https://bbengfort.github.io/2017/07/lazy-pirate/</link>
      <pubDate>Fri, 14 Jul 2017 10:24:15 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/07/lazy-pirate/</guid>
      <description>&lt;p&gt;In the [last post]({% post_url 2017-07-13-zmq-basic %}) I discussed a simple REQ/REP pattern for ZMQ. However, by itself &lt;a href=&#34;http://dbeck.github.io/5-lessons-learnt-from-choosing-zeromq-and-protobuf/&#34;&gt;REQ/REP is pretty fragile&lt;/a&gt;. First, every REQ requires a REP and a server can only handle one request at a time. Moreover, if the server fails in the middle of a reply, then everything is hung. We need more reliable REQ/REP, which is actually the subject of &lt;a href=&#34;http://zguide.zeromq.org/page:all#toc86&#34;&gt;an entire chapter&lt;/a&gt; in the ZMQ book.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simple ZMQ Message Passing</title>
      <link>https://bbengfort.github.io/2017/07/zmq-basic/</link>
      <pubDate>Thu, 13 Jul 2017 11:00:27 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/07/zmq-basic/</guid>
      <description>&lt;p&gt;There are many ways to create RPCs and send messages between nodes in a distributed system. Typically when we think about messaging, we think about a transport layer (TCP, IP) and a protocol layer (HTTP) along with some message serialization. Perhaps best known are RESTful APIs which allow us to GET, POST, PUT, and DELETE JSON data to a server. Other methods include gRPC which uses HTTP and protocol buffers for interprocess communication.&lt;/p&gt;</description>
    </item>
    <item>
      <title>PID File Management</title>
      <link>https://bbengfort.github.io/2017/07/pid-management/</link>
      <pubDate>Tue, 11 Jul 2017 09:10:44 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/07/pid-management/</guid>
      <description>&lt;p&gt;In this discussion, I want to propose some code to perform PID file management in a Go program. When a program is backgrounded or daemonized we need some way to communicate with it in order to stop it. All active processes are assigned a &lt;a href=&#34;https://en.wikipedia.org/wiki/Process_identifier&#34;&gt;unique process id&lt;/a&gt; by the operating system and that ID can be used to send signals to the program. Therefore a PID file:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The pid files contains the process id (a number) of a given program. For example, Apache HTTPD may write it&amp;rsquo;s main process number to a pid file - which is a regular text file, nothing more than that - and later use the information there contained to stop itself. You can also use that information (just do a &lt;code&gt;cat filename.pid&lt;/code&gt;) to kill the process yourself, using &lt;code&gt;echo filename.pid | xargs kill&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Public IP Address Discovery</title>
      <link>https://bbengfort.github.io/2017/07/public-ip/</link>
      <pubDate>Sun, 09 Jul 2017 13:14:46 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/07/public-ip/</guid>
      <description>&lt;p&gt;When doing research on peer-to-peer networks, addressing can become pretty complex pretty quickly. Not everyone has the resources to allocate static, public facing IP addresses to machines. A machine that is in a home network for example only has a single public-facing IP address, usually assigned to the router. The router then performs NAT (network address translation) forwarding requests to internal devices.&lt;/p&gt;
&lt;p&gt;In order to get a service running on an internal network, you can port forward external requests to a specific port to a specific device. Requests are made to the router&amp;rsquo;s IP address, and the router passes it on. But how do you know the IP address of the device? Moreover, what happens if the router is assigned a new IP address? Static IP addresses generally cost more.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On the Tracks with Rails</title>
      <link>https://bbengfort.github.io/2017/07/on-track-with-rails/</link>
      <pubDate>Thu, 06 Jul 2017 08:15:13 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/07/on-track-with-rails/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://bbengfort.github.io/images/2017-07-06-kahu-screenshot.png&#34;&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bbengfort.github.io/images/2017-07-06-kahu-screenshot.png&#34; alt=&#34;Kahu Screenshot&#34;  /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m preparing to move into a new job when I finish my dissertation hopefully later this summer. The new role involves web application development with Rails and so I needed to get up to speed. I had a web application requirement for my research so I figured I&amp;rsquo;d knock out two birds with one stone and build that &lt;a href=&#34;https://github.com/bbengfort/kahu&#34;&gt;app with Rails&lt;/a&gt; (a screenshot of the app is above, though of course this is just a front-end and doesn&amp;rsquo;t really tell you it was built with Rails).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Visual Pipelines for Text Analysis</title>
      <link>https://bbengfort.github.io/2017/06/visual-pipelines-for-text-analysis/</link>
      <pubDate>Sat, 24 Jun 2017 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2017/06/visual-pipelines-for-text-analysis/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://data-intelligence.ai/presentations/13&#34;&gt;Visual Pipelines for Text Analysis&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Employing machine learning in practice is half search, half expertise, and half blind luck. In this talk we will explore how to make the luck half less blind by using visual pipelines to steer model selection from raw input to operational prediction. We will look specifically at extending transformer pipelines with visualizers for sentiment analysis and topic modeling text corpora.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concurrent Subprocesses and Fabric</title>
      <link>https://bbengfort.github.io/2017/06/concurrent-subprocesses-fabric/</link>
      <pubDate>Wed, 14 Jun 2017 15:56:24 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/06/concurrent-subprocesses-fabric/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve ben using &lt;a href=&#34;http://docs.fabfile.org/&#34;&gt;Fabric&lt;/a&gt; to concurrently start multiple processes on several machines. These processes have to run at the same time (since they are experimental processes and are interacting with each other) and shut down at more or less the same time so that I can collect results and immediately execute the next sample in the experiment. However, I was having a some difficulties directly using Fabric:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Fabric can parallelize one task across multiple hosts accordint to roles.&lt;/li&gt;
&lt;li&gt;Fabric can be hacked to run multiple tasks on multiple hosts by setting &lt;code&gt;env.dedupe_hosts = False&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fabric can only parallelize one type of task, not multiple types&lt;/li&gt;
&lt;li&gt;Fabric can&amp;rsquo;t handle large numbers of SSH connections&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post we&amp;rsquo;ll explore my approach with Fabric and my current solution.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Appending Results to a File</title>
      <link>https://bbengfort.github.io/2017/06/append-json-results/</link>
      <pubDate>Mon, 12 Jun 2017 16:04:24 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/06/append-json-results/</guid>
      <description>&lt;p&gt;In my current experimental setup, each process is a single instance of sample, from start to finish. This means that I need to aggregate results across multiple process runs that are running concurrently. Moreover, I may need to aggregate those results between machines.&lt;/p&gt;
&lt;p&gt;The most compact format to store results in is CSV. This was my first approach and it had some benefits including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;small file sizes&lt;/li&gt;
&lt;li&gt;readability&lt;/li&gt;
&lt;li&gt;CSV files can just be concatenated together&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The problems were:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Compression Benchmarks</title>
      <link>https://bbengfort.github.io/2017/06/compression-benchmarks/</link>
      <pubDate>Wed, 07 Jun 2017 10:45:35 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/06/compression-benchmarks/</guid>
      <description>&lt;p&gt;One of the projects I&amp;rsquo;m currently working on is the &lt;a href=&#34;http://baleen.districtdatalabs.com/&#34;&gt;ingestion of RSS feeds into a Mongo database&lt;/a&gt;. It&amp;rsquo;s been running for the past year, and as of this post has collected 1,575,987 posts for 373 feeds after 8,126 jobs. This equates to about 585GB of raw data, and a firm requirement for compression in order to exchange data.&lt;/p&gt;
&lt;p&gt;Recently, &lt;a href=&#34;https://github.com/ojedatony1616&#34;&gt;@ojedatony1616&lt;/a&gt; downloaded the compressed zip file (53GB) onto a 1TB external hard disk and attempted to decompress it. After three days, he tried to cancel it and ended up restarting his computer because it wouldn&amp;rsquo;t cancel. His approach was simply to double click the file on OS X, but that got me to thinking &amp;ndash; it shouldn&amp;rsquo;t have taken that long; why did it choke? Inspecting the export logs on the server, I noted that it took 137 minutes to compress the directory; shouldn&amp;rsquo;t it take that long to decompress as well?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Decorating Nose Tests</title>
      <link>https://bbengfort.github.io/2017/05/test-decorators/</link>
      <pubDate>Mon, 22 May 2017 13:05:08 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/05/test-decorators/</guid>
      <description>&lt;p&gt;Was introduced to an interesting problem today when decorating tests that need to be discovered by the &lt;a href=&#34;https://pypi.python.org/pypi/nose/1.3.7&#34;&gt;nose&lt;/a&gt; runner. By default, nose explores a directory looking for things named &lt;code&gt;test&lt;/code&gt; or &lt;code&gt;tests&lt;/code&gt; and then executes those functions, classes, modules, etc. as tests. A standard test suite for me looks something like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;unittest&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;MyTests&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unittest&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TestCase&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;test_undecorated&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        assert undecorated works
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;assertEqual&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The problem came up when we wanted to decorate a test with some extra functionality, for example loading a fixture:&lt;/p&gt;</description>
    </item>
    <item>
      <title>In Process Cacheing</title>
      <link>https://bbengfort.github.io/2017/05/in-process-caches/</link>
      <pubDate>Wed, 17 May 2017 08:16:34 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/05/in-process-caches/</guid>
      <description>&lt;p&gt;I have had some recent discussions regarding cacheing to improve application performance that I wanted to share. Most of the time those conversations go something like this: “have you heard of Redis?” I&amp;rsquo;m fascinated by the fact that an independent, distributed key-value store has won the market to this degree. However, as I&amp;rsquo;ve pointed out in these conversations, cacheing is a hierarchy (heck, even the processor has varying levels of cacheing). Especially when considering micro-service architectures that require extremely low latency responses, cacheing should be a critical part of the design, not just a bolt-on after thought!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unique Values in Python: A Benchmark</title>
      <link>https://bbengfort.github.io/2017/05/python-unique-benchmark/</link>
      <pubDate>Tue, 02 May 2017 14:24:18 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/05/python-unique-benchmark/</guid>
      <description>&lt;p&gt;An interesting question came up in the development of &lt;a href=&#34;http://www.scikit-yb.org/&#34;&gt;Yellowbrick&lt;/a&gt;: given a vector of values, what is the quickest way to get the unique values? Ok, so maybe this isn&amp;rsquo;t a terribly interesting question, however the results surprised us and may surprise you as well. First we&amp;rsquo;ll do a little background, then I&amp;rsquo;ll give the results and then discuss the benchmarking method.&lt;/p&gt;
&lt;p&gt;The problem comes up in Yellowbrick when we want to get the discrete values for a target vector, &lt;code&gt;y&lt;/code&gt; — a problem that comes up in classification tasks. By getting the unique set of values we know the number of classes, as well as the class names. This information is necessary during visualization because it is vital in assigning colors to individual classes. Therefore in a Visualizer we might have a method as follows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Measuring Throughput</title>
      <link>https://bbengfort.github.io/2017/04/throughput/</link>
      <pubDate>Fri, 28 Apr 2017 15:22:40 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/04/throughput/</guid>
      <description>&lt;p&gt;Part of my research is taking me down a path where I want to measure the number of reads and writes from a client to a storage server. A key metric that we&amp;rsquo;re looking for is &lt;em&gt;throughput&lt;/em&gt; — the number of accesses per second that a system supports. As I discovered in a very simple test to get some baseline metrics, even this simple metric can have some interesting complications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OAuth Tokens on the Command Line</title>
      <link>https://bbengfort.github.io/2017/04/oauth-token-command-line/</link>
      <pubDate>Thu, 20 Apr 2017 10:26:32 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/04/oauth-token-command-line/</guid>
      <description>&lt;p&gt;This week I discovered I had a problem with my Google Calendar — events accidentally got duplicated or deleted and I needed a way to verify that my primary calendar was correct. Rather than painstakingly go through the web interface and spot check every event, I instead wrote a Go console program using the &lt;a href=&#34;https://developers.google.com/google-apps/calendar/quickstart/go&#34;&gt;Google Calendar API&lt;/a&gt; to retrieve events and save them in a CSV so I could inspect them all at once. This was great, and very easy using Google&amp;rsquo;s Go libraries for their APIs, and the quick start was very handy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gmail Notifications with Python</title>
      <link>https://bbengfort.github.io/2017/04/gmail-notifications-python/</link>
      <pubDate>Mon, 17 Apr 2017 12:26:55 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/04/gmail-notifications-python/</guid>
      <description>&lt;p&gt;I routinely have long-running scripts (e.g. for a data processing task) that I  want to know when they&amp;rsquo;re complete. It seems like it should be simple for me to add in a little snippet of code that will send an email using Gmail to notify me, right? Unfortunately, it isn&amp;rsquo;t quite that simple for a lot of reasons, including security, attachment handling, configuration, etc. In this snippet, I&amp;rsquo;ve attached my constant copy and paste &lt;code&gt;notify()&lt;/code&gt; function, written into a command line script for easy sending on the command line.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Benchmark of Grumpy Transpiling</title>
      <link>https://bbengfort.github.io/2017/03/grumpy-transpiling-fib-benchmark/</link>
      <pubDate>Thu, 23 Mar 2017 08:47:04 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/03/grumpy-transpiling-fib-benchmark/</guid>
      <description>&lt;p&gt;On Tuesday evening I attended a &lt;a href=&#34;https://www.meetup.com/django-district/events/238128100/&#34;&gt;Django District meetup&lt;/a&gt; on &lt;a href=&#34;https://github.com/google/grumpy&#34;&gt;Grumpy&lt;/a&gt;, a &lt;a href=&#34;https://www.stevefenton.co.uk/2012/11/compiling-vs-transpiling/&#34;&gt;transpiler&lt;/a&gt; from Python to Go. Because it was a Python meetup, the talk naturally focused on introducing Go to a Python audience, and because it was a Django meetup, we also focused on web services. The premise for Grumpy, as discussed in the announcing &lt;a href=&#34;https://opensource.googleblog.com/2017/01/grumpy-go-running-python.html&#34;&gt;Google blog post&lt;/a&gt;, is also a web focused one — to take YouTube&amp;rsquo;s API that&amp;rsquo;s primarily written in Python and transpile it to Go to improve the overall performance and stability of YouTube&amp;rsquo;s front-end services.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sanely gRPC Dial a Remote</title>
      <link>https://bbengfort.github.io/2017/03/sanely-grpc-dial-a-remote/</link>
      <pubDate>Tue, 21 Mar 2017 16:27:39 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/03/sanely-grpc-dial-a-remote/</guid>
      <description>&lt;p&gt;In my systems I need to handle failure; so unlike in a typical client-server relationship, I&amp;rsquo;m prepared for the remote I&amp;rsquo;m dialing to not be available. Unfortunately when you do this with &lt;a href=&#34;https://godoc.org/google.golang.org/grpc&#34;&gt;gRPC-Go&lt;/a&gt; there are a couple of annoyances you have to address. They are (in order of solutions):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Verbose connection logging&lt;/li&gt;
&lt;li&gt;Background and back-off for reconnection attempts&lt;/li&gt;
&lt;li&gt;Errors are not returned on demand.&lt;/li&gt;
&lt;li&gt;There is no ability to keep track of statistics&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So first the logging. When you dial an unavailable remote as follows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Contributing a Multiprocess Memory Profiler</title>
      <link>https://bbengfort.github.io/2017/03/contributing-a-multiprocess-memory-profiler/</link>
      <pubDate>Mon, 20 Mar 2017 11:42:58 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/03/contributing-a-multiprocess-memory-profiler/</guid>
      <description>&lt;p&gt;In this post I wanted to catalog the process of an open source contribution I was a part of, which added a feature to the &lt;a href=&#34;https://pypi.python.org/pypi/memory_profiler/&#34;&gt;memory profiler&lt;/a&gt; Python library by &lt;a href=&#34;http://fseoane.net/&#34;&gt;Fabian Pedregosa&lt;/a&gt; and &lt;a href=&#34;https://github.com/pgervais&#34;&gt;Philippe Gervais&lt;/a&gt;. It&amp;rsquo;s a quick story to tell but took over a year to complete, and I learned a lot from the process. I hope that the story is revealing, particularly to first time contributors and shows that even folks that have been doing this for a long time still have to find ways to positively approach collaboration in an open source environment. I also think it&amp;rsquo;s a fairly standard example of how contributions work in practice and perhaps this story will help us all think about how to better approach the pull request process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pseudo Merkle Tree</title>
      <link>https://bbengfort.github.io/2017/03/pseudo-merkle-tree/</link>
      <pubDate>Thu, 16 Mar 2017 12:23:21 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/03/pseudo-merkle-tree/</guid>
      <description>&lt;p&gt;A &lt;a href=&#34;https://en.wikipedia.org/wiki/Merkle_tree&#34;&gt;Merkle tree&lt;/a&gt; is a data structure in which every non-leaf node is labeled with the hash of its child nodes. This makes them particular useful for comparing large data structures quickly and efficiently. Given trees &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, if the root hash of either is different, it means that part of the tree below is different (if they are identical, they are probably also identical). You can then proceed in a a breadth first fashion, pruning nodes with identical hashes to directly identify the differences.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Select in Go</title>
      <link>https://bbengfort.github.io/2017/03/channel-select/</link>
      <pubDate>Wed, 08 Mar 2017 10:52:39 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/03/channel-select/</guid>
      <description>&lt;p&gt;Ask a Go programmer what makes Go special and they will immediately say “concurrency is baked into the language”. Go&amp;rsquo;s concurrency model is one of communication (as opposed to locks) and so concurrency primitives are implemented using &lt;em&gt;channels&lt;/em&gt;. In order to synchronize across multiple channels, go provides the &lt;code&gt;select&lt;/code&gt; statement.&lt;/p&gt;
&lt;p&gt;A common pattern for me has become to use a &lt;code&gt;select&lt;/code&gt; to manage broadcasted work (either in a publisher/subscriber model or a fanout model) by initializing go routines and passing them &lt;em&gt;directional channels&lt;/em&gt; for synchronization and communication. In the example below, I create a buffered channel for output (so that the workers don&amp;rsquo;t block waiting for the receiver to collect data), a channel for errors (first error kills the program) and a timer to update the state of my process on a routine basis. The &lt;code&gt;select&lt;/code&gt; waits for the first channel to receive a message and then continues processing. By keeping the &lt;code&gt;select&lt;/code&gt; in a &lt;code&gt;for&lt;/code&gt; loop, I can continually read of the channels until I&amp;rsquo;m done.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Benchmarking Secure gRPC</title>
      <link>https://bbengfort.github.io/2017/03/tls-grpc-benchmarks/</link>
      <pubDate>Sun, 05 Mar 2017 17:26:24 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/03/tls-grpc-benchmarks/</guid>
      <description>&lt;p&gt;A natural question to ask after the previous post is “how much overhead does security add?” So I&amp;rsquo;ve benchmarked the three methods discussed; mutual TLS, server-side TLS, and no encryption. The results are below:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bbengfort.github.io/images/2017-03-05-benchmark.png&#34;&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bbengfort.github.io/images/2017-03-05-benchmark.png&#34; alt=&#34;Secure gRPC Benchmarks&#34;  /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here are the numeric results for one of the runs:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;BenchmarkMutualTLS-8   	     200	   9331850 ns/op
BenchmarkServerTLS-8   	     300	   5004505 ns/op
BenchmarkInsecure-8    	    2000	   1179252 ns/op
PASS
ok  	github.com/bbengfort/sping	7.364s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here is the code for the benchmarking for reference:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Secure gRPC with TLS/SSL</title>
      <link>https://bbengfort.github.io/2017/03/secure-grpc/</link>
      <pubDate>Fri, 03 Mar 2017 09:41:39 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/03/secure-grpc/</guid>
      <description>&lt;p&gt;One of the primary requirements for the systems we build is something we call the “minimum security requirement”. Although our systems are not designed specifically for high security applications, they must use minimum standards of encryption and authentication. For example, it seems obvious to me that a web application that &lt;a href=&#34;https://docs.djangoproject.com/en/1.10/topics/auth/passwords/&#34;&gt;stores passwords&lt;/a&gt; or &lt;a href=&#34;https://www.pcisecuritystandards.org/&#34;&gt;credit card information&lt;/a&gt; would encrypt their data on disk on a per-record basis with a &lt;a href=&#34;https://www.codeproject.com/Articles/704865/Salted-Password-Hashing-Doing-it-Right&#34;&gt;salted hash&lt;/a&gt;. In the same way, a distributed system must be able to handle &lt;a href=&#34;https://www.usenix.org/legacy/event/osdi04/tech/full_papers/li_j/li_j.pdf&#34;&gt;encrypted blobs&lt;/a&gt;, &lt;a href=&#34;http://blog.cloudera.com/blog/2013/03/how-to-set-up-a-hadoop-cluster-with-network-encryption/&#34;&gt;encrypt all inter-node communication&lt;/a&gt;, and &lt;a href=&#34;https://alexbilbie.com/2012/11/hawk-a-new-http-authentication-scheme/&#34;&gt;authenticate and sign all messages&lt;/a&gt;. This adds some overhead to the system but the cost of overhead is far smaller than the cost of a breach, and if minimum security is the baseline then the overhead is just an accepted part of doing business.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Synchronizing Structs for Safe Concurrency in Go</title>
      <link>https://bbengfort.github.io/2017/02/synchronizing-structs/</link>
      <pubDate>Tue, 21 Feb 2017 10:48:24 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/02/synchronizing-structs/</guid>
      <description>&lt;p&gt;Go is &lt;a href=&#34;https://divan.github.io/posts/go_concurrency_visualize/&#34;&gt;built for concurrency&lt;/a&gt; by providing language features that allow developers to embed complex concurrency patterns into their applications. These language features can be intuitive and a lot of safety is built in (for example a &lt;a href=&#34;https://blog.golang.org/race-detector&#34;&gt;race detector&lt;/a&gt;) but developers still need to be aware of the interactions between various threads in their programs.&lt;/p&gt;
&lt;p&gt;In any shared memory system the biggest concern is &lt;a href=&#34;https://en.wikipedia.org/wiki/Synchronization_(computer_science)&#34;&gt;synchronization&lt;/a&gt;: ensuring that separate go routines operate in the correct order and that no race conditions occur. The primary way to handle synchronization is the use of &lt;a href=&#34;https://gobyexample.com/channels&#34;&gt;channels&lt;/a&gt;. Channels synchronize execution by forcing sends on the channel to block until the value on the channel is received. In this way, channels act as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Barrier_(computer_science)&#34;&gt;barrier&lt;/a&gt; since the go routine can not progress while being blocked by the channel and enforce a specific ordering to execution, the ordering of routines arriving at the barrier.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fixed vs. Variable Length Chunking</title>
      <link>https://bbengfort.github.io/2017/02/chunking/</link>
      <pubDate>Wed, 08 Feb 2017 19:51:28 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/02/chunking/</guid>
      <description>&lt;p&gt;FluidFS and other file systems break large files into recipes of hash-identified blobs of binary data. Blobs can then be replicated with far more ease than a single file, as well as streamed from disk in a memory safe manner. Blobs are treated as single, independent units so the underlying data store doesn&amp;rsquo;t grow as files are duplicated. Finally, blobs can be encrypted individually and provide more opportunities for privacy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Extracting a TOC from Markup</title>
      <link>https://bbengfort.github.io/2017/02/extract-toc/</link>
      <pubDate>Sun, 05 Feb 2017 09:11:27 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/02/extract-toc/</guid>
      <description>&lt;p&gt;In today&amp;rsquo;s addition of “really simple things that come in handy all the time” I present a simple script to extract the table of contents from markdown or asciidoc files:&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/bbengfort/6ab36e0f518fe3e0f92bce6f53bdd80f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;So this is pretty simple, just use regular expressions to look for lines that start with one or more &lt;code&gt;&amp;quot;#&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;=&amp;quot;&lt;/code&gt; (for markdown and asciidoc, respectively) and print them out with an indent according to their depth (e.g. indent &lt;code&gt;##&lt;/code&gt; heading 2 one block). Because this script goes from top to bottom, you get a quick view of the document structure without creating a nested data structure under the hood. I&amp;rsquo;ve also implemented some simple type detection using common extensions to decide which regex to use.&lt;/p&gt;</description>
    </item>
    <item>
      <title>In-Memory File System with FUSE</title>
      <link>https://bbengfort.github.io/2017/01/fuse-inmem-fs/</link>
      <pubDate>Mon, 30 Jan 2017 16:17:26 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/01/fuse-inmem-fs/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Filesystem_in_Userspace&#34;&gt;Filesystem in Userspace (FUSE)&lt;/a&gt; software interface allows developers to create file systems without editing kernel code. This is especially useful when creating replicated file systems, file protocols, backup systems, or other computer systems that require intervention for FS operations but not an entire operating system. FUSE works by running the FS code as a user process while FUSE provides a bridge through a request/response protocol to the kernel.&lt;/p&gt;
&lt;p&gt;In Go, the FUSE library is implemented by &lt;a href=&#34;https://github.com/bazil/fuse&#34;&gt;bazil.org/fuse&lt;/a&gt;. It is a from-scratch implementation of the kernel-userspace communication protocol and does not use the C library. The library has been excellent for research implementations, particularly because Go is such an excellent language (named &lt;a href=&#34;http://www.tiobe.com/tiobe-index/&#34;&gt;programming language of 2016&lt;/a&gt;). However, it does lead to some questions (particularly because of the questions in the Go documentation):&lt;/p&gt;</description>
    </item>
    <item>
      <title>FUSE Calls on Go Writes</title>
      <link>https://bbengfort.github.io/2017/01/fuse-calls/</link>
      <pubDate>Thu, 26 Jan 2017 20:04:40 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/01/fuse-calls/</guid>
      <description>&lt;p&gt;For close-to-open consistency, we need to be able to implement a file system that can detect atomic changes to a single file. Most programming languages implement &lt;code&gt;open()&lt;/code&gt; and &lt;code&gt;close()&lt;/code&gt; methods for files - but what they are really modifying is the access of a &lt;em&gt;handle&lt;/em&gt; to an open file that the operating system provides. Writes are buffered in an asynchronous fashion so that the operating system and user program don&amp;rsquo;t have to wait for the spinning disk to figure itself out before carrying on. Additional file calls such as &lt;code&gt;sync()&lt;/code&gt; and &lt;code&gt;flush()&lt;/code&gt; give the user the ability to hint to the OS about what should happen relative to the state of data and the disk, but the OS provides no guarantees that will happen.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Error Descriptions for System Calls</title>
      <link>https://bbengfort.github.io/2017/01/syscall-errno/</link>
      <pubDate>Mon, 23 Jan 2017 14:29:50 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/01/syscall-errno/</guid>
      <description>&lt;p&gt;Working with &lt;a href=&#34;https://bazil.org/fuse/&#34;&gt;FUSE&lt;/a&gt; to build file systems means inevitably you have to deal with (or return) system call errors. The &lt;a href=&#34;https://godoc.org/bazil.org/fuse#pkg-constants&#34;&gt;Go FUSE&lt;/a&gt; implementation includes helpers and constants for returning these errors, but simply wraps them around the &lt;a href=&#34;https://golang.org/pkg/syscall/#pkg-constants&#34;&gt;syscall&lt;/a&gt; error numbers. I needed descriptions to better understand what was doing what. Pete saved the day by pointing me towards the &lt;code&gt;errno.h&lt;/code&gt; header file on my Macbook. Some Python later and we had the descriptions:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Run Until Error with Go Channels</title>
      <link>https://bbengfort.github.io/2017/01/run-until-err/</link>
      <pubDate>Thu, 19 Jan 2017 11:00:40 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/01/run-until-err/</guid>
      <description>&lt;p&gt;Writing systems means the heavy use of go routines to support concurrent operations. My current architecture employs several go routines to run a server for a simple web interface as well as command line app, file system servers, replica servers, consensus coordination, etc. Using multiple go routines (threads) instead of processes allows for easier development and shared resources, such as a database that can support transactions. However, management of all these threads can be tricky.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generic JSON Serialization with Go</title>
      <link>https://bbengfort.github.io/2017/01/generic-json-serialization-go/</link>
      <pubDate>Wed, 18 Jan 2017 11:31:06 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/01/generic-json-serialization-go/</guid>
      <description>&lt;p&gt;This post is just a reminder as I work through handling JSON data with Go. Go provides first class JSON support through its standard library &lt;code&gt;json&lt;/code&gt; package. The interface is simple, primarily through &lt;code&gt;json.Marshal&lt;/code&gt; and &lt;code&gt;json.Unmarshal&lt;/code&gt; functions which are analagous to typed versions of &lt;code&gt;json.load&lt;/code&gt; and &lt;code&gt;json.dump&lt;/code&gt;. Type safety is the trick, however, and generally speaking you define a &lt;code&gt;struct&lt;/code&gt; to serialize and deserialize as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;Person&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nx&#34;&gt;Name&lt;/span&gt;   &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;`json:&amp;#34;name,omitempty&amp;#34;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nx&#34;&gt;Age&lt;/span&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt;    &lt;span class=&#34;s&#34;&gt;`json:&amp;#34;age,omitempty&amp;#34;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nx&#34;&gt;Salary&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt;    &lt;span class=&#34;s&#34;&gt;`json:&amp;#34;-&amp;#34;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;op&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Person&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;John Doe&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;42&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Marshal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;op&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;np&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;Person&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Unmarshall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So this is all well and good, until you start wanting to just send around arbirtray data. Luckly the &lt;code&gt;json&lt;/code&gt; package will allow you to do that using reflection to load data into a &lt;code&gt;map[string]interface{}&lt;/code&gt;, e.g. a dictionary whose keys are strings and whose values are any arbitrary type (anything that implements the null interface, that is has zero or more methods, which all Go types do). So you might see code like this:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Resolving Matplotlib Colors</title>
      <link>https://bbengfort.github.io/2017/01/resolving-matplotlib-colors/</link>
      <pubDate>Tue, 17 Jan 2017 14:52:50 +0000</pubDate>
      <guid>https://bbengfort.github.io/2017/01/resolving-matplotlib-colors/</guid>
      <description>&lt;p&gt;One of the challenges we&amp;rsquo;ve been dealing with in the Yellowbrick library is the proper resolution of colors, a problem that seems to have parallels in &lt;code&gt;matplotlib&lt;/code&gt; as well. The issue is that colors can be described by the user in a variety of ways, then that description has to be parsed and rendered as specific colors. To name a few color specifications that exist in &lt;code&gt;matplotlib&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;None: choose a reasonable default color&lt;/li&gt;
&lt;li&gt;The name of the color, e.g. &lt;code&gt;&amp;quot;b&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;blue&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The hex code of the color e.g. &lt;code&gt;&amp;quot;#377eb8&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The RGB or RGBA tuples of the color, e.g. &lt;code&gt;(0.0078, 0.4470, 0.6353)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A greyscale intensity string, e.g. &lt;code&gt;&amp;quot;0.76&amp;quot;&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;a href=&#34;http://matplotlib.org/api/pyplot_api.html&#34;&gt;pyplot api documentation&lt;/a&gt; sums it up as follows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Benchmarking Readline Iterators</title>
      <link>https://bbengfort.github.io/2016/12/benchmarking-readlines/</link>
      <pubDate>Fri, 23 Dec 2016 10:18:01 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/12/benchmarking-readlines/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m starting to get serious about programming in Go, trying to move from an intermediate level to an advanced/expert level as I start to build larger systems. Right now I&amp;rsquo;m working on a problem that involves on demand iteration, and I don&amp;rsquo;t want to pass around entire arrays and instead be a bit more frugal about my memory usage. Yesterday, I discussed using [channels to yield iterators from functions]({% post_url 2016-12-22-yielding-functions-for-iteration-golang %}) and was a big fan of the API, but had some questions about memory usage. So today I created a package, &lt;a href=&#34;https://github.com/bbengfort/iterfile&#34;&gt;iterfile&lt;/a&gt; to benchmark and profile various iteration constructs in Go.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yielding Functions for Iteration in Go</title>
      <link>https://bbengfort.github.io/2016/12/yielding-functions-for-iteration-golang/</link>
      <pubDate>Thu, 22 Dec 2016 06:54:26 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/12/yielding-functions-for-iteration-golang/</guid>
      <description>&lt;p&gt;It is very common for me to design code that expects functions to return an iterable context, particularly because I have been developing in Python with the &lt;code&gt;yield&lt;/code&gt; statement. The &lt;code&gt;yield&lt;/code&gt; statement allows functions to “return” the execution context to the caller while still maintaining state such that the caller can return state to the function and continue to iterate. It does this by actually returning a &lt;code&gt;generator&lt;/code&gt;, iterable object constructed from the local state of the closure.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Product Architectures: O&#39;Reilly Webinar</title>
      <link>https://bbengfort.github.io/2016/12/data-product-architectures-oreilly-webinar/</link>
      <pubDate>Wed, 07 Dec 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/12/data-product-architectures-oreilly-webinar/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.oreilly.com/pub/e/3800&#34;&gt;Data Product Architectures: O&amp;rsquo;Reilly Webinar&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Data products derive their value from data and generate new data in return. As a result, machine-learning techniques must be applied to their architecture and development. Machine learning fits models to make predictions on unknown inputs and must be generalizable and adaptable. As such, fitted models cannot exist in isolation; they must be operationalized and user facing so that applications can benefit from the new data, respond to it, and feed it back into the data product.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exception Handling</title>
      <link>https://bbengfort.github.io/2016/11/exception-handling/</link>
      <pubDate>Mon, 21 Nov 2016 12:53:30 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/11/exception-handling/</guid>
      <description>&lt;p&gt;This short tutorial is intended to demonstrate the basics of exception handling and the use of context management in order to handle standard cases. These notes were originally created for a training I gave, and the notebook can be found at &lt;a href=&#34;https://github.com/DistrictDataLabs/ceb-training/blob/master/notes/Exception%20Handling.ipynb&#34;&gt;Exception Handling&lt;/a&gt;. I&amp;rsquo;m happy for any comments or pull requests on the notebook.&lt;/p&gt;
&lt;h2 id=&#34;exceptions&#34;&gt;Exceptions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Exceptions&lt;/strong&gt; are a tool that programmers use to describe errors or faults that are &lt;em&gt;fatal&lt;/em&gt; to the program; e.g. the program cannot or should not continue when an exception occurs. Exceptions can occur due to programming errors, user errors, or simply unexpected conditions like no internet access. Exceptions themselves are simply objects that contain information about what went wrong. Exceptions are usually defined by their &lt;code&gt;type&lt;/code&gt; - which describes broadly the class of exception that occurred, and by a &lt;code&gt;message&lt;/code&gt; that says specifically what happened. Here are a few common exception types:&lt;/p&gt;</description>
    </item>
    <item>
      <title>SVG Vertex with a Timer</title>
      <link>https://bbengfort.github.io/2016/11/svg-timer-vertex/</link>
      <pubDate>Fri, 04 Nov 2016 10:30:29 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/11/svg-timer-vertex/</guid>
      <description>&lt;p&gt;In order to promote the use of graph data structures for data analysis, I&amp;rsquo;ve recently given talks on &lt;a href=&#34;https://youtu.be/RgixxVpfXDY&#34;&gt;dynamic graphs&lt;/a&gt;: embedding time into graph structures to analyze change. In order to embed time into a graph there are two primary mechanisms: make time a graph element (a vertex or an edge) or have multiple subgraphs where each graph represents a discrete time step. By using either of these techniques, opportunities exist to perform a structural analysis using graph algorithms on time; for example - asking what time is most central to a particular set of relationships.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Message Latency: Ping vs. gRPC</title>
      <link>https://bbengfort.github.io/2016/11/ping-vs-grpc/</link>
      <pubDate>Wed, 02 Nov 2016 15:46:31 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/11/ping-vs-grpc/</guid>
      <description>&lt;p&gt;Building distributed systems means passing messages between devices over a network connection. My research specifically considers networks that have extremely variable latencies or that can be partition prone. This led me to the natural question, “how variable are real world networks?” In order to get real numbers, I built a simple echo protocol using Go and gRPC called &lt;a href=&#34;https://github.com/bbengfort/orca&#34;&gt;Orca&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I ran Orca for a few days and got some latency measurements as I traveled around with my laptop. Orca does a lot of work, including GeoIP look ups, IP address resolution, and database queries and storage. This post, however, is not about Orca. The latencies I was getting were very high relative to the round-trip latencies reported by the simple &lt;code&gt;ping&lt;/code&gt; command that implements the &lt;a href=&#34;https://en.wikipedia.org/wiki/Internet_Control_Message_Protocol&#34;&gt;ICMP protocol&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Computing Reading Speed</title>
      <link>https://bbengfort.github.io/2016/10/reading-speed/</link>
      <pubDate>Fri, 28 Oct 2016 13:16:24 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/10/reading-speed/</guid>
      <description>&lt;p&gt;Ashley and I have been going over the &lt;a href=&#34;http://blog.districtdatalabs.com/&#34;&gt;District Data Labs Blog&lt;/a&gt; trying to figure out a method to make it more accessible both to readers (who are at various levels) and to encourage writers to contribute. To that end, she&amp;rsquo;s been exploring other blogs to see if we can put multiple forms of content up; long form tutorials (the bulk of what&amp;rsquo;s there) and shorter idea articles, possibly even as short as the posts I put on my dev journal. One interesting suggestion she had was to mark the reading time of each post, something that &lt;a href=&#34;http://bit.ly/2ePtm3z&#34;&gt; the Longreads Blog&lt;/a&gt; does. This may help give readers a better sense of the time committment and be able to engage more easily.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dynamics in Graph Analysis Adding Time as a Structure for Visual and Statistical Insight</title>
      <link>https://bbengfort.github.io/2016/10/dynamics-in-graph-analysis-adding-time-as-a-structure-for-visual-and-statistical-insight/</link>
      <pubDate>Mon, 24 Oct 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/10/dynamics-in-graph-analysis-adding-time-as-a-structure-for-visual-and-statistical-insight/</guid>
      <description>&lt;p&gt;I gave this talk twice, both at &lt;a href=&#34;http://pydata.org/dc2016/schedule/presentation/36/&#34;&gt;PyData DC&lt;/a&gt; on October 24, 2016 and at &lt;a href=&#34;http://pydata.org/carolinas2016/schedule/presentation/39/&#34;&gt;PyData Carolinas&lt;/a&gt; on September 15, 2016. Both videos are below if you feel like figuring out which presentation was better!&lt;/p&gt;
&lt;h3 id=&#34;pydata-dc&#34;&gt;PyData DC&lt;/h3&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/QhMZ1PmlJn4?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;pydata-carolinas&#34;&gt;PyData Carolinas&lt;/h3&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/RgixxVpfXDY?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;slides&#34;&gt;Slides&lt;/h3&gt;
&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/66065281?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Network analyses are powerful methods for both visual analytics and machine learning but can suffer as their complexity increases. By embedding time as a structural element rather than a property, we will explore how time series and interactive analysis can be improved on Graph structures. Primarily we will look at decomposition in NLP-extracted concept graphs using NetworkX and Graph Tool.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modifying an Image&#39;s Aspect Ratio</title>
      <link>https://bbengfort.github.io/2016/09/image-aspect-ratio/</link>
      <pubDate>Tue, 13 Sep 2016 14:19:14 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/09/image-aspect-ratio/</guid>
      <description>&lt;p&gt;When making slides, I generally like to use &lt;a href=&#34;https://www.flickr.com/&#34;&gt;Flickr&lt;/a&gt; to search for images that are licensed via &lt;a href=&#34;https://creativecommons.org/&#34;&gt;Creative Commons&lt;/a&gt; to use as backgrounds. My slide deck tools of choice are either &lt;a href=&#34;http://lab.hakim.se/reveal-js/#/&#34;&gt;Reveal.js&lt;/a&gt; or &lt;a href=&#34;https://www.google.com/slides/about/&#34;&gt;Google Slides&lt;/a&gt;. Both tools allow you to specify an image as a background for the slide, but for Google Slides in particular, if the aspect ratio of the image doesn&amp;rsquo;t match the aspect ratio of the slide deck, then weird things can happen.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Serializing GraphML</title>
      <link>https://bbengfort.github.io/2016/09/serialize-graphml/</link>
      <pubDate>Fri, 09 Sep 2016 17:13:13 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/09/serialize-graphml/</guid>
      <description>&lt;p&gt;This is mostly a post of annoyance. I&amp;rsquo;ve been working with graphs in Python via NetworkX and trying to serialize them to GraphML for use in Gephi and graph-tool. Unfortunately the following error is really starting to get on my nerves:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;networkx.exception.NetworkXError: GraphML writer does not support &amp;lt;class &amp;#39;datetime.datetime&amp;#39;&amp;gt; as data values.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Also it doesn&amp;rsquo;t support &lt;code&gt;&amp;lt;type NoneType&amp;gt;&lt;/code&gt; or &lt;code&gt;list&lt;/code&gt; or &lt;code&gt;dict&lt;/code&gt; or &amp;hellip;&lt;/p&gt;
&lt;p&gt;So I have to do something about it:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parallel Enqueue and Workers</title>
      <link>https://bbengfort.github.io/2016/09/parallel-enqueue-and-work/</link>
      <pubDate>Wed, 07 Sep 2016 14:29:51 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/09/parallel-enqueue-and-work/</guid>
      <description>&lt;p&gt;I was recently asked about the parallelization of both the enqueuing of tasks and their processing. This is a tricky subject because there are a lot of factors that come into play. For example do you have two parallel phases, e.g. a map and a reduce phase that need to be synchronized, or is there some sort of data parallelism that requires multiple tasks to be applied to the data (e.g. Storm-style topology). While there are a lot of tools for parallel processing in batch for large data sets, how do you take care of simple problems with large datasets (say hundreds of gigabytes) on a single machine with a quad core or hyperthreading multiprocessor?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parallel NLP Preprocessing</title>
      <link>https://bbengfort.github.io/2016/08/parallel-nlp-preprocessing/</link>
      <pubDate>Fri, 12 Aug 2016 22:09:25 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/08/parallel-nlp-preprocessing/</guid>
      <description>&lt;p&gt;A common source of natural language corpora comes from the web, usually in the form of HTML documents. However, in order to actually build models on the natural language, the structured HTML needs to be transformed into units of discourse that can then be used for learning. In particular, we need to strip away extraneous material such as navigation or advertisements, targeting exactly the content we&amp;rsquo;re looking for. Once done, we need to split paragraphs into sentences, sentences into tokens, and assign part-of-speech tags to each token. The preprocessing therefore transforms HTML documents to a list of paragraphs, which are themselves a list of sentences, which are lists of token, tag tuples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pretty Print Directories</title>
      <link>https://bbengfort.github.io/2016/08/pretty-print-directories/</link>
      <pubDate>Mon, 01 Aug 2016 12:17:47 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/08/pretty-print-directories/</guid>
      <description>&lt;p&gt;It feels like there are many questions like this one on Stack Overflow: &lt;a href=&#34;http://stackoverflow.com/questions/19699059/representing-directory-file-structure-in-markdown-syntax&#34;&gt;Representing Directory &amp;amp; File Structure in Markdown Syntax&lt;/a&gt;, basically asking &amp;ldquo;how can we represent a directory structure in text in a pleasant way?&amp;rdquo; I too use these types of text representations in slides, blog posts, books, etc. It would be very helpful if I had an automatic way of doing this so I didn&amp;rsquo;t have to create it from scratch.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Interview - Ben Bengfort of District Data Labs</title>
      <link>https://bbengfort.github.io/2016/07/interview-ben-bengfort-of-district-data-labs/</link>
      <pubDate>Thu, 28 Jul 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/07/interview-ben-bengfort-of-district-data-labs/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/ZiY5tjgg7lU?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;We talk to Benjamin Bengfort about his Data Day Seattle talks, District Data Labs, and Ben&amp;rsquo;s popular O&amp;rsquo;Reilly books.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Visualizing the Model Selection Process</title>
      <link>https://bbengfort.github.io/2016/07/visualizing-the-model-selection-process/</link>
      <pubDate>Sat, 23 Jul 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/07/visualizing-the-model-selection-process/</guid>
      <description>&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/64311820?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Machine learning is the hacker art of describing the features of instances that we want to make predictions about, then fitting the data that describes those instances to a model form. Applied machine learning has come a long way from it&amp;rsquo;s beginnings in academia, and with tools like Scikit-Learn, it&amp;rsquo;s easier than ever to generate operational models for a wide variety of applications. Thanks to the ease and variety of the tools in Scikit-Learn, the primary job of the data scientist is &lt;em&gt;model selection&lt;/em&gt;. Model selection involves performing feature engineering, hyperparameter tuning, and algorithm selection. These dimensions of machine learning often lead computer scientists towards automatic model selection via optimization (maximization) of a model&amp;rsquo;s evaluation metric. However, the search space is large, and grid search approaches to machine learning can easily lead to failure and frustration. Human intuition is still essential to machine learning, and visual analysis in concert with automatic methods can allow data scientists to steer model selection towards better fitted models, faster. In this talk, we will discuss interactive visual methods for better understanding, steering, and tuning machine learning models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Product Architectures: Seattle Data Day</title>
      <link>https://bbengfort.github.io/2016/07/data-product-architectures-seattle-data-day/</link>
      <pubDate>Thu, 21 Jul 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/07/data-product-architectures-seattle-data-day/</guid>
      <description>&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/64265501?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Data products derive their value from data and generate new data in return; as a result, machine learning techniques must be applied to their architecture and their development. Machine learning fits models to make predictions on unknown inputs and must be &lt;em&gt;generalizable&lt;/em&gt; and &lt;em&gt;adaptable&lt;/em&gt;. As such, fitted models cannot exist in isolation; they must be operationalized and user facing so that applications can benefit from the new data, respond to it, and feed it back in to the data product. Data product architectures are therefore &lt;em&gt;life cycles&lt;/em&gt; and understanding the data product life cycle will enable architects to develop robust, failure free workflows and applications. In this talk we will discuss the data product life cycle, explore how to engage a model build, evaluation, and selection phase with an operation and interaction phase. Following the lambda architecture, we will investigate wrapping a central computational store for speed and querying, as well as incorporating a discussion of monitoring, management, and data exploration for hypothesis driven development. From web applications to big data appliances; this architecture serves as a blueprint for handling data services of all sizes!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Color Map Utility</title>
      <link>https://bbengfort.github.io/2016/07/color-mapper/</link>
      <pubDate>Fri, 15 Jul 2016 16:28:11 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/07/color-mapper/</guid>
      <description>&lt;p&gt;Many of us are spoiled by the use of matplotlib&amp;rsquo;s &lt;a href=&#34;http://matplotlib.org/examples/color/colormaps_reference.html&#34;&gt;colormaps&lt;/a&gt; which allow you to specify a string or object name of a color map (e.g. &lt;code&gt;Blues&lt;/code&gt;) then simply pass in a range of nearly continuous values which are spread along the color map. However, using these color maps for categorical or discrete values (like the colors of nodes) can pose challenges as the colors may not be distinct enough for the representation you&amp;rsquo;re looking for.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Visualizing Normal Distributions</title>
      <link>https://bbengfort.github.io/2016/06/normal-distribution-viz/</link>
      <pubDate>Mon, 27 Jun 2016 08:28:07 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/06/normal-distribution-viz/</guid>
      <description>&lt;p&gt;Normal distributions are the backbone of random number generation for simulation. By selecting a mean (μ) and standard deviation (σ) you can generate simulated data representative of the types of models you&amp;rsquo;re trying to build (and certainly better than simple uniform random number generators). However, you might already be able to tell that selecting μ and σ is a little backward! Typically these metrics are computed from data, not used to describe data. As a result, utilities for tuning the behavior of your random number generators are simply not discussed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Background Work with Goroutines on a Timer</title>
      <link>https://bbengfort.github.io/2016/06/background-work-goroutines-timer/</link>
      <pubDate>Sun, 26 Jun 2016 06:52:38 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/06/background-work-goroutines-timer/</guid>
      <description>&lt;p&gt;As I&amp;rsquo;m moving deeper into my PhD, I&amp;rsquo;m getting into more Go programming for the systems that I&amp;rsquo;m building. One thing that I&amp;rsquo;m constantly doing is trying to create a background process that runs forever, and does some work at an interval. Concurrency in Go is native and therefore the use of threads and parallel processing is very simple, syntax-wise. However I am still solving problems that I wanted to make sure I recorded here.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Converting NetworkX to Graph-Tool</title>
      <link>https://bbengfort.github.io/2016/06/graph-tool-from-networkx/</link>
      <pubDate>Thu, 23 Jun 2016 19:21:58 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/06/graph-tool-from-networkx/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bbengfort.github.io/images/2016-06-23-graph-tool-viz.png&#34; alt=&#34;A Directed Graph Visualization Generated by Graph-Tool&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;This week I discovered &lt;a href=&#34;https://graph-tool.skewed.de/&#34;&gt;graph-tool&lt;/a&gt;, a Python library for network analysis and visualization that is implemented in C++ with Boost. As a result, it can quickly and efficiently perform manipulations, statistical analyses of Graphs, and draw them in a visual pleasing style. It&amp;rsquo;s like using Python with the performance of C++, and I was rightly excited:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;It&amp;#39;s a bear to get setup, but once you do things get pretty nice. Moving my network viz over to it now!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Natural Language Processing with NLTK and Gensim</title>
      <link>https://bbengfort.github.io/2016/05/natural-language-processing-with-nltk-and-gensim/</link>
      <pubDate>Mon, 30 May 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/05/natural-language-processing-with-nltk-and-gensim/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://us.pycon.org/2016/schedule/presentation/1597/&#34;&gt;Natural Language Processing with NLTK and Gensim&lt;/a&gt;&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/itKNpCPHq3I?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Natural Language Processing (NLP) is often taught at the academic level from the perspective of computational linguists. However, as data scientists, we have a richer view of the natural language world - unstructured data that by its very nature has latent information that is important to humans. NLP practitioners have benefited from machine learning techniques to unlock meaning from large corpora, and in this class we’ll explore how to do that particularly with Python, Gensim, and the Natural Language Toolkit (NLTK).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Text Classification with NLTK and Scikit-Learn</title>
      <link>https://bbengfort.github.io/2016/05/text-classification-nltk-sckit-learn/</link>
      <pubDate>Thu, 19 May 2016 08:06:40 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/05/text-classification-nltk-sckit-learn/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;This post is an early draft of expanded work that will eventually appear on the &lt;a href=&#34;http://blog.districtdatalabs.com/&#34;&gt;District Data Labs Blog&lt;/a&gt;. Your feedback is welcome, and you can submit your comments on the &lt;a href=&#34;https://github.com/bbengfort/bbengfort.github.io/issues/4&#34;&gt;draft GitHub issue&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I&amp;rsquo;ve often been asked which is better for text processing, NLTK or Scikit-Learn (and sometimes Gensim). The answer is that I use all three tools on a regular basis, but I often have a problem mixing and matching them or combining them in meaningful ways. In this post, I want to show how I use NLTK for preprocessing and tokenization, but then apply machine learning techniques (e.g. building a linear SVM using stochastic gradient descent) using Scikit-Learn. In a follow on post, I&amp;rsquo;ll talk about vectorizing text with word2vec for machine learning in Scikit-Learn.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Creating a Microservice in Go</title>
      <link>https://bbengfort.github.io/2016/05/a-microservice-in-go/</link>
      <pubDate>Wed, 11 May 2016 09:52:13 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/05/a-microservice-in-go/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bbengfort.github.io/images/2016-05-10-mora-architecture.png&#34; alt=&#34;The Mora Architecture Diagram&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;Yesterday I built my first &lt;a href=&#34;http://martinfowler.com/articles/microservices.html&#34;&gt;microservice&lt;/a&gt; (a RESTful API) using &lt;a href=&#34;https://golang.org/&#34;&gt;Go&lt;/a&gt;, and I wanted to collect a few of my thoughts on the experience here before I forgot them. The project, &lt;a href=&#34;https://github.com/bbengfort/scribo&#34;&gt;Scribo&lt;/a&gt;, is intended to aid in my research by collecting data about a specific network that I&amp;rsquo;m looking to build distributed systems for. I do have &lt;a href=&#34;https://mora-scribo.herokuapp.com/&#34;&gt;something running&lt;/a&gt;, which will need to evolve a lot, and it could be helpful to know where it started.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Extracting Diffs from Git with Python</title>
      <link>https://bbengfort.github.io/2016/05/git-diff-extract/</link>
      <pubDate>Fri, 06 May 2016 08:43:29 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/05/git-diff-extract/</guid>
      <description>&lt;p&gt;One of the first steps to performing analysis of Git repositories is extracting the changes over time, e.g. the Git log. This seems like it should be a very simple thing to do, as visualizations on GitHub and elsewhere show file change analyses through history on a commit by commit basis. Moreover, by using the &lt;a href=&#34;http://gitpython.readthedocs.io/en/stable/&#34;&gt;GitPython&lt;/a&gt; library you have direct access to Git repositories that is scriptable. Unfortunately, things aren&amp;rsquo;t as simple as that, so I present a snippet for extracting change information from a Repository.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Visualizing Distributed Systems</title>
      <link>https://bbengfort.github.io/2016/04/visualizing-distributed-systems/</link>
      <pubDate>Tue, 26 Apr 2016 11:34:42 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/04/visualizing-distributed-systems/</guid>
      <description>&lt;p&gt;As I&amp;rsquo;ve dug into my distributed systems research, one question keeps coming up:
&lt;em&gt;“How do you visualize distributed systems?”&lt;/em&gt; Distributed systems are &lt;a href=&#34;https://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/&#34;&gt;hard&lt;/a&gt;, so it feels like being able to visualize the data flow would go a long way to understanding them in detail and avoiding bugs. Unfortunately, the same things that make architecting distributed systems difficult also make them hard to visualize.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t have an answer to this question, unfortunately. However, in this post I&amp;rsquo;d like to state my requirements and highlight some visualizations that I think are important. Hopefully this will be the start of a more complete investigation or at least allow others to comment on what they&amp;rsquo;re doing and whether or not visualization is important.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Scikit-Learn Data Management: Bunches</title>
      <link>https://bbengfort.github.io/2016/04/bunch-data-management/</link>
      <pubDate>Tue, 19 Apr 2016 11:29:30 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/04/bunch-data-management/</guid>
      <description>&lt;p&gt;One large issue that I encounter in development with machine learning is the need to structure our data on disk in a way that we can load into Scikit-Learn in a repeatable fashion for continued analysis. My proposal is to use the &lt;code&gt;sklearn.datasets.base.Bunch&lt;/code&gt; object to load the data into data and target attributes respectively, similar to how Scikit-Learn&amp;rsquo;s toy datasets are structured. Using this object to manage our data will mirror the native API and allow us to easily copy and paste code that demonstrates classifiers and techniques with the built in datasets. Importantly, this API will also allow us to communicate to other developers and our future-selves exactly how to use the data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lessons in Discrete Event Simulation</title>
      <link>https://bbengfort.github.io/2016/04/lessons-in-discrete-event-simulation/</link>
      <pubDate>Fri, 15 Apr 2016 06:26:42 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/04/lessons-in-discrete-event-simulation/</guid>
      <description>&lt;p&gt;Part of my research involves the creation of large scale distributed systems, and while we do build these systems and deploy them, we do find that simulating them for development and research gives us an advantage in trying new things out. To that end, I employ &lt;a href=&#34;https://en.wikipedia.org/wiki/Discrete_event_simulation&#34;&gt;discrete event simulation&lt;/a&gt; (DES) using Python&amp;rsquo;s &lt;a href=&#34;https://simpy.readthedocs.org/en/latest/&#34;&gt;SimPy&lt;/a&gt; library to build very large simulations of distributed systems, such as the one I&amp;rsquo;ve built to inspect consistency patterns in variable latency, heterogenous, partition prone networks: &lt;a href=&#34;https://github.com/bbengfort/cloudscope&#34;&gt;CloudScope&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>NLTK Corpus Reader for Extracted Corpus</title>
      <link>https://bbengfort.github.io/2016/04/nltk-corpus-reader/</link>
      <pubDate>Mon, 11 Apr 2016 21:03:18 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/04/nltk-corpus-reader/</guid>
      <description>&lt;p&gt;Yesterday I wrote a blog about [extracting a corpus]({% post_url 2016-04-10-extract-ddl-corpus %}) from a directory containing Markdown, such as for a blog that is deployed with Silvrback or Jekyll. In this post, I&amp;rsquo;ll briefly show how to use the built in &lt;code&gt;CorpusReader&lt;/code&gt; objects in &lt;code&gt;nltk&lt;/code&gt; for streaming the data to the segmentation and tokenization preprocessing functions that are built into NLTK for performing analytics.&lt;/p&gt;
&lt;p&gt;The dataset that I&amp;rsquo;ll be working with is the &lt;a href=&#34;http://blog.districtdatalabs.com/&#34;&gt;District Data Labs Blog&lt;/a&gt;, in particular the state of the blog as of today. The dataset can be downloaded from the &lt;a href=&#34;http://bit.ly/ddl-blogs-corpus&#34;&gt;ddl corpus&lt;/a&gt;, which also has the code in this post for you to use to perform other analytics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Extracting the DDL Blog Corpus</title>
      <link>https://bbengfort.github.io/2016/04/extract-ddl-corpus/</link>
      <pubDate>Sun, 10 Apr 2016 06:44:28 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/04/extract-ddl-corpus/</guid>
      <description>&lt;p&gt;We have some simple text analyses coming up and as an example, I thought it might be nice to use the DDL blog corpus as a data set. There are relatively few DDL blogs, but they all are long with a lot of significant text and discourse. It might be interesting to try to do some lightweight analysis on them.&lt;/p&gt;
&lt;p&gt;So, how to extract the corpus? The &lt;a href=&#34;http://blog.districtdatalabs.com&#34;&gt;DDL blog&lt;/a&gt; is currently hosted on &lt;a href=&#34;https://www.silvrback.com/&#34;&gt;Silvrback&lt;/a&gt; which is designed for text-forward, distraction-free blogging. As a result, there isn&amp;rsquo;t a lot of cruft on the page. I considered doing a scraper that pulled the web pages down or using the RSS feed to do the data ingestion. After all, I wouldn&amp;rsquo;t have to do a lot of HTML cleaning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dispatching Types to Handler Methods</title>
      <link>https://bbengfort.github.io/2016/04/dispatching-types-handler-methods/</link>
      <pubDate>Tue, 05 Apr 2016 08:58:32 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/04/dispatching-types-handler-methods/</guid>
      <description>&lt;p&gt;A while I ago, I discussed the [observer pattern]({% post_url 2016-02-16-observer-pattern %}) for dispatching events based on a series of registered callbacks. In this post, I take a look at a similar, but very different methodology for dispatching based on type with pre-assigned handlers. For me, this is actually the more common pattern because the observer pattern is usually implemented as an API to outsider code. On the other hand, this type of dispatcher is usually a programmer&amp;rsquo;s pattern, used for development and decoupling.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Class Variables</title>
      <link>https://bbengfort.github.io/2016/04/class-variables/</link>
      <pubDate>Mon, 04 Apr 2016 19:52:46 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/04/class-variables/</guid>
      <description>&lt;p&gt;These snippets are just a short reminder of how class variables work in Python. I understand this topic a bit too well, I think; I always remember the gotchas and can&amp;rsquo;t remember which gotcha belongs to which important detail. I generally come up with the right answer then convince myself I&amp;rsquo;m wrong until I write a bit of code and experiment. Hopefully this snippet will shortcut that process.&lt;/p&gt;
&lt;p&gt;Consider the following class hierarchy:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simple Password Generation</title>
      <link>https://bbengfort.github.io/2016/03/password-generator/</link>
      <pubDate>Wed, 30 Mar 2016 09:07:21 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/03/password-generator/</guid>
      <description>&lt;p&gt;I was talking with &lt;a href=&#34;https://github.com/looselycoupled&#34;&gt;@looselycoupled&lt;/a&gt; the other day about how we generate passwords for use on websites. We both agree that every single domain should have its own password (to prevent one crack ruling all your Internets). However, we&amp;rsquo;ve both evolved on the method over time, and I&amp;rsquo;ve written a simple script that allows me to generate passwords using methodologies discussed in this post.&lt;/p&gt;
&lt;p&gt;In particular I use the generator to create passwords for &lt;a href=&#34;http://pwsafe.info/&#34;&gt;pwSafe&lt;/a&gt;, the tool I currently use for password management (due to its use of the &lt;a href=&#34;https://raw.githubusercontent.com/jpvasquez/PasswordSafe/master/docs/formatV3.txt&#34;&gt;open source database format&lt;/a&gt; created by &lt;a href=&#34;https://www.schneier.com/blog/archives/2005/06/password_safe.html&#34;&gt;Bruce Schneier&lt;/a&gt;). It is my hope that this script can be embedded directly into pwSafe, or at least allow me to write directly to the database; but for now I just copy and paste with the &lt;code&gt;pbcopy&lt;/code&gt; utility.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Visualizing Pi with matplotlib</title>
      <link>https://bbengfort.github.io/2016/03/pi-day/</link>
      <pubDate>Mon, 14 Mar 2016 10:56:57 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/03/pi-day/</guid>
      <description>&lt;p&gt;Happy Pi day! As is the tradition at the University of Maryland (and to a certain extent, in my family) we are celebrating March 14 with pie and Pi. A shoutout to &lt;a href=&#34;https://github.com/konstantinosx/&#34;&gt;@konstantinosx&lt;/a&gt; who, during last year&amp;rsquo;s Pi day, requested blueberry pie, which was the strangest pie request I&amp;rsquo;ve received for Pi day. Not that blueberry pie is strange, just that someone would want one so badly for Pi day (he got a mixed berry pie).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adding a Git Commit to Header Comments</title>
      <link>https://bbengfort.github.io/2016/03/git-version-id/</link>
      <pubDate>Tue, 08 Mar 2016 13:57:56 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/03/git-version-id/</guid>
      <description>&lt;p&gt;You may have seen the following type of header at the top of my source code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# main&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# short description&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Author:   Benjamin Bengfort &amp;lt;benjamin@bengfort.com&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Created:  Tue Mar 08 14:07:24 2016 -0500&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Copyright (C) 2016 Bengfort.com&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# For license information, see LICENSE.txt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# ID: main.py [] benjamin@bengfort.com $&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;All of this is pretty self explanatory with the exception of the final line. This final line is a throw back to Subversion actually, when you could add a &lt;code&gt;$Id$&lt;/code&gt; tag to your code, and Subversion would &lt;a href=&#34;http://www.startupcto.com/server-tech/subversion/setting-the-id-tag&#34;&gt;automatically populate&lt;/a&gt; it with something that looks like:&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Bengfort Toolkit</title>
      <link>https://bbengfort.github.io/2016/03/toolkit/</link>
      <pubDate>Mon, 07 Mar 2016 13:32:14 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/03/toolkit/</guid>
      <description>&lt;p&gt;Programming life has finally caused me to give into something that I&amp;rsquo;ve resisted for a while: the creation of a &lt;a href=&#34;https://github.com/bbengfort/toolkit&#34;&gt;Bengfort Toolkit&lt;/a&gt; and specifically a &lt;code&gt;benlib&lt;/code&gt;. This post is mostly a reminder that this toolkit now exists and that I spent valuable time creating it against my better judgement. And as a result, I should probably use it and update it.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve already written (whoops, I almost said “you&amp;rsquo;ve already read” but I know no one reads this) posts about tools that I use frequently including [clock.py]({% post_url 2016-01-12-codetime-and-clock %}) and [requires]({% post_url 2016-01-21-freezing-requirements %}). These things have been simply Python scripts that I&amp;rsquo;ve put in &lt;code&gt;~/bin&lt;/code&gt;, which is part of my &lt;code&gt;$PATH&lt;/code&gt;. These are too small or simple to require full blown repositories and PyPI listings on their own merit. Plus, I honestly believe that I&amp;rsquo;m the only one that uses them.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Anonymizing User Profile Data with Faker</title>
      <link>https://bbengfort.github.io/2016/02/anonymizing-profile-data/</link>
      <pubDate>Thu, 25 Feb 2016 12:32:54 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/02/anonymizing-profile-data/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;This post is an early draft of expanded work that will eventually appear on the &lt;a href=&#34;http://blog.districtdatalabs.com/&#34;&gt;District Data Labs Blog&lt;/a&gt;. Your feedback is welcome, and you can submit your comments on the &lt;a href=&#34;https://github.com/bbengfort/bbengfort.github.io/issues/3&#34;&gt;draft GitHub issue&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In order to learn (or teach) data science you need data (surprise!). The best libraries often come with a toy dataset to show examples and how the code works. However, nothing can replace an actual, non-trivial dataset for a tutorial or lesson because it provides for deep and meaningful further exploration. Non-trivial datasets can provide surprise and intuition in a way that toy datasets just cannot. Unfortunately, non-trivial datasets can be hard to find for a few reasons, but one common reason is that the dataset contains personally identifying information (PII).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Implementing the Observer Pattern with an Event System</title>
      <link>https://bbengfort.github.io/2016/02/observer-pattern/</link>
      <pubDate>Tue, 16 Feb 2016 07:24:04 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/02/observer-pattern/</guid>
      <description>&lt;p&gt;I was looking back through some old code (hoping to find a quick post before I got back to work) when I ran across a project I worked on called Mortar. Mortar was a simple daemon that ran in the background and watched a particular directory. When a file was added or removed from that directory, Mortar would notify other services or perform some other task (e.g. if it was integrated into a library). At the time, we used Mortar to keep an eye on FTP directories, and when a file was uploaded Mortar would move it to a staging directory based on who uploaded it, then do some work on the file.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running on Schedule</title>
      <link>https://bbengfort.github.io/2016/02/running-on-schedule/</link>
      <pubDate>Wed, 10 Feb 2016 09:50:33 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/02/running-on-schedule/</guid>
      <description>&lt;p&gt;Automation with Python is a lovely thing, particularly for very repetitive or long running tasks; but unfortunately someone still has to press the button to make it go. It feels like there should be an easy way to set up a program such that it runs routinely, in the background, without much human intervention. Daemonized services are the route to go in server land; but how do you routinely schedule a process to run on your local computer, which may or may not be turned off&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;#ros-footnote-1&#34;&gt;1&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;? Moreover, long running daemon processes seem expensive when you just want a quick job to execute routinely.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Iterators and Generators</title>
      <link>https://bbengfort.github.io/2016/02/iterators-generators/</link>
      <pubDate>Fri, 05 Feb 2016 22:47:15 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/02/iterators-generators/</guid>
      <description>&lt;p&gt;This post is an attempt to explain what iterators and generators are in Python, defend the &lt;code&gt;yield&lt;/code&gt; statement, and reveal why a library like &lt;a href=&#34;https://simpy.readthedocs.org/en/latest/&#34;&gt;SimPy&lt;/a&gt; is possible. But first some terminology (that specifically targets my friends who Java). &lt;em&gt;Iteration&lt;/em&gt; is a syntactic construct that implements a loop over an &lt;em&gt;iterable&lt;/em&gt; object. The &lt;code&gt;for&lt;/code&gt; statement provides &lt;em&gt;iteration&lt;/em&gt;, the &lt;code&gt;while&lt;/code&gt; statement may provide iteration. An &lt;em&gt;iterable&lt;/em&gt; object is something that implements the &lt;em&gt;iteration protocol&lt;/em&gt; (Java folks, read interface). A &lt;em&gt;generator&lt;/em&gt; is a function that produces a sequence of results instead of a single value and is designed to make writing &lt;em&gt;iterable&lt;/em&gt; objects easier.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On Interval Calls with Threading</title>
      <link>https://bbengfort.github.io/2016/02/intervals-with-threads/</link>
      <pubDate>Tue, 02 Feb 2016 20:43:07 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/02/intervals-with-threads/</guid>
      <description>&lt;p&gt;Event driven programming can be a wonderful thing, particularly when the execution of your code is dependent on user input. It is for this reason that JavaScript and other user facing languages implement very strong event based semantics. Many times event driven semantics depends on elapsed time (e.g. wait then execute). Python, however, does not provide a native &lt;code&gt;setTimeout&lt;/code&gt; or &lt;code&gt;setInterval&lt;/code&gt; that will allow you to call a function after a specific amount of time, or to call a function again and again at a specific interval.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Timeline Visualization with Matplotlib</title>
      <link>https://bbengfort.github.io/2016/01/timeline-visualization/</link>
      <pubDate>Thu, 28 Jan 2016 22:24:47 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/timeline-visualization/</guid>
      <description>&lt;p&gt;Several times it&amp;rsquo;s come up that I&amp;rsquo;ve needed to visualize a time sequence for a collection of events across multiple sources. Unlike a normal time series, events don&amp;rsquo;t necessarily have a &lt;em&gt;magnitude&lt;/em&gt;, e.g. a stock market series is a graph with a time and a price. Events simply have times, and possibly types.&lt;/p&gt;
&lt;p&gt;A one dimensional number line is still interesting in this case, because the frequency or density of events reveal patterns that might not easily be analyzed with non-visual methods. Moreover, if you have multiple sources, overlaying a timeline on each can show which is busier, when and possibly also demonstrate some effect or causality.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building a Console Utility with Commis</title>
      <link>https://bbengfort.github.io/2016/01/console-utility-commis/</link>
      <pubDate>Sat, 23 Jan 2016 08:46:18 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/console-utility-commis/</guid>
      <description>&lt;p&gt;Applications like &lt;a href=&#34;https://git-scm.com/&#34;&gt;Git&lt;/a&gt; or &lt;a href=&#34;https://docs.djangoproject.com/en/1.9/ref/django-admin/&#34;&gt;Django&amp;rsquo;s management utility&lt;/a&gt; provide a rich interaction between a software library and their users by exposing many subcommands from a single root command. This style of what is essentially better argument parsing simplifies the user experience by only forcing them to remember one primary command, and allows the exploration of the utility hierarchy by using &lt;code&gt;--help&lt;/code&gt; and other visibility mechanisms. Moreover, it allows the utility writer to decouple different commands or actions from each other.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Freezing Package Requirements</title>
      <link>https://bbengfort.github.io/2016/01/freezing-requirements/</link>
      <pubDate>Thu, 21 Jan 2016 10:23:06 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/freezing-requirements/</guid>
      <description>&lt;p&gt;I have a minor issue with freezing requirements, and so I put together a very complex solution. One that is documented here. Not 100% sure why this week is all about packaging, but there you go.&lt;/p&gt;
&lt;p&gt;First up, what is a &lt;a href=&#34;https://pip.readthedocs.org/en/stable/user_guide/#requirements-files&#34;&gt;requirement file&lt;/a&gt;? Basically they are a list of items that can be installed with &lt;code&gt;pip&lt;/code&gt; using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file therefore mostly serves as a list of arguments to the &lt;a href=&#34;https://pip.pypa.io/en/stable/reference/pip_install/&#34;&gt;&lt;code&gt;pip install&lt;/code&gt;&lt;/a&gt; command. The requirements file itself has a very &lt;a href=&#34;https://pip.readthedocs.org/en/stable/reference/pip_install/#requirements-file-format&#34;&gt;specific format&lt;/a&gt; and can be created by hand, but generally the &lt;a href=&#34;https://pip.pypa.io/en/stable/reference/pip_freeze/&#34;&gt;&lt;code&gt;pip freeze&lt;/code&gt;&lt;/a&gt; command is used to dump out the requirements as follows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Packaging Python Libraries with PyPI</title>
      <link>https://bbengfort.github.io/2016/01/packaging-with-pypi/</link>
      <pubDate>Wed, 20 Jan 2016 15:33:06 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/packaging-with-pypi/</guid>
      <description>&lt;p&gt;Package deployment is something that is so completely necessary, but such a pain in the butt that I avoid it a little bit. However to reuse code in Python and to do awesome things like &lt;code&gt;pip install mycode&lt;/code&gt;, you need to package it up and stick it on to PyPI (pronounced /pīˈpēˈī/ according to one site I read, though I still prefer /pīˈpī/). This process should be easy, but it&amp;rsquo;s detail oriented and there are only two good walk throughs (see links below).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Better JSON Encoding</title>
      <link>https://bbengfort.github.io/2016/01/better-json-encoding/</link>
      <pubDate>Tue, 19 Jan 2016 14:26:27 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/better-json-encoding/</guid>
      <description>&lt;p&gt;The topic of the day is a simple one: JSON serialization. Here is my question, if you have a data structure like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;json&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;datetime&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;now&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;now&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;range&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xrange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;42&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Why can&amp;rsquo;t you do something as simple as: &lt;code&gt;print json.dumps(data)&lt;/code&gt;? These are simple Python datetypes from the standard library. Granted serializing a datetime might have some complications, but JSON does have a datetime specification. Moreover, a generator is just an iterable, which can be put into memory as a list, which is exactly the kind of thing that JSON &lt;em&gt;likes&lt;/em&gt; to serialize. It feels like this should just work. Luckily, there is a solution to the problem as shown in the Gist below:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simple SQL Query Wrapper</title>
      <link>https://bbengfort.github.io/2016/01/query-factory/</link>
      <pubDate>Mon, 18 Jan 2016 10:52:00 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/query-factory/</guid>
      <description>&lt;p&gt;Programming with databases is a fact of life for any seasoned programmer (read, “worth their salt”). From embedded databases like SQLite and LevelDB to server databases like PostgreSQL, data management is a fundamental part of any significant project. The first thing I should say here is &lt;em&gt;skip the ORM and learn SQL&lt;/em&gt;. SQL is such a powerful tool to query and manage a database, and is far more performant thanks to 40 years of research and development.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The codetime and clock Commands</title>
      <link>https://bbengfort.github.io/2016/01/codetime-and-clock/</link>
      <pubDate>Tue, 12 Jan 2016 17:02:51 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/codetime-and-clock/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;ve pair programmed with me, you might have seen me type something to the following effect on my terminal, particularly if I have just created a new file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ codetime
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then somehow I can magically paste a formatted timestamp into the file! Well it&amp;rsquo;s not a mystery, in fact, it&amp;rsquo;s just a simple alias:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;alias&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;codetime&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;clock.py code | pbcopy&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Oh, well that&amp;rsquo;s easy — why the blog post? Hey, what&amp;rsquo;s &lt;code&gt;clock.py&lt;/code&gt;? A great question! This Python script is the &lt;em&gt;dumbest&lt;/em&gt; thing that I have ever written, that has become the most &lt;em&gt;useful&lt;/em&gt; tool that I use on a daily basis. Whenever there is a dumb to useful ratio like that, it&amp;rsquo;s blogging time. Here is &lt;code&gt;clock.py&lt;/code&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Wrapping the Logging Module</title>
      <link>https://bbengfort.github.io/2016/01/logging-mixin/</link>
      <pubDate>Mon, 11 Jan 2016 08:15:05 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/logging-mixin/</guid>
      <description>&lt;p&gt;The standard library &lt;code&gt;logging&lt;/code&gt; module is excellent. It is also quite tedious if you want to use it in a production system. In particular you have to figure out the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;configuration of the formatters, handlers, and loggers&lt;/li&gt;
&lt;li&gt;object management throughout the script (e.g. the &lt;code&gt;logging.getLogger&lt;/code&gt; function)&lt;/li&gt;
&lt;li&gt;adding extra context to log messages for more complex formatters&lt;/li&gt;
&lt;li&gt;handling and logging warnings (and to a lesser extent, exceptions)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;code&gt;logging&lt;/code&gt; module actually does &lt;em&gt;all&lt;/em&gt; of these things. The problem is that it doesn&amp;rsquo;t do them all at once for you, or with one single API. Therefore we typically go the route that we want to &lt;em&gt;wrap&lt;/em&gt; the logging module so that we can provide extra context on demand, as well as handle warnings with ease. Moreover, once we have a wrapped logger, we can do fun things like create mixins to put together classes that have loggers inside of them.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simple CLI Script with Argparse</title>
      <link>https://bbengfort.github.io/2016/01/simple-cli-argparse/</link>
      <pubDate>Sun, 10 Jan 2016 14:48:14 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/simple-cli-argparse/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s face it, most of the Python programs we write are going to be used from the command line. There are &lt;em&gt;tons&lt;/em&gt; of command line interface helper libraries out there. My preferred CLI method is the style of Django&amp;rsquo;s management utility. More on this later, when we hopefully publish a library that gives us that out of the box (we use it in many of our projects already).&lt;/p&gt;
&lt;p&gt;Sometimes though, you just want a simple CLI script. These days we use the standard library &lt;code&gt;argparse&lt;/code&gt; module to parse commands off the command line. Here is my basic script that I use for most of my projects:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Basic Python Project Files</title>
      <link>https://bbengfort.github.io/2016/01/project-start/</link>
      <pubDate>Sat, 09 Jan 2016 14:01:59 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/project-start/</guid>
      <description>&lt;p&gt;I don&amp;rsquo;t use project templates like &lt;a href=&#34;https://cookiecutter.readthedocs.org/en/latest/&#34;&gt;cookiecutter&lt;/a&gt;. I&amp;rsquo;m sure they&amp;rsquo;re fine, but when I start a new project I like to get a cup of coffee, go to my zen place and manually create the workspace. It gets me in the right place to code. Here&amp;rsquo;s the thing, &lt;a href=&#34;http://blog.districtdatalabs.com/how-to-develop-quality-python-code&#34;&gt;there is a right way to set up a Python project&lt;/a&gt;. Plus, I have a particular style for my repositories — particularly how I use Creative Commons &lt;a href=&#34;https://www.flickr.com/&#34;&gt;Flickr&lt;/a&gt; photos as the header for my README files.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Frequently Copied and Pasted</title>
      <link>https://bbengfort.github.io/2016/01/frequently-copy-pasted/</link>
      <pubDate>Fri, 08 Jan 2016 23:14:57 +0000</pubDate>
      <guid>https://bbengfort.github.io/2016/01/frequently-copy-pasted/</guid>
      <description>&lt;p&gt;I have a bit of catch up to do — and I think that this notepad and development journal is the perfect resource to do it. You see, I am &lt;em&gt;constantly&lt;/em&gt; copy and pasting code from other projects into the current project that I&amp;rsquo;m working on. Usually this takes the form of a problem that I had solved previously that has a similar domain to a new problem, but requires a slight amount of tweaking. Other times I am just doing the same task over and over again.&lt;/p&gt;</description>
    </item>
    <item>
      <title>One Big Gift Selection Algorithm</title>
      <link>https://bbengfort.github.io/2015/12/one-big-gift/</link>
      <pubDate>Fri, 25 Dec 2015 11:54:12 +0000</pubDate>
      <guid>https://bbengfort.github.io/2015/12/one-big-gift/</guid>
      <description>&lt;p&gt;My family does &amp;ldquo;one big gift&amp;rdquo; every Christmas; that is instead of everyone simply buying everyone else a smaller gift; every person is assigned to one other person to give them a single large gift. Selection of who gives what to who is a place of some (minor) conflict. Therefore we simply use a random algorithm. Unfortunately, apparently a uniform random sample of pairs is not enough, therefore we take 100 samples to vote for each combination to see who gets what as follows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Natural Language Processing and Hadoop</title>
      <link>https://bbengfort.github.io/2013/11/natural-language-processing-and-hadoop/</link>
      <pubDate>Sat, 02 Nov 2013 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2013/11/natural-language-processing-and-hadoop/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://strataconf.com/stratany2013/public/schedule/detail/30806&#34;&gt;Natural Language Processing and Hadoop&lt;/a&gt;&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/2642kr9-cB0?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Benjamin Bengfort and Sean Murphy discuss how NLP can be integrated with Hadoop to gain insights in big data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unbound Concepts: Columbia TechBreakfast Dec. 2012</title>
      <link>https://bbengfort.github.io/2012/12/unbound-concepts-columbia-techbreakfast/</link>
      <pubDate>Wed, 12 Dec 2012 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2012/12/unbound-concepts-columbia-techbreakfast/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/N-Bi_MwvZiY?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Presentation by the CTO of Unbound Concepts, Benjamin Bengfort, at the Columbia TechBreakfast 2012.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
