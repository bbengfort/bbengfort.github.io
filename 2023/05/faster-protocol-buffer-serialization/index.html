<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Faster Protocol Buffer Serialization | Libelli</title>
<meta name=keywords content><meta name=description content="Performance is key when building streaming gRPC services. When you&rsquo;re trying to maximize throughput (e.g. messages per second) benchmarking is essential to understanding where the bottlenecks in your application are.
However, as a start, you can pretty much guarantee that one bottleneck is going to be the serialization (marshaling) and deserialization (unmarshaling) of protocol buffer messages.
We have a use case where the server does not need all of the information in the message in order to process the message. E.g. we have header information such as IDs and client information that the server does need to update as part of processing. The other part of the message is data that needs to be saved to disk and does not have to be unmarshaled until it&rsquo;s read. However, our protocol buffer schema right now is &ldquo;flat&rdquo; — meaning that all fields whether they are required for processing or not are defined by a single protocol buffer message."><meta name=author content="Benjamin Bengfort"><link rel=canonical href=https://bbengfort.github.io/2023/05/faster-protocol-buffer-serialization/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://bbengfort.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bbengfort.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://bbengfort.github.io/icon.png><link rel=apple-touch-icon href=https://bbengfort.github.io/apple-touch-icon-precomposed.png><link rel=mask-icon href=https://bbengfort.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://bbengfort.github.io/2023/05/faster-protocol-buffer-serialization/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-D3BE7EHHVP"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D3BE7EHHVP")}</script><meta property="og:title" content="Faster Protocol Buffer Serialization"><meta property="og:description" content="Performance is key when building streaming gRPC services. When you&rsquo;re trying to maximize throughput (e.g. messages per second) benchmarking is essential to understanding where the bottlenecks in your application are.
However, as a start, you can pretty much guarantee that one bottleneck is going to be the serialization (marshaling) and deserialization (unmarshaling) of protocol buffer messages.
We have a use case where the server does not need all of the information in the message in order to process the message. E.g. we have header information such as IDs and client information that the server does need to update as part of processing. The other part of the message is data that needs to be saved to disk and does not have to be unmarshaled until it&rsquo;s read. However, our protocol buffer schema right now is &ldquo;flat&rdquo; — meaning that all fields whether they are required for processing or not are defined by a single protocol buffer message."><meta property="og:type" content="article"><meta property="og:url" content="https://bbengfort.github.io/2023/05/faster-protocol-buffer-serialization/"><meta property="og:image" content="https://bbengfort.github.io/bear.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-03T10:30:16-05:00"><meta property="article:modified_time" content="2023-05-03T10:30:16-05:00"><meta property="og:site_name" content="Libelli"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bbengfort.github.io/bear.png"><meta name=twitter:title content="Faster Protocol Buffer Serialization"><meta name=twitter:description content="Performance is key when building streaming gRPC services. When you&rsquo;re trying to maximize throughput (e.g. messages per second) benchmarking is essential to understanding where the bottlenecks in your application are.
However, as a start, you can pretty much guarantee that one bottleneck is going to be the serialization (marshaling) and deserialization (unmarshaling) of protocol buffer messages.
We have a use case where the server does not need all of the information in the message in order to process the message. E.g. we have header information such as IDs and client information that the server does need to update as part of processing. The other part of the message is data that needs to be saved to disk and does not have to be unmarshaled until it&rsquo;s read. However, our protocol buffer schema right now is &ldquo;flat&rdquo; — meaning that all fields whether they are required for processing or not are defined by a single protocol buffer message."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://bbengfort.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Faster Protocol Buffer Serialization","item":"https://bbengfort.github.io/2023/05/faster-protocol-buffer-serialization/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Faster Protocol Buffer Serialization","name":"Faster Protocol Buffer Serialization","description":"Performance is key when building streaming gRPC services. When you\u0026rsquo;re trying to maximize throughput (e.g. messages per second) benchmarking is essential to understanding where the bottlenecks in your application are.\nHowever, as a start, you can pretty much guarantee that one bottleneck is going to be the serialization (marshaling) and deserialization (unmarshaling) of protocol buffer messages.\nWe have a use case where the server does not need all of the information in the message in order to process the message. E.g. we have header information such as IDs and client information that the server does need to update as part of processing. The other part of the message is data that needs to be saved to disk and does not have to be unmarshaled until it\u0026rsquo;s read. However, our protocol buffer schema right now is \u0026ldquo;flat\u0026rdquo; — meaning that all fields whether they are required for processing or not are defined by a single protocol buffer message.\n","keywords":[],"articleBody":"Performance is key when building streaming gRPC services. When you’re trying to maximize throughput (e.g. messages per second) benchmarking is essential to understanding where the bottlenecks in your application are.\nHowever, as a start, you can pretty much guarantee that one bottleneck is going to be the serialization (marshaling) and deserialization (unmarshaling) of protocol buffer messages.\nWe have a use case where the server does not need all of the information in the message in order to process the message. E.g. we have header information such as IDs and client information that the server does need to update as part of processing. The other part of the message is data that needs to be saved to disk and does not have to be unmarshaled until it’s read. However, our protocol buffer schema right now is “flat” — meaning that all fields whether they are required for processing or not are defined by a single protocol buffer message.\nSo we thought - could we break the flat protocol buffer message into two parts with one part wrapping the other? E.g. the outer message contains just the information the server needs for processing and the inner message remains marshalled until it is needed? Would this increase the performance of marshaling and unmarshaling?\nThe answer surprised me — yes, sort of. In the figure below, smaller throughput is better (e.g. it is faster):\nThe event size in this case is the size of the inner message which is just bytes. When the event size is “small” (less than 16KiB) then having a wrapped message outperforms the flat message for both marshaling and unmarshaling. However, as the inner message gets larger, serialization of the flat message gets faster.\nMy hypothesis for this is that the serializing a fixed-size outer wrapper is a constant cost; but the serializer does still have to read the entire data field into memory. At some point the time that reading the data field into memory takes starts to outweigh the benefits of the wrapper object.\nAlso, don’t get me wrong; overall it will take longer to serialize the entire message. The client will have to first serialize the inner message, then the outer message, which will take longer on the client side; and when reading the inner message will have to be deserialized again. However, having the client do the work does increase the throughput of the server, so it’s worth it to us.\nAlso, in the case of unmarshaling when you have nested types, the number of allocs falls by the number of nested types in the inner struct - another bonus!\nThe full code for this benchmark can be found here:\nprotocol buffers benchmarks More detailed results:\nTiny/FlatMarshal-10 1945454\t607.1 ns/op\t1536 B/op\t1 allocs/op Tiny/WrappedMarshal-10 2869138\t418.6 ns/op\t1536 B/op\t1 allocs/op XSmall/FlatMarshal-10 1202126\t919.8 ns/op\t4864 B/op\t1 allocs/op XSmall/WrappedMarshal-10 1773110\t797.0 ns/op\t4864 B/op\t1 allocs/op Small/FlatMarshal-10 1000000\t1217 ns/op\t9472 B/op\t1 allocs/op Small/WrappedMarshal-10 1000000\t1076 ns/op\t9472 B/op\t1 allocs/op Medium/FlatMarshal-10 331611\t3792 ns/op\t40960 B/op\t1 allocs/op Medium/WrappedMarshal-10 335064\t3512 ns/op\t40960 B/op\t1 allocs/op Large/FlatMarshal-10 18306\t64584 ns/op\t1474560 B/op\t1 allocs/op Large/WrappedMarshal-10 16376\t69056 ns/op\t1474561 B/op\t1 allocs/op XLarge/FlatMarshal-10 7090\t167709 ns/op\t5251074 B/op\t1 allocs/op XLarge/WrappedMarshal-10 6192\t174831 ns/op\t5251075 B/op\t1 allocs/op Tiny/FlatUnmarshal-10 1000000\t1102 ns/op\t2280 B/op\t24 allocs/op Tiny/WrappedUnmarshal-10 1783885\t691.9 ns/op\t2008 B/op\t14 allocs/op XSmall/FlatUnmarshal-10 794436\t1528 ns/op\t5352 B/op\t24 allocs/op XSmall/WrappedUnmarshal-10 1256288\t942.3 ns/op\t5464 B/op\t14 allocs/op Small/FlatUnmarshal-10 631819\t1936 ns/op\t9448 B/op\t24 allocs/op Small/WrappedUnmarshal-10 986050\t1243 ns/op\t10072 B/op\t14 allocs/op Medium/FlatUnmarshal-10 320212\t3653 ns/op\t34024 B/op\t24 allocs/op Medium/WrappedUnmarshal-10 322702\t3714 ns/op\t41560 B/op\t14 allocs/op Large/FlatUnmarshal-10 21088\t52541 ns/op\t1475816 B/op\t24 allocs/op Large/WrappedUnmarshal-10 19327\t56723 ns/op\t1475160 B/op\t14 allocs/op XLarge/FlatUnmarshal-10 8012\t131589 ns/op\t5252329 B/op\t24 allocs/op XLarge/WrappedUnmarshal-10 8575\t146534 ns/op\t5251674 B/op\t14 allocs/op ","wordCount":"642","inLanguage":"en","image":"https://bbengfort.github.io/bear.png","datePublished":"2023-05-03T10:30:16-05:00","dateModified":"2023-05-03T10:30:16-05:00","author":{"@type":"Person","name":"Benjamin Bengfort"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bbengfort.github.io/2023/05/faster-protocol-buffer-serialization/"},"publisher":{"@type":"Organization","name":"Libelli","logo":{"@type":"ImageObject","url":"https://bbengfort.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://bbengfort.github.io/ accesskey=h title="Libelli (Alt + H)"><img src=https://bbengfort.github.io/icon.png alt aria-label=logo height=35>Libelli</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://bbengfort.github.io/archive/ title=archive><span>archive</span></a></li><li><a href=https://bbengfort.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://bbengfort.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://bbengfort.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://bbengfort.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://bbengfort.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Faster Protocol Buffer Serialization</h1><div class=post-meta><span title='2023-05-03 10:30:16 -0500 -0500'>May 3, 2023</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;642 words&nbsp;·&nbsp;Benjamin Bengfort&nbsp;|&nbsp;<a href=https://github.com/bbengfort/bbengfort.github.io/tree/main/content/posts/2023-05-03-faster-protocol-buffer-serialization.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>Performance is key when building streaming gRPC services. When you&rsquo;re trying to maximize throughput (e.g. messages per second) benchmarking is essential to understanding where the bottlenecks in your application are.</p><p>However, as a start, you can pretty much guarantee that one bottleneck is going to be the serialization (marshaling) and deserialization (unmarshaling) of protocol buffer messages.</p><p>We have a use case where the server does not need all of the information in the message in order to process the message. E.g. we have header information such as IDs and client information that the server does need to update as part of processing. The other part of the message is data that needs to be saved to disk and does not have to be unmarshaled until it&rsquo;s read. However, our protocol buffer schema right now is &ldquo;flat&rdquo; — meaning that all fields whether they are required for processing or not are defined by a single protocol buffer message.</p><p>So we thought - could we break the flat protocol buffer message into two parts with one part wrapping the other? E.g. the outer message contains just the information the server needs for processing and the inner message remains marshalled until it is needed? Would this increase the performance of marshaling and unmarshaling?</p><p>The answer surprised me — yes, sort of. In the figure below, smaller throughput is better (e.g. it is faster):</p><p><img loading=lazy src=/images/2023-05-03-serialization-benchmark.png alt="Serialization Benchmark"></p><p>The event size in this case is the size of the inner message which is just <code>bytes</code>. When the event size is &ldquo;small&rdquo; (less than 16KiB) then having a wrapped message outperforms the flat message for both marshaling and unmarshaling. However, as the inner message gets larger, serialization of the flat message gets faster.</p><p>My hypothesis for this is that the serializing a fixed-size outer wrapper is a constant cost; but the serializer does still have to read the entire data field into memory. At some point the time that reading the data field into memory takes starts to outweigh the benefits of the wrapper object.</p><p>Also, don&rsquo;t get me wrong; overall it will take longer to serialize the <em>entire</em> message. The client will have to first serialize the inner message, then the outer message, which will take longer on the client side; and when reading the inner message will have to be deserialized again. However, having the client do the work does increase the throughput of the server, so it&rsquo;s worth it to us.</p><p>Also, in the case of unmarshaling when you have nested types, the number of allocs falls by the number of nested types in the inner struct - another bonus!</p><p>The full code for this benchmark can be found here:</p><ul><li><a href=https://github.com/bbengfort/protobuf-oneof/blob/main/pb/wrapper.proto>protocol buffers</a></li><li><a href=https://github.com/bbengfort/protobuf-oneof/blob/main/wrapper_test.go>benchmarks</a></li></ul><p>More detailed results:</p><pre tabindex=0><code>Tiny/FlatMarshal-10         	 1945454	       607.1 ns/op	    1536 B/op	       1 allocs/op
Tiny/WrappedMarshal-10      	 2869138	       418.6 ns/op	    1536 B/op	       1 allocs/op
XSmall/FlatMarshal-10       	 1202126	       919.8 ns/op	    4864 B/op	       1 allocs/op
XSmall/WrappedMarshal-10    	 1773110	       797.0 ns/op	    4864 B/op	       1 allocs/op
Small/FlatMarshal-10        	 1000000	      1217 ns/op	    9472 B/op	       1 allocs/op
Small/WrappedMarshal-10     	 1000000	      1076 ns/op	    9472 B/op	       1 allocs/op
Medium/FlatMarshal-10       	  331611	      3792 ns/op	   40960 B/op	       1 allocs/op
Medium/WrappedMarshal-10    	  335064	      3512 ns/op	   40960 B/op	       1 allocs/op
Large/FlatMarshal-10        	   18306	     64584 ns/op	 1474560 B/op	       1 allocs/op
Large/WrappedMarshal-10     	   16376	     69056 ns/op	 1474561 B/op	       1 allocs/op
XLarge/FlatMarshal-10       	    7090	    167709 ns/op	 5251074 B/op	       1 allocs/op
XLarge/WrappedMarshal-10    	    6192	    174831 ns/op	 5251075 B/op	       1 allocs/op
</code></pre><pre tabindex=0><code>Tiny/FlatUnmarshal-10       	 1000000	      1102 ns/op	    2280 B/op	      24 allocs/op
Tiny/WrappedUnmarshal-10    	 1783885	       691.9 ns/op	    2008 B/op	      14 allocs/op
XSmall/FlatUnmarshal-10     	  794436	      1528 ns/op	    5352 B/op	      24 allocs/op
XSmall/WrappedUnmarshal-10  	 1256288	       942.3 ns/op	    5464 B/op	      14 allocs/op
Small/FlatUnmarshal-10      	  631819	      1936 ns/op	    9448 B/op	      24 allocs/op
Small/WrappedUnmarshal-10   	  986050	      1243 ns/op	   10072 B/op	      14 allocs/op
Medium/FlatUnmarshal-10     	  320212	      3653 ns/op	   34024 B/op	      24 allocs/op
Medium/WrappedUnmarshal-10  	  322702	      3714 ns/op	   41560 B/op	      14 allocs/op
Large/FlatUnmarshal-10      	   21088	     52541 ns/op	 1475816 B/op	      24 allocs/op
Large/WrappedUnmarshal-10   	   19327	     56723 ns/op	 1475160 B/op	      14 allocs/op
XLarge/FlatUnmarshal-10     	    8012	    131589 ns/op	 5252329 B/op	      24 allocs/op
XLarge/WrappedUnmarshal-10  	    8575	    146534 ns/op	 5251674 B/op	      14 allocs/op
</code></pre></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://bbengfort.github.io/2022/11/atomic-vs-mutex/><span class=title>Next »</span><br><span>Atomic vs Mutex</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://bbengfort.github.io/>Libelli</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>