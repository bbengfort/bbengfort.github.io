<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Synchronization in Write Throughput | Libelli</title><meta name=keywords content><meta name=description content="This post serves as a reminder of how to perform benchmarks when accounting for synchronized writing in Go. The normal benchmarking process involves running a command a large number of times and determining the average amount of time that operation took. When threads come into play, we consider throughput - that is the number of operations that can be conducted per second. However, in order to successfully measure this without duplicating time, the throughput must be measured from the server&rsquo;s perspective."><meta name=author content="Benjamin Bengfort"><link rel=canonical href=https://bbengfort.github.io/2018/02/sync-write-throughput/><link crossorigin=anonymous href=/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://bbengfort.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bbengfort.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://bbengfort.github.io/icon.png><link rel=apple-touch-icon href=https://bbengfort.github.io/apple-touch-icon-precomposed.png><link rel=mask-icon href=https://bbengfort.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-D3BE7EHHVP"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D3BE7EHHVP",{anonymize_ip:!1})}</script><meta property="og:title" content="Synchronization in Write Throughput"><meta property="og:description" content="This post serves as a reminder of how to perform benchmarks when accounting for synchronized writing in Go. The normal benchmarking process involves running a command a large number of times and determining the average amount of time that operation took. When threads come into play, we consider throughput - that is the number of operations that can be conducted per second. However, in order to successfully measure this without duplicating time, the throughput must be measured from the server&rsquo;s perspective."><meta property="og:type" content="article"><meta property="og:url" content="https://bbengfort.github.io/2018/02/sync-write-throughput/"><meta property="og:image" content="https://bbengfort.github.io/bear.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-02-13T07:10:06+00:00"><meta property="article:modified_time" content="2018-02-13T07:10:06+00:00"><meta property="og:site_name" content="Libelli"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bbengfort.github.io/bear.png"><meta name=twitter:title content="Synchronization in Write Throughput"><meta name=twitter:description content="This post serves as a reminder of how to perform benchmarks when accounting for synchronized writing in Go. The normal benchmarking process involves running a command a large number of times and determining the average amount of time that operation took. When threads come into play, we consider throughput - that is the number of operations that can be conducted per second. However, in order to successfully measure this without duplicating time, the throughput must be measured from the server&rsquo;s perspective."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://bbengfort.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Synchronization in Write Throughput","item":"https://bbengfort.github.io/2018/02/sync-write-throughput/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Synchronization in Write Throughput","name":"Synchronization in Write Throughput","description":"This post serves as a reminder of how to perform benchmarks when accounting for synchronized writing in Go. The normal benchmarking process involves running a command a large number of times and determining the average amount of time that operation took. When threads come into play, we consider throughput - that is the number of operations that can be conducted per second. However, in order to successfully measure this without duplicating time, the throughput must be measured from the server\u0026rsquo;s perspective.","keywords":[],"articleBody":"This post serves as a reminder of how to perform benchmarks when accounting for synchronized writing in Go. The normal benchmarking process involves running a command a large number of times and determining the average amount of time that operation took. When threads come into play, we consider throughput - that is the number of operations that can be conducted per second. However, in order to successfully measure this without duplicating time, the throughput must be measured from the server’s perspective.\nLet w be the amount of time a single operation takes and n be the number of operations per thread. Given t threads, the cost for each operation from the perspective of the client thread will be t*w because the server is synchronizing writes as shown in the figure above, e.g. the thread has to wait for t-1 other writes to complete before conducting it’s write. This means that each thread returns a latency of n*t*w from it’s perspective. If this is aggregated, the total time is computed n*w*t^2, even though the real time that has passed is actually n*t*w as shown in the single threaded case.\nFor normal server-client throughput we measure the start time of the first access on the server end and the end time of the last access and the duration as the difference between these two timestamps. We then compute the number of operations conducted in that time to measure throughput. This only works if the server is pegged, e.g. it is not waiting for requests.\nHowever, for synchronization we can’t measure at the server since we’re trying to determine the cost of locks and scheduling. We’ve used an external benchmarking procedure that may underestimate throughput but allows us to measure from the client-side rather than at the server. I’ve put up a simple library called syncwrite to benchmark this code.\nHere is the use case: consider a system that has multiple goroutines, each of which want to append to a log file on disk. The append-only log determines the sequence or ordering of events in the system, so appends must be atomic and reads to an index in the log, idempotent. The interface of the Log is as follows:\ntype Log interface { Open(path string) error Append(value []byte) error Get(index int) (*Entry, error) Close() error } The log embeds sync.RWMutex to ensure no race conditions occur and that our read/write invariants are met. The Open, Append, and Close methods are all protected by a write lock, and the Get method is protected with a read lock. The structs that implement Log all deal with the disk and in-memory storage in different ways:\nInMemoryLog: appends to an in-memory slice and does not write to disk. FileLog: on open, reads entries from file into in-memory slice and reads from it, writes append to both the slice and the file. LevelDBLog: both writes and reads go to a LevelDB database. In the future (probably in the next post), I will also implement an AsyncLog that wraps a Log and causes write operations to be asynchronous by storing them on a channel and allowing the goroutine to immediately return.\nBenchmarks The benchmarks are associated with an action, which can be one or more operations to disk. In this benchmark we simply evaluate an action that calls the Write method of a log with \"foo\" as the value. Per-action benchmarks are computed using go-bench, which computes the average time it takes to run the action once:\nBenchmarkInMemoryLog-8 10000000\t210 ns/op BenchmarkFileLog-8 200000\t7456 ns/op BenchmarkLevelDBLog-8 100000\t12379 ns/op Writing to the in-memory log is by far the fastest, while writing to the LevelDB log is by far the slowest operation. We expect throughput, that is the number of operations per second, to be equivalent with these per-action benchmarks in a single thread. The theoretical throughput is simply 1/w*1e-9 (converting nanoseconds to seconds). The question is how throughput changes with more threads and in a real workload.\nThroughput benchmarks are conducted by running t threads, each of which run n actions and returns the amount of time it takes all threads to run n*t actions. As the t increases, the workload stays static, e.g. n becomes smaller to keep n*t constant. The throughput is the number of operations divided by the duration in seconds.\nNote that the y-axis is on a logarithm scale, and because of the magnitude of in-memory writes, the chart is a bit difficult to read. Therefore the next chart shows the percentage of the theoretical throughput (as computed by w) the real system achieves:\nObservations Looking at the percent theoretical chart, I believe the reason that both In-Memory and LevelDB achieve \u003e100% for up to 4 threads is because the benchmark that computes w has such a high variability; though this does not explain why the File log has dramatically lower throughput.\nBecause the benchmarks were run on a 4 core machine, up to 4 threads for In-Memory and LevelDB can operate without a noticeable decrease in throughput since there is no scheduling issue. However, at 8 threads and above, there is a noticeable drop in the percent of theoretical throughput, probably due to synchronization or scheduling issues. This same drop does not occur in the File log because it was already below it’s theoretical maximum throughput.\n","wordCount":"882","inLanguage":"en","datePublished":"2018-02-13T07:10:06Z","dateModified":"2018-02-13T07:10:06Z","author":{"@type":"Person","name":"Benjamin Bengfort"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bbengfort.github.io/2018/02/sync-write-throughput/"},"publisher":{"@type":"Organization","name":"Libelli","logo":{"@type":"ImageObject","url":"https://bbengfort.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://bbengfort.github.io accesskey=h title="Libelli (Alt + H)"><img src=https://bbengfort.github.io/icon.png alt aria-label=logo height=35>Libelli</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://bbengfort.github.io/archive/ title=archive><span>archive</span></a></li><li><a href=https://bbengfort.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://bbengfort.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://bbengfort.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://bbengfort.github.io>Home</a>&nbsp;»&nbsp;<a href=https://bbengfort.github.io/posts/>Posts</a></div><h1 class=post-title>Synchronization in Write Throughput</h1><div class=post-meta><span title='2018-02-13 07:10:06 +0000 UTC'>February 13, 2018</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;882 words&nbsp;·&nbsp;Benjamin Bengfort&nbsp;|&nbsp;<a href=https://github.com/bbengfort/bbengfort.github.io/tree/main/content/posts/2018-02-13-sync-write-throughput.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>This post serves as a reminder of how to perform benchmarks when accounting for synchronized writing in Go. The normal benchmarking process involves running a command a large number of times and determining the average amount of time that operation took. When threads come into play, we consider <em>throughput</em> - that is the number of operations that can be conducted per second. However, in order to successfully measure this without duplicating time, the throughput must be measured from the server&rsquo;s perspective.</p><p><img loading=lazy src=/images/2018-02-13-syncwrite-overlapping.png alt="Time in Multiple Threads"></p><p>Let <code>w</code> be the amount of time a single operation takes and <code>n</code> be the number of operations per thread. Given <code>t</code> threads, the cost for each operation from the perspective of the client thread will be <code>t*w</code> because the server is synchronizing writes as shown in the figure above, e.g. the thread has to wait for <code>t-1</code> other writes to complete before conducting it&rsquo;s write. This means that each thread returns a latency of <code>n*t*w</code> from it&rsquo;s perspective. If this is aggregated, the total time is computed <code>n*w*t^2</code>, even though the real time that has passed is actually <code>n*t*w</code> as shown in the single threaded case.</p><p>For normal server-client throughput we measure the start time of the first access on the server end and the end time of the last access and the duration as the difference between these two timestamps. We then compute the number of operations conducted in that time to measure throughput. This only works if the server is pegged, e.g. it is not waiting for requests.</p><p>However, for synchronization we can&rsquo;t measure at the server since we&rsquo;re trying to determine the cost of locks and scheduling. We&rsquo;ve used an external benchmarking procedure that may underestimate throughput but allows us to measure from the client-side rather than at the server. I&rsquo;ve put up a simple library called <a href=https://github.com/bbengfort/syncwrite>syncwrite</a> to benchmark this code.</p><p>Here is the use case: consider a system that has multiple goroutines, each of which want to append to a log file on disk. The append-only log determines the sequence or ordering of events in the system, so appends must be atomic and reads to an index in the log, idempotent. The interface of the Log is as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>type</span> <span class=nx>Log</span> <span class=kd>interface</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=nf>Open</span><span class=p>(</span><span class=nx>path</span> <span class=kt>string</span><span class=p>)</span> <span class=kt>error</span>
</span></span><span class=line><span class=cl>	<span class=nf>Append</span><span class=p>(</span><span class=nx>value</span> <span class=p>[]</span><span class=kt>byte</span><span class=p>)</span> <span class=kt>error</span>
</span></span><span class=line><span class=cl>	<span class=nf>Get</span><span class=p>(</span><span class=nx>index</span> <span class=kt>int</span><span class=p>)</span> <span class=p>(</span><span class=o>*</span><span class=nx>Entry</span><span class=p>,</span> <span class=kt>error</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=nf>Close</span><span class=p>()</span> <span class=kt>error</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The log embeds <code>sync.RWMutex</code> to ensure no race conditions occur and that our read/write invariants are met. The <code>Open</code>, <code>Append</code>, and <code>Close</code> methods are all protected by a write lock, and the <code>Get</code> method is protected with a read lock. The structs that implement <code>Log</code> all deal with the disk and in-memory storage in different ways:</p><ul><li><code>InMemoryLog</code>: appends to an in-memory slice and does not write to disk.</li><li><code>FileLog</code>: on open, reads entries from file into in-memory slice and reads from it, writes append to both the slice and the file.</li><li><code>LevelDBLog</code>: both writes and reads go to a LevelDB database.</li></ul><p>In the future (probably in the next post), I will also implement an <code>AsyncLog</code> that wraps a Log and causes write operations to be asynchronous by storing them on a channel and allowing the goroutine to immediately return.</p><h2 id=benchmarks>Benchmarks<a hidden class=anchor aria-hidden=true href=#benchmarks>#</a></h2><p>The benchmarks are associated with an <em>action</em>, which can be one or more operations to disk. In this benchmark we simply evaluate an action that calls the <code>Write</code> method of a log with <code>"foo"</code> as the value. Per-action benchmarks are computed using <code>go-bench</code>, which computes the average time it takes to run the action once:</p><pre tabindex=0><code>BenchmarkInMemoryLog-8   	10000000	       210 ns/op
BenchmarkFileLog-8       	  200000	      7456 ns/op
BenchmarkLevelDBLog-8    	  100000	     12379 ns/op
</code></pre><p>Writing to the in-memory log is by far the fastest, while writing to the LevelDB log is by far the slowest operation. We expect <em>throughput</em>, that is the number of operations per second, to be equivalent with these per-action benchmarks in a single thread. The theoretical throughput is simply <code>1/w*1e-9</code> (converting nanoseconds to seconds). The question is how throughput changes with more threads and in a real workload.</p><p>Throughput benchmarks are conducted by running <code>t</code> threads, each of which run <code>n</code> actions and returns the amount of time it takes <em>all threads</em> to run <code>n*t</code> actions. As the <code>t</code> increases, the workload stays static, e.g. <code>n</code> becomes smaller to keep <code>n*t</code> constant. The throughput is the number of operations divided by the duration in seconds.</p><p><img loading=lazy src=/images/2018-02-13-sync-write-throughput.png alt="Sync Write Throughput with Increasing # of Threads"></p><p>Note that the y-axis is on a logarithm scale, and because of the magnitude of in-memory writes, the chart is a bit difficult to read. Therefore the next chart shows the percentage of the theoretical throughput (as computed by <code>w</code>) the real system achieves:</p><p><img loading=lazy src=/images/2018-02-03-percent-theoretical.png alt="Percentage of Theoretical"></p><h2 id=observations>Observations<a hidden class=anchor aria-hidden=true href=#observations>#</a></h2><p>Looking at the percent theoretical chart, I believe the reason that both In-Memory and LevelDB achieve >100% for up to 4 threads is because the benchmark that computes <code>w</code> has such a high variability; though this does not explain why the File log has dramatically lower throughput.</p><p>Because the benchmarks were run on a 4 core machine, up to 4 threads for In-Memory and LevelDB can operate without a noticeable decrease in throughput since there is no scheduling issue. However, at 8 threads and above, there is a noticeable drop in the percent of theoretical throughput, probably due to synchronization or scheduling issues. This same drop does not occur in the File log because it was already below it&rsquo;s theoretical maximum throughput.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://bbengfort.github.io/2018/02/prediction-distribution/><span class=title>« Prev</span><br><span>Class Balance Prediction Distribution</span></a>
<a class=next href=https://bbengfort.github.io/2018/01/go-set/><span class=title>Next »</span><br><span>Thread and Non-Thread Safe Go Set</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://bbengfort.github.io>Libelli</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>