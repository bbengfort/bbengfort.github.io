<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NLTK Corpus Reader for Extracted Corpus | Libelli</title><meta name=keywords content><meta name=description content="Yesterday I wrote a blog about [extracting a corpus]({% post_url 2016-04-10-extract-ddl-corpus %}) from a directory containing Markdown, such as for a blog that is deployed with Silvrback or Jekyll. In this post, I&rsquo;ll briefly show how to use the built in CorpusReader objects in nltk for streaming the data to the segmentation and tokenization preprocessing functions that are built into NLTK for performing analytics.
The dataset that I&rsquo;ll be working with is the District Data Labs Blog, in particular the state of the blog as of today."><meta name=author content="Benjamin Bengfort"><link rel=canonical href=https://bbengfort.github.io/2016/04/nltk-corpus-reader/><link crossorigin=anonymous href=/assets/css/stylesheet.min.2d6dbfc6e0f8a1db1c9d082a76dc11d094328cf63f247bbc2421dfaa7f2bb170.css integrity="sha256-LW2/xuD4odscnQgqdtwR0JQyjPY/JHu8JCHfqn8rsXA=" rel="preload stylesheet" as=style><link rel=preload href=/icon.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://bbengfort.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bbengfort.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://bbengfort.github.io/icon.png><link rel=apple-touch-icon href=https://bbengfort.github.io/apple-touch-icon-precomposed.png><link rel=mask-icon href=https://bbengfort.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.110.0"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-8096804-11","auto"),ga("send","pageview"))</script><meta property="og:title" content="NLTK Corpus Reader for Extracted Corpus"><meta property="og:description" content="Yesterday I wrote a blog about [extracting a corpus]({% post_url 2016-04-10-extract-ddl-corpus %}) from a directory containing Markdown, such as for a blog that is deployed with Silvrback or Jekyll. In this post, I&rsquo;ll briefly show how to use the built in CorpusReader objects in nltk for streaming the data to the segmentation and tokenization preprocessing functions that are built into NLTK for performing analytics.
The dataset that I&rsquo;ll be working with is the District Data Labs Blog, in particular the state of the blog as of today."><meta property="og:type" content="article"><meta property="og:url" content="https://bbengfort.github.io/2016/04/nltk-corpus-reader/"><meta property="og:image" content="https://bbengfort.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-04-11T21:03:18+00:00"><meta property="article:modified_time" content="2016-04-11T21:03:18+00:00"><meta property="og:site_name" content="Libelli"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bbengfort.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="NLTK Corpus Reader for Extracted Corpus"><meta name=twitter:description content="Yesterday I wrote a blog about [extracting a corpus]({% post_url 2016-04-10-extract-ddl-corpus %}) from a directory containing Markdown, such as for a blog that is deployed with Silvrback or Jekyll. In this post, I&rsquo;ll briefly show how to use the built in CorpusReader objects in nltk for streaming the data to the segmentation and tokenization preprocessing functions that are built into NLTK for performing analytics.
The dataset that I&rsquo;ll be working with is the District Data Labs Blog, in particular the state of the blog as of today."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://bbengfort.github.io/posts/"},{"@type":"ListItem","position":3,"name":"NLTK Corpus Reader for Extracted Corpus","item":"https://bbengfort.github.io/2016/04/nltk-corpus-reader/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NLTK Corpus Reader for Extracted Corpus","name":"NLTK Corpus Reader for Extracted Corpus","description":"Yesterday I wrote a blog about [extracting a corpus]({% post_url 2016-04-10-extract-ddl-corpus %}) from a directory containing Markdown, such as for a blog that is deployed with Silvrback or Jekyll. In this post, I\u0026rsquo;ll briefly show how to use the built in CorpusReader objects in nltk for streaming the data to the segmentation and tokenization preprocessing functions that are built into NLTK for performing analytics.\nThe dataset that I\u0026rsquo;ll be working with is the District Data Labs Blog, in particular the state of the blog as of today.","keywords":[],"articleBody":"Yesterday I wrote a blog about [extracting a corpus]({% post_url 2016-04-10-extract-ddl-corpus %}) from a directory containing Markdown, such as for a blog that is deployed with Silvrback or Jekyll. In this post, I’ll briefly show how to use the built in CorpusReader objects in nltk for streaming the data to the segmentation and tokenization preprocessing functions that are built into NLTK for performing analytics.\nThe dataset that I’ll be working with is the District Data Labs Blog, in particular the state of the blog as of today. The dataset can be downloaded from the ddl corpus, which also has the code in this post for you to use to perform other analytics.\nThe mdec.py program extracted our corpus in two formats: html and text. It also setup the corpus as follows:\nREADME describing the corpus (no extension) all text files in the same directory with the .txt or .html extension If this had been a categorized corpus, then we would have created subdirectories for each category in the corpus, and placed the correct files there. This organization has important implications for using the base readers without too much extension. Plus it helps others understand how to set up corpora with ease.\nReading Corpora NLTK’s CorpusReader objects provide a useful interface to streaming, end-to-end reads of a text corpus from multiple files on disk. To construct a corpus you need to pass the path to the directory containing the corpus, as well as a pattern for a regular expression matching the files that belong to the corpus. By default the CorpusReader opens everything with UTF-8 encoding and generally provides the following descriptive methods:\nreadme(): returns the contents of a README file citation(): returns the contents of a citation.bib file license(): returns the contents of a LICENSE file Generally speaking, your corpora should include all of these meta data files in the root directory in order to be considered complete.\nThere are many types of CorpusReader subclasses available in NLTK. The base classes provide readers for syntax corpora (those that are already structured as parses), bracket corpora (already part of speech tagged), and categorized corpora (documents associated with specific files). There are also a host of readers for the specific corpora that come included with NLTK. In general, these readers should provide an API that contain the following methods:\nparas(): returns an iterable of paragraphs (a list of lists of sentences) sents(): returns an iterable of sentences (a list of lists of words) words(): returns an iterable of words (a list of words) raw(): simply returns the raw text from the corpus Most CorpusReader classes can be accessed and filtered by a specific file or category or a list of files or categories. There are two primary methods for listing these if available to the corpus:\nfileids(): lists the names of the files that are in the corpus categories(): lists the names of the categories in the corpus This listing of API methods is by no means comprehensive. However, for most of the text analytics you’ll be doing, these methods will do the bulk of the work. I would consider a CorpusReader complete if it contained all of these methods.\nReading the Text Corpus The simplest thing to do is read our plaintext corpus, as we have to write no code to do so. Instead we can simply use the nltk.corpus.PlaintextCorpusReader directly, instantiating it with the correct path and pattern for our files. For the DDL corpus this looks like something as follows:\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader corpus = PlaintextCorpusReader(CORPUS_TEXT, '.*\\.txt') That’s it! As long as we path it a correct path to the corpus and a pattern for identifying text files, then we’re good to go! Note that the pattern is formatted as a Python regular expression, hence the escaped . – unfortunately NLTK doesn’t use glob or other patterns for file identification.\nWe can now print out some information about our corpus using the reader directly:\nfrom nltk import FreqDist def corpus_info(corpus): \"\"\" Prints out information about the status of a corpus. \"\"\" fids = len(corpus.fileids()) paras = len(corpus.paras()) sents = len(corpus.sents()) sperp = sum(len(para) for para in corpus.paras()) / float(paras) tokens = FreqDist(corpus.words()) count = sum(tokens.values()) vocab = len(tokens) lexdiv = float(count) / float(vocab) print(( \"Text corpus contains {} files\\n\" \"Composed of {} paragraphs and {} sentences.\\n\" \"{:0.3f} sentences per paragraph\\n\" \"Word count of {} with a vocabulary of {}\\n\" \"lexical diversity is {:0.3f}\" ).format( fids, paras, sents, sperp, count, vocab, lexdiv )) And the result is:\nText corpus contains 17 files Composed of 1367 paragraphs and 2817 sentences. 2.061 sentences per paragraph Word count of 57762 with a vocabulary of 5602 lexical diversity is 10.311 Pretty simple!\nReading the HTML Corpus The PlaintextCorpusReader determined paragraphs as those separated by newlines, something that is not guaranteed for all corpora. HTML documents provide a bit more structure for us to parse, but there is no built in HTML corpus reader, unfortunately. Let’s take a look at how to extend our corpus reader to read HTML:\nimport bs4 class HTMLCorpusReader(PlaintextCorpusReader): tags = [ 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li' ] def _read_word_block(self, stream): soup = bs4.BeautifulSoup(stream, 'lxml') return self._word_tokenizer.tokenize(soup.get_text()) def _read_para_block(self, stream): \"\"\" The stream is a single block (file) to extract paragraphs from. Method must return list(list(list(str))) of paragraphs, sentences, and words, so all tokenizers must be used here. \"\"\" soup = bs4.BeautifulSoup(stream, 'lxml') paras = [] for para in soup.find_all(self.tags): paras.append([ self._word_tokenizer.tokenize(sent) for sent in self._sent_tokenizer.tokenize(para.text) ]) return paras The PlaintextCorpusReader accepts as additional input a word_tokenizer, a sent_tokenizer, and a para_block: functions that deal with tokenizing the text into various chunks. By default these are the wordpunct_tokenzie, sent_tokenize, and blank line blocks reader, respectively.\nIn order to add different functionality, you can either pass a callable into the constructor, or you can override some internal methods. Note that you should not override the paras, sents, or words methods – these methods handle the streaming. Instead you should override the following protected methods:\n_read_word_block: tokenizes 20 lines at a time from the stream. _read_sent_block: passes the file paragraph at a time into the segmenter. _read_para_block: deals with a file at a time from the stream. Although protected, you can see how easy it is to get access to the block stream and override it. Here we simply look for a variety of tags to call “paragraphs” by using BeautifulSoup, then correctly return the segmented and tokenized text. Our word block tokenizer simply does an HTML strip tags.\n","wordCount":"1081","inLanguage":"en","datePublished":"2016-04-11T21:03:18Z","dateModified":"2016-04-11T21:03:18Z","author":{"@type":"Person","name":"Benjamin Bengfort"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bbengfort.github.io/2016/04/nltk-corpus-reader/"},"publisher":{"@type":"Organization","name":"Libelli","logo":{"@type":"ImageObject","url":"https://bbengfort.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://bbengfort.github.io accesskey=h title="Libelli (Alt + H)"><img src=/icon.png alt=logo aria-label=logo height=35>Libelli</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://bbengfort.github.io/archive/ title=archive><span>archive</span></a></li><li><a href=https://bbengfort.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://bbengfort.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://bbengfort.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>NLTK Corpus Reader for Extracted Corpus</h1><div class=post-meta>April 11, 2016&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Benjamin Bengfort</div></header><div class=post-content><p>Yesterday I wrote a blog about [extracting a corpus]({% post_url 2016-04-10-extract-ddl-corpus %}) from a directory containing Markdown, such as for a blog that is deployed with Silvrback or Jekyll. In this post, I&rsquo;ll briefly show how to use the built in <code>CorpusReader</code> objects in <code>nltk</code> for streaming the data to the segmentation and tokenization preprocessing functions that are built into NLTK for performing analytics.</p><p>The dataset that I&rsquo;ll be working with is the <a href=http://blog.districtdatalabs.com/>District Data Labs Blog</a>, in particular the state of the blog as of today. The dataset can be downloaded from the <a href=http://bit.ly/ddl-blogs-corpus>ddl corpus</a>, which also has the code in this post for you to use to perform other analytics.</p><p>The <code>mdec.py</code> program extracted our corpus in two formats: html and text. It also setup the corpus as follows:</p><ul><li>README describing the corpus (no extension)</li><li>all text files in the same directory with the .txt or .html extension</li></ul><p>If this had been a categorized corpus, then we would have created subdirectories for each category in the corpus, and placed the correct files there. This organization has important implications for using the base readers without too much extension. Plus it helps others understand how to set up corpora with ease.</p><h2 id=reading-corpora>Reading Corpora<a hidden class=anchor aria-hidden=true href=#reading-corpora>#</a></h2><p>NLTK&rsquo;s <code>CorpusReader</code> objects provide a useful interface to streaming, end-to-end reads of a text corpus from multiple files on disk. To construct a corpus you need to pass the path to the directory containing the corpus, as well as a pattern for a regular expression matching the files that belong to the corpus. By default the <code>CorpusReader</code> opens everything with UTF-8 encoding and generally provides the following descriptive methods:</p><ul><li><code>readme()</code>: returns the contents of a README file</li><li><code>citation()</code>: returns the contents of a citation.bib file</li><li><code>license()</code>: returns the contents of a LICENSE file</li></ul><p>Generally speaking, your corpora should include all of these meta data files in the root directory in order to be considered complete.</p><p>There are many types of <code>CorpusReader</code> subclasses available in NLTK. The base classes provide readers for syntax corpora (those that are already structured as parses), bracket corpora (already part of speech tagged), and categorized corpora (documents associated with specific files). There are also a host of readers for the specific corpora that come included with NLTK. In general, these readers should provide an API that contain the following methods:</p><ul><li><code>paras()</code>: returns an iterable of paragraphs (a list of lists of sentences)</li><li><code>sents()</code>: returns an iterable of sentences (a list of lists of words)</li><li><code>words()</code>: returns an iterable of words (a list of words)</li><li><code>raw()</code>: simply returns the raw text from the corpus</li></ul><p>Most <code>CorpusReader</code> classes can be accessed and filtered by a specific file or category or a list of files or categories. There are two primary methods for listing these if available to the corpus:</p><ul><li><code>fileids()</code>: lists the names of the files that are in the corpus</li><li><code>categories()</code>: lists the names of the categories in the corpus</li></ul><p>This listing of API methods is by no means comprehensive. However, for most of the text analytics you&rsquo;ll be doing, these methods will do the bulk of the work. I would consider a CorpusReader complete if it contained all of these methods.</p><h2 id=reading-the-text-corpus>Reading the Text Corpus<a hidden class=anchor aria-hidden=true href=#reading-the-text-corpus>#</a></h2><p>The simplest thing to do is read our plaintext corpus, as we have to write no code to do so. Instead we can simply use the <code>nltk.corpus.PlaintextCorpusReader</code> directly, instantiating it with the correct path and pattern for our files. For the DDL corpus this looks like something as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> nltk.corpus.reader.plaintext <span style=color:#f92672>import</span> PlaintextCorpusReader
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>corpus <span style=color:#f92672>=</span> PlaintextCorpusReader(CORPUS_TEXT, <span style=color:#e6db74>&#39;.*\.txt&#39;</span>)
</span></span></code></pre></div><p>That&rsquo;s it! As long as we path it a correct path to the corpus and a <em>pattern</em> for identifying text files, then we&rsquo;re good to go! Note that the pattern is formatted as a Python regular expression, hence the escaped <code>.</code> &ndash; unfortunately NLTK doesn&rsquo;t use <code>glob</code> or other patterns for file identification.</p><p>We can now print out some information about our corpus using the reader directly:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> nltk <span style=color:#f92672>import</span> FreqDist
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>corpus_info</span>(corpus):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Prints out information about the status of a corpus.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    fids   <span style=color:#f92672>=</span> len(corpus<span style=color:#f92672>.</span>fileids())
</span></span><span style=display:flex><span>    paras  <span style=color:#f92672>=</span> len(corpus<span style=color:#f92672>.</span>paras())
</span></span><span style=display:flex><span>    sents  <span style=color:#f92672>=</span> len(corpus<span style=color:#f92672>.</span>sents())
</span></span><span style=display:flex><span>    sperp  <span style=color:#f92672>=</span> sum(len(para) <span style=color:#66d9ef>for</span> para <span style=color:#f92672>in</span> corpus<span style=color:#f92672>.</span>paras()) <span style=color:#f92672>/</span> float(paras)
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> FreqDist(corpus<span style=color:#f92672>.</span>words())
</span></span><span style=display:flex><span>    count  <span style=color:#f92672>=</span> sum(tokens<span style=color:#f92672>.</span>values())
</span></span><span style=display:flex><span>    vocab  <span style=color:#f92672>=</span> len(tokens)
</span></span><span style=display:flex><span>    lexdiv <span style=color:#f92672>=</span> float(count) <span style=color:#f92672>/</span> float(vocab)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print((
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;Text corpus contains </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> files</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;Composed of </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> paragraphs and </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> sentences.</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{:0.3f}</span><span style=color:#e6db74> sentences per paragraph</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;Word count of </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> with a vocabulary of </span><span style=color:#e6db74>{}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;lexical diversity is </span><span style=color:#e6db74>{:0.3f}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>    )<span style=color:#f92672>.</span>format(
</span></span><span style=display:flex><span>        fids, paras, sents, sperp, count, vocab, lexdiv
</span></span><span style=display:flex><span>    ))
</span></span></code></pre></div><p>And the result is:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Text corpus contains 17 files
</span></span><span style=display:flex><span>Composed of 1367 paragraphs and 2817 sentences.
</span></span><span style=display:flex><span>2.061 sentences per paragraph
</span></span><span style=display:flex><span>Word count of 57762 with a vocabulary of 5602
</span></span><span style=display:flex><span>lexical diversity is 10.311
</span></span></code></pre></div><p>Pretty simple!</p><h2 id=reading-the-html-corpus>Reading the HTML Corpus<a hidden class=anchor aria-hidden=true href=#reading-the-html-corpus>#</a></h2><p>The <code>PlaintextCorpusReader</code> determined paragraphs as those separated by newlines, something that is not guaranteed for all corpora. HTML documents provide a bit more structure for us to parse, but there is no built in HTML corpus reader, unfortunately. Let&rsquo;s take a look at how to extend our corpus reader to read HTML:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> bs4
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>HTMLCorpusReader</span>(PlaintextCorpusReader):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    tags <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;h1&#39;</span>, <span style=color:#e6db74>&#39;h2&#39;</span>, <span style=color:#e6db74>&#39;h3&#39;</span>, <span style=color:#e6db74>&#39;h4&#39;</span>, <span style=color:#e6db74>&#39;h5&#39;</span>, <span style=color:#e6db74>&#39;h6&#39;</span>, <span style=color:#e6db74>&#39;h7&#39;</span>, <span style=color:#e6db74>&#39;p&#39;</span>, <span style=color:#e6db74>&#39;li&#39;</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_read_word_block</span>(self, stream):
</span></span><span style=display:flex><span>        soup  <span style=color:#f92672>=</span> bs4<span style=color:#f92672>.</span>BeautifulSoup(stream, <span style=color:#e6db74>&#39;lxml&#39;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>_word_tokenizer<span style=color:#f92672>.</span>tokenize(soup<span style=color:#f92672>.</span>get_text())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_read_para_block</span>(self, stream):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        The stream is a single block (file) to extract paragraphs from.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Method must return list(list(list(str))) of paragraphs, sentences,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        and words, so all tokenizers must be used here.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        soup  <span style=color:#f92672>=</span> bs4<span style=color:#f92672>.</span>BeautifulSoup(stream, <span style=color:#e6db74>&#39;lxml&#39;</span>)
</span></span><span style=display:flex><span>        paras <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> para <span style=color:#f92672>in</span> soup<span style=color:#f92672>.</span>find_all(self<span style=color:#f92672>.</span>tags):
</span></span><span style=display:flex><span>            paras<span style=color:#f92672>.</span>append([
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>_word_tokenizer<span style=color:#f92672>.</span>tokenize(sent)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> sent <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>_sent_tokenizer<span style=color:#f92672>.</span>tokenize(para<span style=color:#f92672>.</span>text)
</span></span><span style=display:flex><span>            ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> paras
</span></span></code></pre></div><p>The <code>PlaintextCorpusReader</code> accepts as additional input a <code>word_tokenizer</code>, a <code>sent_tokenizer</code>, and a <code>para_block</code>: functions that deal with tokenizing the text into various chunks. By default these are the <code>wordpunct_tokenzie</code>, <code>sent_tokenize</code>, and blank line blocks reader, respectively.</p><p>In order to add different functionality, you can either pass a callable into the constructor, or you can override some internal methods. Note that you <em>should not override the <code>paras</code>, <code>sents</code>, or <code>words</code> methods</em> &ndash; these methods handle the streaming. Instead you should override the following protected methods:</p><ul><li><code>_read_word_block</code>: tokenizes 20 lines at a time from the stream.</li><li><code>_read_sent_block</code>: passes the file paragraph at a time into the segmenter.</li><li><code>_read_para_block</code>: deals with a file at a time from the stream.</li></ul><p>Although protected, you can see how easy it is to get access to the block stream and override it. Here we simply look for a variety of tags to call &ldquo;paragraphs&rdquo; by using <code>BeautifulSoup</code>, then correctly return the segmented and tokenized text. Our word block tokenizer simply does an HTML strip tags.</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://bbengfort.github.io>Libelli</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById("menu");menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>