<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scikit-Learn Data Management: Bunches | Libelli</title><meta name=keywords content><meta name=description content="One large issue that I encounter in development with machine learning is the need to structure our data on disk in a way that we can load into Scikit-Learn in a repeatable fashion for continued analysis. My proposal is to use the sklearn.datasets.base.Bunch object to load the data into data and target attributes respectively, similar to how Scikit-Learn&rsquo;s toy datasets are structured. Using this object to manage our data will mirror the native API and allow us to easily copy and paste code that demonstrates classifiers and techniques with the built in datasets."><meta name=author content="Benjamin Bengfort"><link rel=canonical href=https://bbengfort.github.io/2016/04/bunch-data-management/><link crossorigin=anonymous href=/assets/css/stylesheet.min.d0c0348c2d0cff14148d0e347258519d8df2ce53ce5ac32c7bd9a549182cb8ae.css integrity="sha256-0MA0jC0M/xQUjQ40clhRnY3yzlPOWsMse9mlSRgsuK4=" rel="preload stylesheet" as=style><link rel=preload href=/icon.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://bbengfort.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bbengfort.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://bbengfort.github.io/icon.png><link rel=apple-touch-icon href=https://bbengfort.github.io/apple-touch-icon-precomposed.png><link rel=mask-icon href=https://bbengfort.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.79.0"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-8096804-11','auto');ga('send','pageview');}</script><meta property="og:title" content="Scikit-Learn Data Management: Bunches"><meta property="og:description" content="One large issue that I encounter in development with machine learning is the need to structure our data on disk in a way that we can load into Scikit-Learn in a repeatable fashion for continued analysis. My proposal is to use the sklearn.datasets.base.Bunch object to load the data into data and target attributes respectively, similar to how Scikit-Learn&rsquo;s toy datasets are structured. Using this object to manage our data will mirror the native API and allow us to easily copy and paste code that demonstrates classifiers and techniques with the built in datasets."><meta property="og:type" content="article"><meta property="og:url" content="https://bbengfort.github.io/2016/04/bunch-data-management/"><meta property="og:image" content="https://bbengfort.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-04-19T11:29:30+00:00"><meta property="article:modified_time" content="2016-04-19T11:29:30+00:00"><meta property="og:site_name" content="Libelli"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bbengfort.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Scikit-Learn Data Management: Bunches"><meta name=twitter:description content="One large issue that I encounter in development with machine learning is the need to structure our data on disk in a way that we can load into Scikit-Learn in a repeatable fashion for continued analysis. My proposal is to use the sklearn.datasets.base.Bunch object to load the data into data and target attributes respectively, similar to how Scikit-Learn&rsquo;s toy datasets are structured. Using this object to manage our data will mirror the native API and allow us to easily copy and paste code that demonstrates classifiers and techniques with the built in datasets."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://bbengfort.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Scikit-Learn Data Management: Bunches","item":"https://bbengfort.github.io/2016/04/bunch-data-management/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scikit-Learn Data Management: Bunches","name":"Scikit-Learn Data Management: Bunches","description":"One large issue that I encounter in development with machine learning is the need to structure our data on disk in a way that we can load into Scikit-Learn in a repeatable fashion for continued analysis. My proposal is to use the sklearn.datasets.base.Bunch object to load the data into data and target attributes respectively, similar to how Scikit-Learn\u0026rsquo;s toy datasets are structured. Using this object to manage our data will mirror the native API and allow us to easily copy and paste code that demonstrates classifiers and techniques with the built in datasets.","keywords":[],"articleBody":"One large issue that I encounter in development with machine learning is the need to structure our data on disk in a way that we can load into Scikit-Learn in a repeatable fashion for continued analysis. My proposal is to use the sklearn.datasets.base.Bunch object to load the data into data and target attributes respectively, similar to how Scikit-Learn’s toy datasets are structured. Using this object to manage our data will mirror the native API and allow us to easily copy and paste code that demonstrates classifiers and techniques with the built in datasets. Importantly, this API will also allow us to communicate to other developers and our future-selves exactly how to use the data.\nMoreover, we need to be able to structure more and varied datasets as most projects aren’t dedicated to building a single classifier, but rather lots of them. Data is extracted and written to disk through SQL queries, then models are written back into the database. All of these fixtures (for building models) as well as the extraction method, and meta data need to be versioned so that we can have a repeatable process (for science). The workflow is as follows:\nThis post is largely concerned with the “Data Directory” and the “Load and Transform Data” highlighted processes in the flow chart. The first step is to structure a fixtures directory with our data code. The fixtures directory will contain named subdirectories where each name is related to a dataset we want to load. These directories will contain the following files.\n query.sql: a sql file that can be executed against the database to extract and wrangle the dataset. dataset.txt: a numpy whitespace delimited file containing either a dense or sparse matrix of numeric data to pass to the model fit process. (This can be easily adapted to a CSV file of raw data if needed). README.md: a markdown file containing information about the dataset and attribution. Will be exposed by the DESCR attribute. meta.json: a helper file that contains machine readable information about the dataset like target_names and feature_names.  A very simple project will therefore have a fixtures directory that looks like:\n$ project . ├── fixtures | ├── energy | | ├── dataset.txt | | ├── meta.json | | ├── README.md | | └── query.sql | └── solar | ├── dataset.txt | ├── meta.json | ├── README.md | └── query.sql └── index.json Dataset utilities code should know about this directory and how to access it by using paths relative to the source code and environment variables as follows:\nimport os SKL_DATA = \"SCIKIT_LEARN_DATA\" BASE_DIR = os.path.normpath(os.path.join(os.path.dirname(__file__), \"..\")) DATA_DIR = os.path.join(BASE_DIR, \"fixtures\") def get_data_home(data_home=None): \"\"\" Returns the path of the data directory \"\"\" if data_home is None: data_home = os.environ.get(SKL_DATA, DATA_DIR) data_home = os.path.expanduser(data_home) if not os.path.exists(data_home): os.makedirs(data_home) return data_home The get_data_home variable looks for the root directory of the fixtures, by accepting a passed in path, or by looking in the environment, finally defaulting to the project fixtures directory. Note that this function creates the directory if it doesn’t exist in order for automatic writes to go through without failing.\nThe Bunch object in Scikit-Learn is simply a dictionary that exposes dictionary keys as properties so that you can access them with dot notation. This by itself isn’t particularly useful, but let’s look at how the toy datasets are structured:\n from sklearn.datasets import load_digits, load_boston  dataset = load_digits()  print dataset.keys() ['images', 'data', 'target_names', 'DESCR', 'target']  print load_boston().keys() ['data', 'feature_names', 'DESCR', 'target'] We can see that the bunch object keeps track of the primary matrix (usually labeled X) in the data attribute and the targets (usually called y) in the target attribute. Moreover, it shows a README with information about the dataset including citations in the DESCR property, as well as other information like names and images. We will create a similar load_data methodology to use in our projects.\nNow that we have everything we need stored on disk, we can create a load_data function, which will accept the name of a dataset, and appropriately look it up using the structure above. Moreover, it extracts the data required for a Bunch object including extracting the target value from the first or last columns of the dataset and using the meta.json file for other important information.\nimport json import numpy as np from sklearn.datasets.base import Bunch def load_data(path, descr=None, target_index=-1): \"\"\" Returns a sklearn dataset Bunch which includes several important attributes that are used in modeling: data: array of shape n_samples * n_features target: array of length n_samples feature_names: names of the features target_names: names of the targets filenames: names of the files that were loaded DESCR: contents of the readme This data therefore has the look and feel of the toy datasets. \"\"\" root = os.path.join(get_data_home(), path) filenames = { 'meta': os.path.join(root, 'meta.json'), 'rdme': os.path.join(root, 'README.md'), 'data': os.path.join(root, 'dataset.txt'), } target_names = None feature_names = None DESCR = None with open(filenames['meta'], 'r') as f: meta = json.load(f) target_names = meta['target_names'] feature_names = meta['feature_names'] with open(filenames['rdme'], 'r') as f: DESCR = f.read() dataset = np.loadtxt(filenames['data']) data = None target = None # Target assumed to be either last or first row if target_index == -1: data = dataset[:, 0:-1] target = dataset[:, -1] elif target_index == 0: data = dataset[:, 1:] target = dataset[:, 0] else: raise ValueError(\"Target index must be either -1 or 0\") return Bunch(data=data, target=target, filenames=filenames, target_names=target_names, feature_names=feature_names, DESCR=DESCR) The primary work of the load_data function is to locate the appropriate files on disk, given a root directory that’s passed in as an argument (if you saved your data in a different directory, you can modify the root to have it look in the right place). The meta data is included with the bunch, and is also used split the train and test datasets into data and target variables appropriately, such that we can pass them correctly to the Scikit-Learn fit and predict estimator methods.\nNow we can create named aliases for specific datasets as follows:\ndef load_energy(): return load_data('energy') def load_solar(): return load_data('solar') And we have a system that looks and feels exactly like the datasets that Scikit-Learn ships with.\n","wordCount":"1020","inLanguage":"en","datePublished":"2016-04-19T11:29:30Z","dateModified":"2016-04-19T11:29:30Z","author":{"@type":"Person","name":"Benjamin Bengfort"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bbengfort.github.io/2016/04/bunch-data-management/"},"publisher":{"@type":"Organization","name":"Libelli","logo":{"@type":"ImageObject","url":"https://bbengfort.github.io/favicon.ico"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: #1d1e20;--entry: #2e2e33;--primary: rgba(255, 255, 255, 0.84);--secondary: rgba(255, 255, 255, 0.56);--tertiary: rgba(255, 255, 255, 0.16);--content: rgba(255, 255, 255, 0.74);--hljs-bg: #2e2e33;--code-bg: #37383e;--border: #333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://bbengfort.github.io accesskey=h title="Libelli (Alt + H)"><img src=/icon.png alt=logo aria-label=logo height=35>Libelli</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://bbengfort.github.io/archive/ title=archive><span>archive</span></a></li><li><a href=https://bbengfort.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://bbengfort.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://bbengfort.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Scikit-Learn Data Management: Bunches</h1><div class=post-meta>April 19, 2016&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Benjamin Bengfort</div></header><div class=post-content><p>One large issue that I encounter in development with machine learning is the need to structure our data on disk in a way that we can load into Scikit-Learn in a repeatable fashion for continued analysis. My proposal is to use the <code>sklearn.datasets.base.Bunch</code> object to load the data into data and target attributes respectively, similar to how Scikit-Learn&rsquo;s toy datasets are structured. Using this object to manage our data will mirror the native API and allow us to easily copy and paste code that demonstrates classifiers and techniques with the built in datasets. Importantly, this API will also allow us to communicate to other developers and our future-selves exactly how to use the data.</p><p>Moreover, we need to be able to structure more and varied datasets as most projects aren&rsquo;t dedicated to building a single classifier, but rather <em>lots</em> of them. Data is extracted and written to disk through SQL queries, then models are written back into the database. All of these fixtures (for building models) as well as the extraction method, and meta data need to be versioned so that we can have a repeatable process (for science). The workflow is as follows:</p><p><img loading=lazy src=/images/2016-04-19-ml-data-management-workflow.png alt="ML Data Management Workflow"></p><p>This post is largely concerned with the “Data Directory” and the “Load and Transform Data” highlighted processes in the flow chart. The first step is to structure a fixtures directory with our data code. The fixtures directory will contain <em>named</em> subdirectories where each name is related to a dataset we want to load. These directories will contain the following files.</p><ul><li><code>query.sql</code>: a sql file that can be executed against the database to extract and wrangle the dataset.</li><li><code>dataset.txt</code>: a numpy whitespace delimited file containing either a dense or sparse matrix of numeric data to pass to the model fit process. (This can be easily adapted to a CSV file of raw data if needed).</li><li><code>README.md</code>: a markdown file containing information about the dataset and attribution. Will be exposed by the DESCR attribute.</li><li><code>meta.json</code>: a helper file that contains machine readable information about the dataset like target_names and feature_names.</li></ul><p>A very simple project will therefore have a fixtures directory that looks like:</p><pre><code>$ project
.
├── fixtures
|   ├── energy
|   |   ├── dataset.txt
|   |   ├── meta.json
|   |   ├── README.md
|   |   └── query.sql
|   └── solar
|       ├── dataset.txt
|       ├── meta.json
|       ├── README.md
|       └── query.sql
└── index.json
</code></pre><p>Dataset utilities code should know about this directory and how to access it by using paths relative to the source code and environment variables as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> os

SKL_DATA <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;SCIKIT_LEARN_DATA&#34;</span>
BASE_DIR <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>normpath(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(__file__), <span style=color:#e6db74>&#34;..&#34;</span>))
DATA_DIR <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(BASE_DIR, <span style=color:#e6db74>&#34;fixtures&#34;</span>)


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_data_home</span>(data_home<span style=color:#f92672>=</span>None):
    <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>    Returns the path of the data directory
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
    <span style=color:#66d9ef>if</span> data_home <span style=color:#f92672>is</span> None:
        data_home <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(SKL_DATA, DATA_DIR)

    data_home <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>expanduser(data_home)
    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(data_home):
        os<span style=color:#f92672>.</span>makedirs(data_home)

    <span style=color:#66d9ef>return</span> data_home
</code></pre></div><p>The <code>get_data_home</code> variable looks for the root directory of the fixtures, by accepting a passed in path, or by looking in the environment, finally defaulting to the project fixtures directory. Note that this function creates the directory if it doesn&rsquo;t exist in order for automatic writes to go through without failing.</p><p>The <code>Bunch</code> object in Scikit-Learn is simply a dictionary that exposes dictionary keys as properties so that you can access them with dot notation. This by itself isn&rsquo;t particularly useful, but let&rsquo;s look at how the toy datasets are structured:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> load_digits, load_boston
<span style=color:#f92672>&gt;&gt;&gt;</span> dataset <span style=color:#f92672>=</span> load_digits()
<span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#66d9ef>print</span> dataset<span style=color:#f92672>.</span>keys()
[<span style=color:#e6db74>&#39;images&#39;</span>, <span style=color:#e6db74>&#39;data&#39;</span>, <span style=color:#e6db74>&#39;target_names&#39;</span>, <span style=color:#e6db74>&#39;DESCR&#39;</span>, <span style=color:#e6db74>&#39;target&#39;</span>]
<span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#66d9ef>print</span> load_boston()<span style=color:#f92672>.</span>keys()
[<span style=color:#e6db74>&#39;data&#39;</span>, <span style=color:#e6db74>&#39;feature_names&#39;</span>, <span style=color:#e6db74>&#39;DESCR&#39;</span>, <span style=color:#e6db74>&#39;target&#39;</span>]
</code></pre></div><p>We can see that the bunch object keeps track of the primary matrix (usually labeled <code>X</code>) in the data attribute and the targets (usually called <code>y</code>) in the target attribute. Moreover, it shows a README with information about the dataset including citations in the <code>DESCR</code> property, as well as other information like names and images. We will create a similar <code>load_data</code> methodology to use in our projects.</p><p>Now that we have everything we need stored on disk, we can create a <code>load_data</code> function, which will accept the name of a dataset, and appropriately look it up using the structure above. Moreover, it extracts the data required for a <code>Bunch</code> object including extracting the target value from the first or last columns of the dataset and using the <code>meta.json</code> file for other important information.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> json
<span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np

<span style=color:#f92672>from</span> sklearn.datasets.base <span style=color:#f92672>import</span> Bunch

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_data</span>(path, descr<span style=color:#f92672>=</span>None, target_index<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>):
    <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>    Returns a sklearn dataset Bunch which includes several important
</span><span style=color:#e6db74>    attributes that are used in modeling:
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>        data: array of shape n_samples * n_features
</span><span style=color:#e6db74>        target: array of length n_samples
</span><span style=color:#e6db74>        feature_names: names of the features
</span><span style=color:#e6db74>        target_names: names of the targets
</span><span style=color:#e6db74>        filenames: names of the files that were loaded
</span><span style=color:#e6db74>        DESCR: contents of the readme
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>    This data therefore has the look and feel of the toy datasets.
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>

    root          <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(get_data_home(), path)
    filenames     <span style=color:#f92672>=</span> {
        <span style=color:#e6db74>&#39;meta&#39;</span>: os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(root, <span style=color:#e6db74>&#39;meta.json&#39;</span>),
        <span style=color:#e6db74>&#39;rdme&#39;</span>: os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(root, <span style=color:#e6db74>&#39;README.md&#39;</span>),
        <span style=color:#e6db74>&#39;data&#39;</span>: os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(root, <span style=color:#e6db74>&#39;dataset.txt&#39;</span>),
    }

    target_names  <span style=color:#f92672>=</span> None
    feature_names <span style=color:#f92672>=</span> None
    DESCR         <span style=color:#f92672>=</span> None

    <span style=color:#66d9ef>with</span> open(filenames[<span style=color:#e6db74>&#39;meta&#39;</span>], <span style=color:#e6db74>&#39;r&#39;</span>) <span style=color:#66d9ef>as</span> f:
        meta <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>load(f)
        target_names  <span style=color:#f92672>=</span> meta[<span style=color:#e6db74>&#39;target_names&#39;</span>]
        feature_names <span style=color:#f92672>=</span> meta[<span style=color:#e6db74>&#39;feature_names&#39;</span>]

    <span style=color:#66d9ef>with</span> open(filenames[<span style=color:#e6db74>&#39;rdme&#39;</span>], <span style=color:#e6db74>&#39;r&#39;</span>) <span style=color:#66d9ef>as</span> f:
        DESCR <span style=color:#f92672>=</span> f<span style=color:#f92672>.</span>read()

    dataset <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>loadtxt(filenames[<span style=color:#e6db74>&#39;data&#39;</span>])
    data    <span style=color:#f92672>=</span> None
    target  <span style=color:#f92672>=</span> None

    <span style=color:#75715e># Target assumed to be either last or first row</span>
    <span style=color:#66d9ef>if</span> target_index <span style=color:#f92672>==</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>:
        data   <span style=color:#f92672>=</span> dataset[:, <span style=color:#ae81ff>0</span>:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
        target <span style=color:#f92672>=</span> dataset[:, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
    <span style=color:#66d9ef>elif</span> target_index <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
        data   <span style=color:#f92672>=</span> dataset[:, <span style=color:#ae81ff>1</span>:]
        target <span style=color:#f92672>=</span> dataset[:, <span style=color:#ae81ff>0</span>]
    <span style=color:#66d9ef>else</span>:
        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;Target index must be either -1 or 0&#34;</span>)

    <span style=color:#66d9ef>return</span> Bunch(data<span style=color:#f92672>=</span>data,
                 target<span style=color:#f92672>=</span>target,
                 filenames<span style=color:#f92672>=</span>filenames,
                 target_names<span style=color:#f92672>=</span>target_names,
                 feature_names<span style=color:#f92672>=</span>feature_names,
                 DESCR<span style=color:#f92672>=</span>DESCR)
</code></pre></div><p>The primary work of the load_data function is to locate the appropriate files on disk, given a root directory that&rsquo;s passed in as an argument (if you saved your data in a different directory, you can modify the root to have it look in the right place). The meta data is included with the bunch, and is also used split the train and test datasets into data and target variables appropriately, such that we can pass them correctly to the Scikit-Learn fit and predict estimator methods.</p><p>Now we can create named aliases for specific datasets as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_energy</span>():
    <span style=color:#66d9ef>return</span> load_data(<span style=color:#e6db74>&#39;energy&#39;</span>)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_solar</span>():
    <span style=color:#66d9ef>return</span> load_data(<span style=color:#e6db74>&#39;solar&#39;</span>)
</code></pre></div><p>And we have a system that looks and feels exactly like the datasets that Scikit-Learn ships with.</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2021 <a href=https://bbengfort.github.io>Libelli</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu')
menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>