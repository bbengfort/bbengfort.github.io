<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Extracting the DDL Blog Corpus | Libelli</title>
<meta name=keywords content><meta name=description content="We have some simple text analyses coming up and as an example, I thought it might be nice to use the DDL blog corpus as a data set. There are relatively few DDL blogs, but they all are long with a lot of significant text and discourse. It might be interesting to try to do some lightweight analysis on them.
So, how to extract the corpus? The DDL blog is currently hosted on Silvrback which is designed for text-forward, distraction-free blogging. As a result, there isn&rsquo;t a lot of cruft on the page. I considered doing a scraper that pulled the web pages down or using the RSS feed to do the data ingestion. After all, I wouldn&rsquo;t have to do a lot of HTML cleaning."><meta name=author content="Benjamin Bengfort"><link rel=canonical href=https://bbengfort.github.io/2016/04/extract-ddl-corpus/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://bbengfort.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bbengfort.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://bbengfort.github.io/icon.png><link rel=apple-touch-icon href=https://bbengfort.github.io/apple-touch-icon-precomposed.png><link rel=mask-icon href=https://bbengfort.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://bbengfort.github.io/2016/04/extract-ddl-corpus/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-D3BE7EHHVP"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D3BE7EHHVP")}</script><meta property="og:title" content="Extracting the DDL Blog Corpus"><meta property="og:description" content="We have some simple text analyses coming up and as an example, I thought it might be nice to use the DDL blog corpus as a data set. There are relatively few DDL blogs, but they all are long with a lot of significant text and discourse. It might be interesting to try to do some lightweight analysis on them.
So, how to extract the corpus? The DDL blog is currently hosted on Silvrback which is designed for text-forward, distraction-free blogging. As a result, there isn&rsquo;t a lot of cruft on the page. I considered doing a scraper that pulled the web pages down or using the RSS feed to do the data ingestion. After all, I wouldn&rsquo;t have to do a lot of HTML cleaning."><meta property="og:type" content="article"><meta property="og:url" content="https://bbengfort.github.io/2016/04/extract-ddl-corpus/"><meta property="og:image" content="https://bbengfort.github.io/bear.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-04-10T06:44:28+00:00"><meta property="article:modified_time" content="2016-04-10T06:44:28+00:00"><meta property="og:site_name" content="Libelli"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bbengfort.github.io/bear.png"><meta name=twitter:title content="Extracting the DDL Blog Corpus"><meta name=twitter:description content="We have some simple text analyses coming up and as an example, I thought it might be nice to use the DDL blog corpus as a data set. There are relatively few DDL blogs, but they all are long with a lot of significant text and discourse. It might be interesting to try to do some lightweight analysis on them.
So, how to extract the corpus? The DDL blog is currently hosted on Silvrback which is designed for text-forward, distraction-free blogging. As a result, there isn&rsquo;t a lot of cruft on the page. I considered doing a scraper that pulled the web pages down or using the RSS feed to do the data ingestion. After all, I wouldn&rsquo;t have to do a lot of HTML cleaning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://bbengfort.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Extracting the DDL Blog Corpus","item":"https://bbengfort.github.io/2016/04/extract-ddl-corpus/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Extracting the DDL Blog Corpus","name":"Extracting the DDL Blog Corpus","description":"We have some simple text analyses coming up and as an example, I thought it might be nice to use the DDL blog corpus as a data set. There are relatively few DDL blogs, but they all are long with a lot of significant text and discourse. It might be interesting to try to do some lightweight analysis on them.\nSo, how to extract the corpus? The DDL blog is currently hosted on Silvrback which is designed for text-forward, distraction-free blogging. As a result, there isn\u0026rsquo;t a lot of cruft on the page. I considered doing a scraper that pulled the web pages down or using the RSS feed to do the data ingestion. After all, I wouldn\u0026rsquo;t have to do a lot of HTML cleaning.\n","keywords":[],"articleBody":"We have some simple text analyses coming up and as an example, I thought it might be nice to use the DDL blog corpus as a data set. There are relatively few DDL blogs, but they all are long with a lot of significant text and discourse. It might be interesting to try to do some lightweight analysis on them.\nSo, how to extract the corpus? The DDL blog is currently hosted on Silvrback which is designed for text-forward, distraction-free blogging. As a result, there isn’t a lot of cruft on the page. I considered doing a scraper that pulled the web pages down or using the RSS feed to do the data ingestion. After all, I wouldn’t have to do a lot of HTML cleaning.\nThen I realized – hey, we have all the Markdown in a repository!\nBy having everything in one place, as Markdown, I don’t have to do a search or a crawl to get all the links. Moreover, I get a bit finer-grained control of what text I want. The question came down to rendering – do I try to analyze the Markdown, or do I render it into HTML?\nIn the end, I figured rendering the Markdown to HTML with Python would probably provide the best corpus result. I’ve created a tool that takes a directory of Markdown files, renders them as HTML or text and then creates the corpus organized directory expected by NLTK. Nicely, this also works with Jekyll! Here is the code:\nSorry that was so long, I tried to cut it down a bit, but the argparse stuff really does make it quite verbose. Still the basic methodology is to loop through all the files (recursively going down subdirectories) looking for *.md or *.markdown files. I then use the Python Markdown library with the markdown.extensions.extra package to render HTML, and to render the text from the HTML, I’m currently using BeautifulSoup get_text.\nNote also that this tool writes a README with information about the extraction. You can now use the nltk.PlainTextCorpusReader to get access to this text!\n","wordCount":"347","inLanguage":"en","image":"https://bbengfort.github.io/bear.png","datePublished":"2016-04-10T06:44:28Z","dateModified":"2016-04-10T06:44:28Z","author":{"@type":"Person","name":"Benjamin Bengfort"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bbengfort.github.io/2016/04/extract-ddl-corpus/"},"publisher":{"@type":"Organization","name":"Libelli","logo":{"@type":"ImageObject","url":"https://bbengfort.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://bbengfort.github.io/ accesskey=h title="Libelli (Alt + H)"><img src=https://bbengfort.github.io/icon.png alt aria-label=logo height=35>Libelli</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://bbengfort.github.io/archive/ title=archive><span>archive</span></a></li><li><a href=https://bbengfort.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://bbengfort.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://bbengfort.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://bbengfort.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://bbengfort.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Extracting the DDL Blog Corpus</h1><div class=post-meta><span title='2016-04-10 06:44:28 +0000 UTC'>April 10, 2016</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;347 words&nbsp;·&nbsp;Benjamin Bengfort&nbsp;|&nbsp;<a href=https://github.com/bbengfort/bbengfort.github.io/tree/main/content/posts/2016-04-10-extract-ddl-corpus.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>We have some simple text analyses coming up and as an example, I thought it might be nice to use the DDL blog corpus as a data set. There are relatively few DDL blogs, but they all are long with a lot of significant text and discourse. It might be interesting to try to do some lightweight analysis on them.</p><p>So, how to extract the corpus? The <a href=http://blog.districtdatalabs.com>DDL blog</a> is currently hosted on <a href=https://www.silvrback.com/>Silvrback</a> which is designed for text-forward, distraction-free blogging. As a result, there isn&rsquo;t a lot of cruft on the page. I considered doing a scraper that pulled the web pages down or using the RSS feed to do the data ingestion. After all, I wouldn&rsquo;t have to do a lot of HTML cleaning.</p><p>Then I realized &ndash; hey, we have all the Markdown in a repository!</p><p>By having everything in one place, as Markdown, I don&rsquo;t have to do a search or a crawl to get all the links. Moreover, I get a bit finer-grained control of what text I want. The question came down to rendering &ndash; do I try to analyze the Markdown, or do I render it into HTML?</p><p>In the end, I figured rendering the Markdown to HTML with Python would probably provide the best corpus result. I&rsquo;ve created a tool that takes a directory of Markdown files, renders them as HTML or text and then creates the corpus organized directory expected by NLTK. Nicely, this also works with Jekyll! Here is the code:</p><script src=https://gist.github.com/bbengfort/d8bc35265861f57c4058ef5b2873b31d.js></script><p>Sorry that was so long, I tried to cut it down a bit, but the <code>argparse</code> stuff really does make it quite verbose. Still the basic methodology is to loop through all the files (recursively going down subdirectories) looking for <code>*.md</code> or <code>*.markdown</code> files. I then use the Python <a href=https://pythonhosted.org/Markdown/>Markdown</a> library with the <code>markdown.extensions.extra</code> package to render HTML, and to render the text from the HTML, I&rsquo;m currently using <a href=https://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-text>BeautifulSoup <code>get_text</code></a>.</p><p>Note also that this tool writes a README with information about the extraction. You can now use the <code>nltk.PlainTextCorpusReader</code> to get access to this text!</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://bbengfort.github.io/2016/04/nltk-corpus-reader/><span class=title>« Prev</span><br><span>NLTK Corpus Reader for Extracted Corpus</span>
</a><a class=next href=https://bbengfort.github.io/2016/04/dispatching-types-handler-methods/><span class=title>Next »</span><br><span>Dispatching Types to Handler Methods</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://bbengfort.github.io/>Libelli</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>