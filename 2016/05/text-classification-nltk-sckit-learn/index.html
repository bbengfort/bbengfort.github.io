<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Text Classification with NLTK and Scikit-Learn | Libelli</title><meta name=keywords content><meta name=description content="This post is an early draft of expanded work that will eventually appear on the District Data Labs Blog. Your feedback is welcome, and you can submit your comments on the draft GitHub issue.
 I&rsquo;ve often been asked which is better for text processing, NLTK or Scikit-Learn (and sometimes Gensim). The answer is that I use all three tools on a regular basis, but I often have a problem mixing and matching them or combining them in meaningful ways."><meta name=author content="Benjamin Bengfort"><link rel=canonical href=https://bbengfort.github.io/2016/05/text-classification-nltk-sckit-learn/><link crossorigin=anonymous href=/assets/css/stylesheet.min.d0c0348c2d0cff14148d0e347258519d8df2ce53ce5ac32c7bd9a549182cb8ae.css integrity="sha256-0MA0jC0M/xQUjQ40clhRnY3yzlPOWsMse9mlSRgsuK4=" rel="preload stylesheet" as=style><link rel=preload href=/icon.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=https://bbengfort.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bbengfort.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://bbengfort.github.io/icon.png><link rel=apple-touch-icon href=https://bbengfort.github.io/apple-touch-icon-precomposed.png><link rel=mask-icon href=https://bbengfort.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.79.0"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-8096804-11','auto');ga('send','pageview');}</script><meta property="og:title" content="Text Classification with NLTK and Scikit-Learn"><meta property="og:description" content="This post is an early draft of expanded work that will eventually appear on the District Data Labs Blog. Your feedback is welcome, and you can submit your comments on the draft GitHub issue.
 I&rsquo;ve often been asked which is better for text processing, NLTK or Scikit-Learn (and sometimes Gensim). The answer is that I use all three tools on a regular basis, but I often have a problem mixing and matching them or combining them in meaningful ways."><meta property="og:type" content="article"><meta property="og:url" content="https://bbengfort.github.io/2016/05/text-classification-nltk-sckit-learn/"><meta property="og:image" content="https://bbengfort.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-05-19T08:06:40+00:00"><meta property="article:modified_time" content="2016-05-19T08:06:40+00:00"><meta property="og:site_name" content="Libelli"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bbengfort.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Text Classification with NLTK and Scikit-Learn"><meta name=twitter:description content="This post is an early draft of expanded work that will eventually appear on the District Data Labs Blog. Your feedback is welcome, and you can submit your comments on the draft GitHub issue.
 I&rsquo;ve often been asked which is better for text processing, NLTK or Scikit-Learn (and sometimes Gensim). The answer is that I use all three tools on a regular basis, but I often have a problem mixing and matching them or combining them in meaningful ways."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://bbengfort.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Text Classification with NLTK and Scikit-Learn","item":"https://bbengfort.github.io/2016/05/text-classification-nltk-sckit-learn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Text Classification with NLTK and Scikit-Learn","name":"Text Classification with NLTK and Scikit-Learn","description":"This post is an early draft of expanded work that will eventually appear on the District Data Labs Blog. Your feedback is welcome, and you can submit your comments on the draft GitHub issue.\n I\u0026rsquo;ve often been asked which is better for text processing, NLTK or Scikit-Learn (and sometimes Gensim). The answer is that I use all three tools on a regular basis, but I often have a problem mixing and matching them or combining them in meaningful ways.","keywords":[],"articleBody":" This post is an early draft of expanded work that will eventually appear on the District Data Labs Blog. Your feedback is welcome, and you can submit your comments on the draft GitHub issue.\n I’ve often been asked which is better for text processing, NLTK or Scikit-Learn (and sometimes Gensim). The answer is that I use all three tools on a regular basis, but I often have a problem mixing and matching them or combining them in meaningful ways. In this post, I want to show how I use NLTK for preprocessing and tokenization, but then apply machine learning techniques (e.g. building a linear SVM using stochastic gradient descent) using Scikit-Learn. In a follow on post, I’ll talk about vectorizing text with word2vec for machine learning in Scikit-Learn.\nAs a note, in this post for the sake of speed, I’ll be building a text classifier on the movie reviews corpus that comes with NLTK. Here, movie reviews are classified as either positive or negative reviews and this follows a simple sentiment analysis pattern. In the DDL post, I will build a multi-class classifier using the Baleen corpus.\nIn order to follow along, make sure that you have NLTK and Scikit-Learn installed, and that you have downloaded the NLTK corpus:\n$ pip install nltk scikit-learn $ python -m nltk.downloader all I will also be using a few helper utilities like a timeit decorator and an identity function. The complete code for this project can be found here: sentiment.py. Note that I will also omit imports for the sake of brevity, so please review the complete code if trying to execute the snippets on this tutorial.\nPipelines The heart of building machine learning tools with Scikit-Learn is the Pipeline. Scikit-Learn exposes a standard API for machine learning that has two primary interfaces: Transformer and Estimator. Both transformers and estimators expose a fit method for adapting internal parameters based on data. Transformers then expose a transform method to perform feature extraction or modify the data for machine learning, and estimators expose a predict method to generate new data from feature vectors.\nPipelines allow developers to combine a sequential DAG of transformers with an estimator, to ensure that the feature extraction process is associated with the predictive process. This is especially important for text, where raw data is usually in the form of documents on disk or a list of strings. While Sckit-Learn does provide some text based feature extraction mechanisms, actually NLTK is far better suited for this type of text processing. As a result, most of my text processing pipelines have something like this at its core:\n\nThe CorpusReader reads files one at a time off a structured corpus (usually zipped) on disk and acts as the source of the data (I also usually include special methods to make sure that I can also get a vector of targets as well). The tokenizer splits raw text into sentences, words and punctuation, then tags their part of speech and lemmatizes them using the WordNet lexicon. The vectorizer encodes the tokens in the document as a feature vector, for example as a TF-IDF vector. Finally the classifier is fit to the documents and their labels, pickled to disk and used to make predictions in the future.\nPreprocessing In order to limit the number of features, as well as to provide a high quality representation of the text, I use NLTK’s advanced text processing mechanisms including the Punkt segmenter and tokenizer, the Brill tagger, and lemmatization using the WordNet lexicon. This not only reduces the vocabulary (and therefore the size of the feature vectors), it also combines redundant features into a single token (e.g. bunny, bunnies, Bunny, bunny!, and _bunny_ all become one feature: bunny).\nIn order to add this type of preprocessing to Scikit-Learn, we must create a Transformer object as follows:\nimport string from nltk.corpus import stopwords as sw from nltk.corpus import wordnet as wn from nltk import wordpunct_tokenize from nltk import WordNetLemmatizer from nltk import sent_tokenize from nltk import pos_tag from sklearn.base import BaseEstimator, TransformerMixin class NLTKPreprocessor(BaseEstimator, TransformerMixin): def __init__(self, stopwords=None, punct=None, lower=True, strip=True): self.lower = lower self.strip = strip self.stopwords = stopwords or set(sw.words('english')) self.punct = punct or set(string.punctuation) self.lemmatizer = WordNetLemmatizer() def fit(self, X, y=None): return self def inverse_transform(self, X): return [\" \".join(doc) for doc in X] def transform(self, X): return [ list(self.tokenize(doc)) for doc in X ] def tokenize(self, document): # Break the document into sentences for sent in sent_tokenize(document): # Break the sentence into part of speech tagged tokens for token, tag in pos_tag(wordpunct_tokenize(sent)): # Apply preprocessing to the token token = token.lower() if self.lower else token token = token.strip() if self.strip else token token = token.strip('_') if self.strip else token token = token.strip('*') if self.strip else token # If stopword, ignore token and continue if token in self.stopwords: continue # If punctuation, ignore token and continue if all(char in self.punct for char in token): continue # Lemmatize the token and yield lemma = self.lemmatize(token, tag) yield lemma def lemmatize(self, token, tag): tag = { 'N': wn.NOUN, 'V': wn.VERB, 'R': wn.ADV, 'J': wn.ADJ }.get(tag[0], wn.NOUN) return self.lemmatizer.lemmatize(token, tag) This is a big chunk of code, so we’ll go through it method by method. First when this transformer is initialized, it loads a variety of corpora and models for use in tokenization. By default the set of english stopwords from NLTK is used, and the WordNetLemmatizer looks up data from the WordNet lexicon. Note that this takes a noticeable amount of time, and should only be done on instantiation of the transformer.\nNext we have the Transformer interface methods: fit, inverse_transform, and transform. The first two are simply pass throughs since there is nothing to fit on this class, nor any ability to do inverse_transform — how would you take a lower case lemmatized, unordered tokens and come up with the original text? The best we can do is simply join the tokens with a space. The transform method takes a list of documents (given as the variable, X) and returns a new list of tokenized documents, where each document is transformed into list of ordered tokens.\nThe tokenize method breaks raw strings into sentences, then breaks those sentences into words and punctuation, and applies a part of speech tag. The token is then normalized: made lower case, then stripped of whitespace and other types of punctuation that may be appended. If the token is a stopword or if every character is punctuation, the token is ignored. If it is not ignored, the part of speech is used to lemmatize the token, which is then yielded.\nLemmatization is the process of looking up a single word form from the variety of morphologic affixes that can be applied to indicate tense, plurality, gender, etc. First we need to identify the WordNet tag form based on the Penn Treebank tag, which is returned from NLTK’s standard pos_tag function. We simply look to see if the Penn tag starts with ‘N’, ‘V’, ‘R’, or ‘J’ and can correctly identify if its a noun, verb, adverb, or adjective. We then use the new tag to look up the lemma in the lexicon.\nBuild and Evaluate The next stage is to create the pipeline, train a classifier, then to evaluate it. Here I present a very simple version of build and evaluate where:\n The model is split into a training and testing set by shuffling the data The model is trained on the training set, and evaluated on testing. A new model is then fit on all of the data and saved to disk.  Elsewhere we can discuss evaluation techniques like K-part cross validation, grid search for hyperparameter tuning, or visual diagnostics for machine learning. My simple method is as follows:\nfrom sklearn.pipeline import Pipeline from sklearn.preprocessing import LabelEncoder from sklearn.linear_model import SGDClassifier from sklearn.metrics import classification_report as clsr from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.cross_validation import train_test_split as tts @timeit def build_and_evaluate(X, y, classifier=SGDClassifier, outpath=None, verbose=True): @timeit def build(classifier, X, y=None): \"\"\" Inner build function that builds a single model. \"\"\" if isinstance(classifier, type): classifier = classifier() model = Pipeline([ ('preprocessor', NLTKPreprocessor()), ('vectorizer', TfidfVectorizer( tokenizer=identity, preprocessor=None, lowercase=False )), ('classifier', classifier), ]) model.fit(X, y) return model # Label encode the targets labels = LabelEncoder() y = labels.fit_transform(y) # Begin evaluation if verbose: print(\"Building for evaluation\") X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2) model, secs = build(classifier, X_train, y_train) if verbose: print(\"Evaluation model fit in {:0.3f} seconds\".format(secs)) print(\"Classification Report:\\n\") y_pred = model.predict(X_test) print(clsr(y_test, y_pred, target_names=labels.classes_)) if verbose: print(\"Building complete model and saving ...\") model, secs = build(classifier, X, y) model.labels_ = labels if verbose: print(\"Complete model fit in {:0.3f} seconds\".format(secs)) if outpath: with open(outpath, 'wb') as f: pickle.dump(model, f) print(\"Model written out to {}\".format(outpath)) return model This is a fairly procedural method of going about things. There is an inner function, build that takes a classifier class or instance (if given a class, it instantiates the classifier with the defaults) and creates the pipeline with that classifier and fits it. Note that when using the TfidfVectorizer you must make sure that its default preprocessor, normalizer, and tokenizer are all turned off using the identity function and passing None to the other parameters.\nThe function times the build process, evaluates it via the classification report that reports precision, recall, and F1. Then builds a new model on the complete dataset and writes it out to disk. In order to build the model, run the following code:\nfrom nltk.corpus import movie_reviews as reviews X = [reviews.raw(fileid) for fileid in reviews.fileids()] y = [reviews.categories(fileid)[0] for fileid in reviews.fileids()] model = build_and_evaluate(X,y, outpath=PATH) The output is as follows:\nBuilding for evaluation Evaluation model fit in 100.777 seconds Classification Report: precision recall f1-score support neg 0.84 0.84 0.84 193 pos 0.85 0.85 0.85 207 avg / total 0.84 0.84 0.84 400 Building complete model and saving ... Complete model fit in 115.402 seconds Model written out to model.pickle This is certainly not too bad — but consider how much time it took. For much larger corpora, you’ll only want to run this once, and in a time saving way. You could also preprocess your corpora in advance, however if you did so you would not be able to use the Pipeline as given, and would have to create separate feature extraction and modeling steps.\nMost Informative Features In order to use the model you just built, you would load the pickle from disk and use it’s predict method on new text as follows:\nwith open(PATH, 'rb') as f: model = pickle.load(f) yhat = model.predict([ \"This is the worst movie I have ever seen!\", \"The movie was action packed and full of adventure!\" ]) print(model.named_steps['classifier'].labels_.inverse_transform(yhat)) # ['neg' 'pos'] In order to better understand how our linear model makes these decisions, we can use the coefficients for each feature (a word) to determine its weight in terms of positivity (and because ‘pos’ is 1, this will be a positive number) and negativity (because ‘neg’ is 0 this will be a negative number). We can also vectorize a piece of text and see how it’s features inform the class decision by multiplying it’s vector against its weights as follows:\ndef show_most_informative_features(model, text=None, n=20): # Extract the vectorizer and the classifier from the pipeline vectorizer = model.named_steps['vectorizer'] classifier = model.named_steps['classifier'] # Check to make sure that we can perform this computation if not hasattr(classifier, 'coef_'): raise TypeError( \"Cannot compute most informative features on {}.\".format( classifier.__class__.__name__ ) ) if text is not None: # Compute the coefficients for the text tvec = model.transform([text]).toarray() else: # Otherwise simply use the coefficients tvec = classifier.coef_ # Zip the feature names with the coefs and sort coefs = sorted( zip(tvec[0], vectorizer.get_feature_names()), key=itemgetter(0), reverse=True ) # Get the top n and bottom n coef, name pairs topn = zip(coefs[:n], coefs[:-(n+1):-1]) # Create the output string to return output = [] # If text, add the predicted value to the output. if text is not None: output.append(\"\\\"{}\\\"\".format(text)) output.append( \"Classified as: {}\".format(model.predict([text])) ) output.append(\"\") # Create two columns with most negative and most positive features. for (cp, fnp), (cn, fnn) in topn: output.append( \"{:0.4f}{: 15} {:0.4f}{: 15}\".format( cp, fnp, cn, fnn ) ) return \"\\n\".join(output) For the model I trained, this reports the 20 most informative features for both positive and negative coefficients as follows:\n3.4326 fun -6.5962 bad 3.3835 great -3.2906 suppose 3.0014 performance -3.2527 plot 2.7226 see -3.1964 nothing 2.5224 quite -3.1688 attempt 2.5076 matrix -3.1104 unfortunately 2.1876 also -3.0741 waste 2.1336 true -2.5946 poor 2.1140 terrific -2.5943 boring 2.1076 different -2.5043 awful 2.0689 job -2.4893 ridiculous 2.0450 hilarious -2.4519 carpenter 2.0088 trek -2.4446 look 1.9704 memorable -2.2874 stupid 1.9501 well -2.2667 guess 1.9267 excellent -2.1953 even 1.8948 sometimes -2.1946 anyway 1.8939 perfectly -2.1719 lame 1.8506 bulworth -2.1406 reason 1.8453 portray -2.1098 script This seems to make a lot of sense!\nConclusion There are great tools for doing machine learning, topic modeling, and text analysis with Python: Scikit-Learn, Gensim, and NLTK respectively. Unfortunately in order to combine these tools in meaningful ways, you often have to jump through some hoops because they overlap. My approach was to leverage the API model of Scikit-Learn to build Pipelines of transformers that took advantage of other libraries.\nHelpful Links  Using Scikit-Learn Pipelines and FeatureUnions Working with Text Data: Sckit-Learn 0.17  ","wordCount":"2208","inLanguage":"en","datePublished":"2016-05-19T08:06:40Z","dateModified":"2016-05-19T08:06:40Z","author":{"@type":"Person","name":"Benjamin Bengfort"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bbengfort.github.io/2016/05/text-classification-nltk-sckit-learn/"},"publisher":{"@type":"Organization","name":"Libelli","logo":{"@type":"ImageObject","url":"https://bbengfort.github.io/favicon.ico"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: #1d1e20;--entry: #2e2e33;--primary: rgba(255, 255, 255, 0.84);--secondary: rgba(255, 255, 255, 0.56);--tertiary: rgba(255, 255, 255, 0.16);--content: rgba(255, 255, 255, 0.74);--hljs-bg: #2e2e33;--code-bg: #37383e;--border: #333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://bbengfort.github.io accesskey=h title="Libelli (Alt + H)"><img src=/icon.png alt=logo aria-label=logo height=35>Libelli</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://bbengfort.github.io/archive/ title=archive><span>archive</span></a></li><li><a href=https://bbengfort.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://bbengfort.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://bbengfort.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Text Classification with NLTK and Scikit-Learn</h1><div class=post-meta>May 19, 2016&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Benjamin Bengfort</div></header><div class=post-content><blockquote><p>This post is an early draft of expanded work that will eventually appear on the <a href=http://blog.districtdatalabs.com/>District Data Labs Blog</a>. Your feedback is welcome, and you can submit your comments on the <a href=https://github.com/bbengfort/bbengfort.github.io/issues/4>draft GitHub issue</a>.</p></blockquote><p>I&rsquo;ve often been asked which is better for text processing, NLTK or Scikit-Learn (and sometimes Gensim). The answer is that I use all three tools on a regular basis, but I often have a problem mixing and matching them or combining them in meaningful ways. In this post, I want to show how I use NLTK for preprocessing and tokenization, but then apply machine learning techniques (e.g. building a linear SVM using stochastic gradient descent) using Scikit-Learn. In a follow on post, I&rsquo;ll talk about vectorizing text with word2vec for machine learning in Scikit-Learn.</p><p>As a note, in this post for the sake of speed, I&rsquo;ll be building a text classifier on the movie reviews corpus that comes with NLTK. Here, movie reviews are classified as either positive or negative reviews and this follows a simple sentiment analysis pattern. In the DDL post, I will build a multi-class classifier using the Baleen corpus.</p><p>In order to follow along, make sure that you have NLTK and Scikit-Learn installed, and that you have downloaded the NLTK corpus:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ pip install nltk scikit-learn
$ python -m nltk.downloader all
</code></pre></div><p>I will also be using a few helper utilities like a <code>timeit</code> decorator and an <code>identity</code> function. The complete code for this project can be found here: <a href=https://gist.github.com/bbengfort/044682e76def583a12e6c09209c664a1>sentiment.py</a>. Note that I will also omit imports for the sake of brevity, so please review the complete code if trying to execute the snippets on this tutorial.</p><h2 id=pipelines>Pipelines<a hidden class=anchor aria-hidden=true href=#pipelines>#</a></h2><p>The heart of building machine learning tools with Scikit-Learn is the <code>Pipeline</code>. Scikit-Learn exposes a standard API for machine learning that has two primary interfaces: <code>Transformer</code> and <code>Estimator</code>. Both transformers and estimators expose a <code>fit</code> method for adapting internal parameters based on data. Transformers then expose a <code>transform</code> method to perform feature extraction or modify the data for machine learning, and estimators expose a <code>predict</code> method to generate new data from feature vectors.</p><p>Pipelines allow developers to combine a sequential DAG of transformers with an estimator, to ensure that the feature extraction process is associated with the predictive process. This is especially important for text, where raw data is usually in the form of documents on disk or a list of strings. While Sckit-Learn does provide some text based feature extraction mechanisms, actually NLTK is far better suited for this type of text processing. As a result, most of my text processing pipelines have something like this at its core:</p><p><a href=/images/2016-05-19-nltk-sklearn-text-pipeline.png><img loading=lazy src=/images/2016-05-19-nltk-sklearn-text-pipeline.png alt="NLTK Scikit-Learn Text Pipeline"></a></p><p>The <code>CorpusReader</code> reads files one at a time off a structured corpus (usually zipped) on disk and acts as the source of the data (I also usually include special methods to make sure that I can also get a vector of targets as well). The tokenizer splits raw text into sentences, words and punctuation, then tags their part of speech and lemmatizes them using the WordNet lexicon. The vectorizer encodes the tokens in the document as a feature vector, for example as a TF-IDF vector. Finally the classifier is fit to the documents and their labels, pickled to disk and used to make predictions in the future.</p><h2 id=preprocessing>Preprocessing<a hidden class=anchor aria-hidden=true href=#preprocessing>#</a></h2><p>In order to limit the number of features, as well as to provide a high quality representation of the text, I use NLTK&rsquo;s advanced text processing mechanisms including the Punkt segmenter and tokenizer, the Brill tagger, and lemmatization using the WordNet lexicon. This not only reduces the vocabulary (and therefore the size of the feature vectors), it also combines redundant features into a single token (e.g. <code>bunny</code>, <code>bunnies</code>, <code>Bunny</code>, <code>bunny!</code>, and <code>_bunny_</code> all become one feature: <code>bunny</code>).</p><p>In order to add this type of preprocessing to Scikit-Learn, we must create a Transformer object as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> string

<span style=color:#f92672>from</span> nltk.corpus <span style=color:#f92672>import</span> stopwords <span style=color:#66d9ef>as</span> sw
<span style=color:#f92672>from</span> nltk.corpus <span style=color:#f92672>import</span> wordnet <span style=color:#66d9ef>as</span> wn
<span style=color:#f92672>from</span> nltk <span style=color:#f92672>import</span> wordpunct_tokenize
<span style=color:#f92672>from</span> nltk <span style=color:#f92672>import</span> WordNetLemmatizer
<span style=color:#f92672>from</span> nltk <span style=color:#f92672>import</span> sent_tokenize
<span style=color:#f92672>from</span> nltk <span style=color:#f92672>import</span> pos_tag

<span style=color:#f92672>from</span> sklearn.base <span style=color:#f92672>import</span> BaseEstimator, TransformerMixin


<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NLTKPreprocessor</span>(BaseEstimator, TransformerMixin):

    <span style=color:#66d9ef>def</span> __init__(self, stopwords<span style=color:#f92672>=</span>None, punct<span style=color:#f92672>=</span>None,
                 lower<span style=color:#f92672>=</span>True, strip<span style=color:#f92672>=</span>True):
        self<span style=color:#f92672>.</span>lower      <span style=color:#f92672>=</span> lower
        self<span style=color:#f92672>.</span>strip      <span style=color:#f92672>=</span> strip
        self<span style=color:#f92672>.</span>stopwords  <span style=color:#f92672>=</span> stopwords <span style=color:#f92672>or</span> set(sw<span style=color:#f92672>.</span>words(<span style=color:#e6db74>&#39;english&#39;</span>))
        self<span style=color:#f92672>.</span>punct      <span style=color:#f92672>=</span> punct <span style=color:#f92672>or</span> set(string<span style=color:#f92672>.</span>punctuation)
        self<span style=color:#f92672>.</span>lemmatizer <span style=color:#f92672>=</span> WordNetLemmatizer()

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y<span style=color:#f92672>=</span>None):
        <span style=color:#66d9ef>return</span> self

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>inverse_transform</span>(self, X):
        <span style=color:#66d9ef>return</span> [<span style=color:#e6db74>&#34; &#34;</span><span style=color:#f92672>.</span>join(doc) <span style=color:#66d9ef>for</span> doc <span style=color:#f92672>in</span> X]

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, X):
        <span style=color:#66d9ef>return</span> [
            list(self<span style=color:#f92672>.</span>tokenize(doc)) <span style=color:#66d9ef>for</span> doc <span style=color:#f92672>in</span> X
        ]

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tokenize</span>(self, document):
        <span style=color:#75715e># Break the document into sentences</span>
        <span style=color:#66d9ef>for</span> sent <span style=color:#f92672>in</span> sent_tokenize(document):
            <span style=color:#75715e># Break the sentence into part of speech tagged tokens</span>
            <span style=color:#66d9ef>for</span> token, tag <span style=color:#f92672>in</span> pos_tag(wordpunct_tokenize(sent)):
                <span style=color:#75715e># Apply preprocessing to the token</span>
                token <span style=color:#f92672>=</span> token<span style=color:#f92672>.</span>lower() <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>lower <span style=color:#66d9ef>else</span> token
                token <span style=color:#f92672>=</span> token<span style=color:#f92672>.</span>strip() <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>strip <span style=color:#66d9ef>else</span> token
                token <span style=color:#f92672>=</span> token<span style=color:#f92672>.</span>strip(<span style=color:#e6db74>&#39;_&#39;</span>) <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>strip <span style=color:#66d9ef>else</span> token
                token <span style=color:#f92672>=</span> token<span style=color:#f92672>.</span>strip(<span style=color:#e6db74>&#39;*&#39;</span>) <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>strip <span style=color:#66d9ef>else</span> token

                <span style=color:#75715e># If stopword, ignore token and continue</span>
                <span style=color:#66d9ef>if</span> token <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>stopwords:
                    <span style=color:#66d9ef>continue</span>

                <span style=color:#75715e># If punctuation, ignore token and continue</span>
                <span style=color:#66d9ef>if</span> all(char <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>punct <span style=color:#66d9ef>for</span> char <span style=color:#f92672>in</span> token):
                    <span style=color:#66d9ef>continue</span>

                <span style=color:#75715e># Lemmatize the token and yield</span>
                lemma <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lemmatize(token, tag)
                <span style=color:#66d9ef>yield</span> lemma

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>lemmatize</span>(self, token, tag):
        tag <span style=color:#f92672>=</span> {
            <span style=color:#e6db74>&#39;N&#39;</span>: wn<span style=color:#f92672>.</span>NOUN,
            <span style=color:#e6db74>&#39;V&#39;</span>: wn<span style=color:#f92672>.</span>VERB,
            <span style=color:#e6db74>&#39;R&#39;</span>: wn<span style=color:#f92672>.</span>ADV,
            <span style=color:#e6db74>&#39;J&#39;</span>: wn<span style=color:#f92672>.</span>ADJ
        }<span style=color:#f92672>.</span>get(tag[<span style=color:#ae81ff>0</span>], wn<span style=color:#f92672>.</span>NOUN)

        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>lemmatizer<span style=color:#f92672>.</span>lemmatize(token, tag)
</code></pre></div><p>This is a big chunk of code, so we&rsquo;ll go through it method by method. First when this transformer is initialized, it loads a variety of corpora and models for use in tokenization. By default the set of english stopwords from NLTK is used, and the <code>WordNetLemmatizer</code> looks up data from the WordNet lexicon. Note that this takes a noticeable amount of time, and should only be done on instantiation of the transformer.</p><p>Next we have the <code>Transformer</code> interface methods: <code>fit</code>, <code>inverse_transform</code>, and <code>transform</code>. The first two are simply pass throughs since there is nothing to fit on this class, nor any ability to do <code>inverse_transform</code> — how would you take a lower case lemmatized, unordered tokens and come up with the original text? The best we can do is simply join the tokens with a space. The <code>transform</code> method takes a list of documents (given as the variable, X) and returns a new list of tokenized documents, where each document is transformed into list of ordered tokens.</p><p>The tokenize method breaks raw strings into sentences, then breaks those sentences into words and punctuation, and applies a part of speech tag. The token is then normalized: made lower case, then stripped of whitespace and other types of punctuation that may be appended. If the token is a stopword or if every character is punctuation, the token is ignored. If it is not ignored, the part of speech is used to lemmatize the token, which is then yielded.</p><p>Lemmatization is the process of looking up a single word form from the variety of morphologic affixes that can be applied to indicate tense, plurality, gender, etc. First we need to identify the WordNet tag form based on the Penn Treebank tag, which is returned from NLTK&rsquo;s standard <code>pos_tag</code> function. We simply look to see if the Penn tag starts with &lsquo;N&rsquo;, &lsquo;V&rsquo;, &lsquo;R&rsquo;, or &lsquo;J&rsquo; and can correctly identify if its a noun, verb, adverb, or adjective. We then use the new tag to look up the lemma in the lexicon.</p><h2 id=build-and-evaluate>Build and Evaluate<a hidden class=anchor aria-hidden=true href=#build-and-evaluate>#</a></h2><p>The next stage is to create the pipeline, train a classifier, then to evaluate it. Here I present a very simple version of build and evaluate where:</p><ol><li>The model is split into a training and testing set by shuffling the data</li><li>The model is trained on the training set, and evaluated on testing.</li><li>A new model is then fit on all of the data and saved to disk.</li></ol><p>Elsewhere we can discuss evaluation techniques like K-part cross validation, grid search for hyperparameter tuning, or visual diagnostics for machine learning. My simple method is as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> sklearn.pipeline <span style=color:#f92672>import</span> Pipeline
<span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> LabelEncoder
<span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> SGDClassifier
<span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> classification_report <span style=color:#66d9ef>as</span> clsr
<span style=color:#f92672>from</span> sklearn.feature_extraction.text <span style=color:#f92672>import</span> TfidfVectorizer
<span style=color:#f92672>from</span> sklearn.cross_validation <span style=color:#f92672>import</span> train_test_split <span style=color:#66d9ef>as</span> tts

<span style=color:#a6e22e>@timeit</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_and_evaluate</span>(X, y,
    classifier<span style=color:#f92672>=</span>SGDClassifier, outpath<span style=color:#f92672>=</span>None, verbose<span style=color:#f92672>=</span>True):

    <span style=color:#a6e22e>@timeit</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build</span>(classifier, X, y<span style=color:#f92672>=</span>None):
        <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>        Inner build function that builds a single model.
</span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
        <span style=color:#66d9ef>if</span> isinstance(classifier, type):
            classifier <span style=color:#f92672>=</span> classifier()

        model <span style=color:#f92672>=</span> Pipeline([
            (<span style=color:#e6db74>&#39;preprocessor&#39;</span>, NLTKPreprocessor()),
            (<span style=color:#e6db74>&#39;vectorizer&#39;</span>, TfidfVectorizer(
                tokenizer<span style=color:#f92672>=</span>identity, preprocessor<span style=color:#f92672>=</span>None, lowercase<span style=color:#f92672>=</span>False
            )),
            (<span style=color:#e6db74>&#39;classifier&#39;</span>, classifier),
        ])

        model<span style=color:#f92672>.</span>fit(X, y)
        <span style=color:#66d9ef>return</span> model

    <span style=color:#75715e># Label encode the targets</span>
    labels <span style=color:#f92672>=</span> LabelEncoder()
    y <span style=color:#f92672>=</span> labels<span style=color:#f92672>.</span>fit_transform(y)

    <span style=color:#75715e># Begin evaluation</span>
    <span style=color:#66d9ef>if</span> verbose: <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Building for evaluation&#34;</span>)
    X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> tts(X, y, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>)
    model, secs <span style=color:#f92672>=</span> build(classifier, X_train, y_train)

    <span style=color:#66d9ef>if</span> verbose:
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Evaluation model fit in {:0.3f} seconds&#34;</span><span style=color:#f92672>.</span>format(secs))
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Classification Report:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)

    y_pred <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(X_test)
    <span style=color:#66d9ef>print</span>(clsr(y_test, y_pred, target_names<span style=color:#f92672>=</span>labels<span style=color:#f92672>.</span>classes_))

    <span style=color:#66d9ef>if</span> verbose:
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Building complete model and saving ...&#34;</span>)
    model, secs <span style=color:#f92672>=</span> build(classifier, X, y)
    model<span style=color:#f92672>.</span>labels_ <span style=color:#f92672>=</span> labels

    <span style=color:#66d9ef>if</span> verbose:
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Complete model fit in {:0.3f} seconds&#34;</span><span style=color:#f92672>.</span>format(secs))

    <span style=color:#66d9ef>if</span> outpath:
        <span style=color:#66d9ef>with</span> open(outpath, <span style=color:#e6db74>&#39;wb&#39;</span>) <span style=color:#66d9ef>as</span> f:
            pickle<span style=color:#f92672>.</span>dump(model, f)

        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Model written out to {}&#34;</span><span style=color:#f92672>.</span>format(outpath))

    <span style=color:#66d9ef>return</span> model
</code></pre></div><p>This is a fairly procedural method of going about things. There is an inner function, <code>build</code> that takes a classifier class or instance (if given a class, it instantiates the classifier with the defaults) and creates the pipeline with that classifier and fits it. Note that when using the <code>TfidfVectorizer</code> you must make sure that its default preprocessor, normalizer, and tokenizer are all turned off using the identity function and passing <code>None</code> to the other parameters.</p><p>The function times the build process, evaluates it via the classification report that reports precision, recall, and F1. Then builds a new model on the complete dataset and writes it out to disk. In order to build the model, run the following code:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> nltk.corpus <span style=color:#f92672>import</span> movie_reviews <span style=color:#66d9ef>as</span> reviews

X <span style=color:#f92672>=</span> [reviews<span style=color:#f92672>.</span>raw(fileid) <span style=color:#66d9ef>for</span> fileid <span style=color:#f92672>in</span> reviews<span style=color:#f92672>.</span>fileids()]
y <span style=color:#f92672>=</span> [reviews<span style=color:#f92672>.</span>categories(fileid)[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>for</span> fileid <span style=color:#f92672>in</span> reviews<span style=color:#f92672>.</span>fileids()]

model <span style=color:#f92672>=</span> build_and_evaluate(X,y, outpath<span style=color:#f92672>=</span>PATH)
</code></pre></div><p>The output is as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>Building for evaluation
Evaluation model fit in 100.777 seconds
Classification Report:

             precision    recall  f1-score   support

        neg       0.84      0.84      0.84       193
        pos       0.85      0.85      0.85       207

avg / total       0.84      0.84      0.84       400

Building complete model and saving ...
Complete model fit in 115.402 seconds
Model written out to model.pickle
</code></pre></div><p>This is certainly not too bad — but consider how much time it took. For much larger corpora, you&rsquo;ll only want to run this once, and in a time saving way. You could also preprocess your corpora in advance, however if you did so you would not be able to use the Pipeline as given, and would have to create separate feature extraction and modeling steps.</p><h2 id=most-informative-features>Most Informative Features<a hidden class=anchor aria-hidden=true href=#most-informative-features>#</a></h2><p>In order to use the model you just built, you would load the pickle from disk and use it&rsquo;s <code>predict</code> method on new text as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>with</span> open(PATH, <span style=color:#e6db74>&#39;rb&#39;</span>) <span style=color:#66d9ef>as</span> f:
    model <span style=color:#f92672>=</span> pickle<span style=color:#f92672>.</span>load(f)

yhat <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict([
    <span style=color:#e6db74>&#34;This is the worst movie I have ever seen!&#34;</span>,
    <span style=color:#e6db74>&#34;The movie was action packed and full of adventure!&#34;</span>
])

<span style=color:#66d9ef>print</span>(model<span style=color:#f92672>.</span>named_steps[<span style=color:#e6db74>&#39;classifier&#39;</span>]<span style=color:#f92672>.</span>labels_<span style=color:#f92672>.</span>inverse_transform(yhat))
<span style=color:#75715e># [&#39;neg&#39; &#39;pos&#39;]</span>
</code></pre></div><p>In order to better understand how our linear model makes these decisions, we can use the coefficients for each feature (a word) to determine its weight in terms of positivity (and because &lsquo;pos&rsquo; is 1, this will be a positive number) and negativity (because &lsquo;neg&rsquo; is 0 this will be a negative number). We can also vectorize a piece of text and see how it&rsquo;s features inform the class decision by multiplying it&rsquo;s vector against its weights as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>show_most_informative_features</span>(model, text<span style=color:#f92672>=</span>None, n<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>):
    <span style=color:#75715e># Extract the vectorizer and the classifier from the pipeline</span>
    vectorizer <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>named_steps[<span style=color:#e6db74>&#39;vectorizer&#39;</span>]
    classifier <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>named_steps[<span style=color:#e6db74>&#39;classifier&#39;</span>]

    <span style=color:#75715e># Check to make sure that we can perform this computation</span>
    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> hasattr(classifier, <span style=color:#e6db74>&#39;coef_&#39;</span>):
        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>TypeError</span>(
            <span style=color:#e6db74>&#34;Cannot compute most informative features on {}.&#34;</span><span style=color:#f92672>.</span>format(
                classifier<span style=color:#f92672>.</span>__class__<span style=color:#f92672>.</span>__name__
            )
        )

    <span style=color:#66d9ef>if</span> text <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None:
        <span style=color:#75715e># Compute the coefficients for the text</span>
        tvec <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>transform([text])<span style=color:#f92672>.</span>toarray()
    <span style=color:#66d9ef>else</span>:
        <span style=color:#75715e># Otherwise simply use the coefficients</span>
        tvec <span style=color:#f92672>=</span> classifier<span style=color:#f92672>.</span>coef_

    <span style=color:#75715e># Zip the feature names with the coefs and sort</span>
    coefs <span style=color:#f92672>=</span> sorted(
        zip(tvec[<span style=color:#ae81ff>0</span>], vectorizer<span style=color:#f92672>.</span>get_feature_names()),
        key<span style=color:#f92672>=</span>itemgetter(<span style=color:#ae81ff>0</span>), reverse<span style=color:#f92672>=</span>True
    )

    <span style=color:#75715e># Get the top n and bottom n coef, name pairs</span>
    topn  <span style=color:#f92672>=</span> zip(coefs[:n], coefs[:<span style=color:#f92672>-</span>(n<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>):<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])

    <span style=color:#75715e># Create the output string to return</span>
    output <span style=color:#f92672>=</span> []

    <span style=color:#75715e># If text, add the predicted value to the output.</span>
    <span style=color:#66d9ef>if</span> text <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None:
        output<span style=color:#f92672>.</span>append(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>{}</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(text))
        output<span style=color:#f92672>.</span>append(
            <span style=color:#e6db74>&#34;Classified as: {}&#34;</span><span style=color:#f92672>.</span>format(model<span style=color:#f92672>.</span>predict([text]))
        )
        output<span style=color:#f92672>.</span>append(<span style=color:#e6db74>&#34;&#34;</span>)

    <span style=color:#75715e># Create two columns with most negative and most positive features.</span>
    <span style=color:#66d9ef>for</span> (cp, fnp), (cn, fnn) <span style=color:#f92672>in</span> topn:
        output<span style=color:#f92672>.</span>append(
            <span style=color:#e6db74>&#34;{:0.4f}{: &gt;15}    {:0.4f}{: &gt;15}&#34;</span><span style=color:#f92672>.</span>format(
                cp, fnp, cn, fnn
            )
        )

    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>join(output)
</code></pre></div><p>For the model I trained, this reports the 20 most informative features for both positive and negative coefficients as follows:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>3.4326            fun    -6.5962            bad
3.3835          great    -3.2906        suppose
3.0014    performance    -3.2527           plot
2.7226            see    -3.1964        nothing
2.5224          quite    -3.1688        attempt
2.5076         matrix    -3.1104  unfortunately
2.1876           also    -3.0741          waste
2.1336           true    -2.5946           poor
2.1140       terrific    -2.5943         boring
2.1076      different    -2.5043          awful
2.0689            job    -2.4893     ridiculous
2.0450      hilarious    -2.4519      carpenter
2.0088           trek    -2.4446           look
1.9704      memorable    -2.2874         stupid
1.9501           well    -2.2667          guess
1.9267      excellent    -2.1953           even
1.8948      sometimes    -2.1946         anyway
1.8939      perfectly    -2.1719           lame
1.8506       bulworth    -2.1406         reason
1.8453        portray    -2.1098         script
</code></pre></div><p>This seems to make a lot of sense!</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>There are great tools for doing machine learning, topic modeling, and text analysis with Python: Scikit-Learn, Gensim, and NLTK respectively. Unfortunately in order to combine these tools in meaningful ways, you often have to jump through some hoops because they overlap. My approach was to leverage the API model of Scikit-Learn to build Pipelines of transformers that took advantage of other libraries.</p><h3 id=helpful-links>Helpful Links<a hidden class=anchor aria-hidden=true href=#helpful-links>#</a></h3><ul><li><a href=http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html>Using Scikit-Learn Pipelines and FeatureUnions</a></li><li><a href=http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html>Working with Text Data: Sckit-Learn 0.17</a></li></ul></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2021 <a href=https://bbengfort.github.io>Libelli</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu')
menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>