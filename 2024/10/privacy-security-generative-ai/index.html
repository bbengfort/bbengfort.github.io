<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Privacy and Security in the Age of Generative AI | Libelli</title>
<meta name=keywords content><meta name=description content="Privacy and Security in the Age of Generative AI is a talk that I gave at ODSC West 2024 in Burlingame, California. The slides of the talk are below:
 

An updated presentation that I gave at C4AI on April 15, 2025 in Columbia, Maryland is below:
 

Abstract
From sensitive data leakage to prompt injection and zero-click worms, LLMs and generative models are the new cyber battleground for hackers. As more AI models are deployed in production, data scientists and ML engineers can&rsquo;t ignore these problems. The good news is that we can influence privacy and security in the machine learning lifecycle using data specific techniques. In this talk, we&rsquo;ll review some of the newest security concerns affecting LLMs and deep learning models and learn how to embed privacy into model training with ACLs and differential privacy, secure text generation and function-calling interfaces, and even leverage models to defend other models."><meta name=author content="Benjamin Bengfort"><link rel=canonical href=https://bbengfort.github.io/2024/10/privacy-security-generative-ai/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://bbengfort.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bbengfort.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://bbengfort.github.io/icon.png><link rel=apple-touch-icon href=https://bbengfort.github.io/apple-touch-icon-precomposed.png><link rel=mask-icon href=https://bbengfort.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://bbengfort.github.io/2024/10/privacy-security-generative-ai/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-D3BE7EHHVP"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D3BE7EHHVP")}</script><meta property="og:title" content="Privacy and Security in the Age of Generative AI"><meta property="og:description" content="Privacy and Security in the Age of Generative AI is a talk that I gave at ODSC West 2024 in Burlingame, California. The slides of the talk are below:
 

An updated presentation that I gave at C4AI on April 15, 2025 in Columbia, Maryland is below:
 

Abstract
From sensitive data leakage to prompt injection and zero-click worms, LLMs and generative models are the new cyber battleground for hackers. As more AI models are deployed in production, data scientists and ML engineers can&rsquo;t ignore these problems. The good news is that we can influence privacy and security in the machine learning lifecycle using data specific techniques. In this talk, we&rsquo;ll review some of the newest security concerns affecting LLMs and deep learning models and learn how to embed privacy into model training with ACLs and differential privacy, secure text generation and function-calling interfaces, and even leverage models to defend other models."><meta property="og:type" content="article"><meta property="og:url" content="https://bbengfort.github.io/2024/10/privacy-security-generative-ai/"><meta property="og:image" content="https://bbengfort.github.io/bear.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-30T11:35:00-07:00"><meta property="article:modified_time" content="2024-10-30T11:35:00-07:00"><meta property="og:site_name" content="Libelli"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bbengfort.github.io/bear.png"><meta name=twitter:title content="Privacy and Security in the Age of Generative AI"><meta name=twitter:description content="Privacy and Security in the Age of Generative AI is a talk that I gave at ODSC West 2024 in Burlingame, California. The slides of the talk are below:
 

An updated presentation that I gave at C4AI on April 15, 2025 in Columbia, Maryland is below:
 

Abstract
From sensitive data leakage to prompt injection and zero-click worms, LLMs and generative models are the new cyber battleground for hackers. As more AI models are deployed in production, data scientists and ML engineers can&rsquo;t ignore these problems. The good news is that we can influence privacy and security in the machine learning lifecycle using data specific techniques. In this talk, we&rsquo;ll review some of the newest security concerns affecting LLMs and deep learning models and learn how to embed privacy into model training with ACLs and differential privacy, secure text generation and function-calling interfaces, and even leverage models to defend other models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://bbengfort.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Privacy and Security in the Age of Generative AI","item":"https://bbengfort.github.io/2024/10/privacy-security-generative-ai/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Privacy and Security in the Age of Generative AI","name":"Privacy and Security in the Age of Generative AI","description":"Privacy and Security in the Age of Generative AI is a talk that I gave at ODSC West 2024 in Burlingame, California. The slides of the talk are below:\nAn updated presentation that I gave at C4AI on April 15, 2025 in Columbia, Maryland is below:\nAbstract From sensitive data leakage to prompt injection and zero-click worms, LLMs and generative models are the new cyber battleground for hackers. As more AI models are deployed in production, data scientists and ML engineers can\u0026rsquo;t ignore these problems. The good news is that we can influence privacy and security in the machine learning lifecycle using data specific techniques. In this talk, we\u0026rsquo;ll review some of the newest security concerns affecting LLMs and deep learning models and learn how to embed privacy into model training with ACLs and differential privacy, secure text generation and function-calling interfaces, and even leverage models to defend other models.\n","keywords":[],"articleBody":"Privacy and Security in the Age of Generative AI is a talk that I gave at ODSC West 2024 in Burlingame, California. The slides of the talk are below:\nAn updated presentation that I gave at C4AI on April 15, 2025 in Columbia, Maryland is below:\nAbstract From sensitive data leakage to prompt injection and zero-click worms, LLMs and generative models are the new cyber battleground for hackers. As more AI models are deployed in production, data scientists and ML engineers can’t ignore these problems. The good news is that we can influence privacy and security in the machine learning lifecycle using data specific techniques. In this talk, we’ll review some of the newest security concerns affecting LLMs and deep learning models and learn how to embed privacy into model training with ACLs and differential privacy, secure text generation and function-calling interfaces, and even leverage models to defend other models.\nSession Outline: Security Concerns in Generative AI (6 minutes) Data Access Controls for MLOps (6 minutes) Building Privacy into Models (4 minutes) LLM Model Evaluation (4 minutes) Security Context for TGI and Function Calling (6 minutes) Can Models Secure Models? (4 minutes) Learning Objectives: Generative AI is an amazing interface for human users to better access the huge amounts of data with natural language queries. However, as AI becomes more important in automating repetitive, inconsistent, and boring tasks it has also become a target for hackers and malicious actors.\nImage modification attacks such as modifying pixels in an image or adding stickers to signs can dramatically influence the output of computer vision systems and classifiers: usually to cause harmful actions to occur (e.g. to cause a vehicle to change lanes or for a fake driver’s license to be recognized). Prompt injection attacks are used to manipulate LLMs into leaking sensitive data or spread misinformation. Researchers have even recently shown that AI worms are possible that target generative AI systems through adversarial self-replicating prompts.\nAs data scientists, we already have a lot of concerns from model quality and generalization to bias and fairness in our outputs; do we really need to take on security and privacy also? Data scientists and machine learning engineers use data to build data products for our users. Generative AI attacks are based on data, and therefore it is squarely in our purview as data scientists to ensure that we create high quality data pipelines and models that preserve the security and privacy of our users and organizations when used in combination with application security techniques.\nIn this talk we will explore data-driven techniques for privacy and security that will augment the security best practices of the application and product teams we belong to.\nWe know that the quality of a model is based on the data, so to is the security of the model. We’ll explore the use of data access controls to influence model training and inferencing. We’ll also look at algorithmic approaches such as differential privacy to prevent model leakage. Finally we’ll explore how to combine security context and awareness in text generation inferences and function calling LLMs.\nAt the end of the the talk we’ll touch on an open question for security researchers: can AI models be used to enhance the security of other models and more rapidly detect emergent threats?\n","wordCount":"548","inLanguage":"en","image":"https://bbengfort.github.io/bear.png","datePublished":"2024-10-30T11:35:00-07:00","dateModified":"2024-10-30T11:35:00-07:00","author":{"@type":"Person","name":"Benjamin Bengfort"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bbengfort.github.io/2024/10/privacy-security-generative-ai/"},"publisher":{"@type":"Organization","name":"Libelli","logo":{"@type":"ImageObject","url":"https://bbengfort.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://bbengfort.github.io/ accesskey=h title="Libelli (Alt + H)"><img src=https://bbengfort.github.io/icon.png alt aria-label=logo height=35>Libelli</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://bbengfort.github.io/archive/ title=archive><span>archive</span></a></li><li><a href=https://bbengfort.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://bbengfort.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://bbengfort.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://bbengfort.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://bbengfort.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Privacy and Security in the Age of Generative AI</h1><div class=post-meta><span title='2024-10-30 11:35:00 -0700 -0700'>October 30, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;548 words&nbsp;·&nbsp;Benjamin Bengfort&nbsp;|&nbsp;<a href=https://github.com/bbengfort/bbengfort.github.io/tree/main/content/posts/2024-10-30-privacy-security-generative-ai.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p><a href=https://odsc.com/speakers/privacy-and-security-in-the-age-of-generative-ai/>Privacy and Security in the Age of Generative AI</a> is a talk that I gave at ODSC West 2024 in Burlingame, California. The slides of the talk are below:</p><iframe style=width:100%;height:500px frameborder=0 marginwidth=0 marginheight=0 scrolling=no src="https://www.slideshare.net/slideshow/embed_code/272867721?rel=0" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe><br><br><p>An updated presentation that I gave at C4AI on April 15, 2025 in Columbia, Maryland is below:</p><iframe style=width:100%;height:500px frameborder=0 marginwidth=0 marginheight=0 scrolling=no src="https://www.slideshare.net/slideshow/embed_code/278029566?rel=0" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe><br><br><h3 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h3><p>From sensitive data leakage to prompt injection and zero-click worms, LLMs and generative models are the new cyber battleground for hackers. As more AI models are deployed in production, data scientists and ML engineers can&rsquo;t ignore these problems. The good news is that we can influence privacy and security in the machine learning lifecycle using data specific techniques. In this talk, we&rsquo;ll review some of the newest security concerns affecting LLMs and deep learning models and learn how to embed privacy into model training with ACLs and differential privacy, secure text generation and function-calling interfaces, and even leverage models to defend other models.</p><h3 id=session-outline>Session Outline:<a hidden class=anchor aria-hidden=true href=#session-outline>#</a></h3><ol><li>Security Concerns in Generative AI (6 minutes)</li><li>Data Access Controls for MLOps (6 minutes)</li><li>Building Privacy into Models (4 minutes)</li><li>LLM Model Evaluation (4 minutes)</li><li>Security Context for TGI and Function Calling (6 minutes)</li><li>Can Models Secure Models? (4 minutes)</li></ol><h3 id=learning-objectives>Learning Objectives:<a hidden class=anchor aria-hidden=true href=#learning-objectives>#</a></h3><p>Generative AI is an amazing interface for human users to better access the huge amounts of data with natural language queries. However, as AI becomes more important in automating repetitive, inconsistent, and boring tasks it has also become a target for hackers and malicious actors.</p><p>Image modification attacks such as modifying pixels in an image or adding stickers to signs can dramatically influence the output of computer vision systems and classifiers: usually to cause harmful actions to occur (e.g. to cause a vehicle to change lanes or for a fake driver&rsquo;s license to be recognized). Prompt injection attacks are used to manipulate LLMs into leaking sensitive data or spread misinformation. Researchers have even recently shown that AI worms are possible that target generative AI systems through adversarial self-replicating prompts.</p><p>As data scientists, we already have a lot of concerns from model quality and generalization to bias and fairness in our outputs; do we really need to take on security and privacy also? Data scientists and machine learning engineers use data to build data products for our users. Generative AI attacks are based on data, and therefore it is squarely in our purview as data scientists to ensure that we create high quality data pipelines and models that preserve the security and privacy of our users and organizations when used in combination with application security techniques.</p><p>In this talk we will explore data-driven techniques for privacy and security that will augment the security best practices of the application and product teams we belong to.</p><p>We know that the quality of a model is based on the data, so to is the security of the model. We&rsquo;ll explore the use of data access controls to influence model training and inferencing. We&rsquo;ll also look at algorithmic approaches such as differential privacy to prevent model leakage. Finally we&rsquo;ll explore how to combine security context and awareness in text generation inferences and function calling LLMs.</p><p>At the end of the the talk we&rsquo;ll touch on an open question for security researchers: can AI models be used to enhance the security of other models and more rapidly detect emergent threats?</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://bbengfort.github.io/2025/04/function-calling-without-fear/><span class=title>« Prev</span><br><span>Implementing Function Calling LLMs without Fear</span>
</a><a class=next href=https://bbengfort.github.io/2023/11/smart-global-replication-using-reinforcement-learning/><span class=title>Next »</span><br><span>Smart Global Replication Using Reinforcement Learning</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://bbengfort.github.io/>Libelli</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>