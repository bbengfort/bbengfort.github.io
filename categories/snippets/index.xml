<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>snippets on Libelli</title>
    <link>https://bbengfort.github.io/categories/snippets/</link>
    <description>Recent content in snippets on Libelli</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 26 Nov 2022 12:24:25 -0600</lastBuildDate><atom:link href="https://bbengfort.github.io/categories/snippets/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Atomic vs Mutex</title>
      <link>https://bbengfort.github.io/2022/11/atomic-vs-mutex/</link>
      <pubDate>Sat, 26 Nov 2022 12:24:25 -0600</pubDate>
      
      <guid>https://bbengfort.github.io/2022/11/atomic-vs-mutex/</guid>
      <description>When implementing Go code, I find myself chasing increased concurrency performance by trying to reduce the number of locks in my code. Often I wonder if using the sync/atomic package is a better choice because I know (as proved by this blog post) that atomics have far more performance than mutexes. The issue is that reading on the internet, including the package documentation itself strongly recommends relying on channels, then mutexes, and finally atomics only if you know what you&amp;rsquo;re doing.</description>
    </item>
    
    <item>
      <title>Nonlinear Workflow for Planning Software Projects</title>
      <link>https://bbengfort.github.io/2021/03/nonlinear-workflow-planning-software-projects/</link>
      <pubDate>Sun, 14 Mar 2021 09:53:49 -0400</pubDate>
      
      <guid>https://bbengfort.github.io/2021/03/nonlinear-workflow-planning-software-projects/</guid>
      <description>Good software development achieves complexity by describing the interactions between simpler components. Although we tend to think of software processes as step-by-step &amp;ldquo;wizards&amp;rdquo;, design and decoupling of components often means that the interactions are non-linear. So why should our software project planning be defined in a linear progression of steps with time estimates? Can we plan projects using a non-linear workflow that mirrors how we think about component design?
The figure above is an experiment in task planning that I recently used to try to describe the complex dependencies between different tasks in a project.</description>
    </item>
    
    <item>
      <title>Documenting a gRPC API with OpenAPI</title>
      <link>https://bbengfort.github.io/2021/01/grpc-openapi-docs/</link>
      <pubDate>Thu, 21 Jan 2021 17:45:35 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2021/01/grpc-openapi-docs/</guid>
      <description>gRPC makes the specification and implementation of networked APIs a snap. But what is the simplest way to document a gRPC API? There seem to be some hosted providers by Google, e.g. SmartDocs, but I have yet to find a gRPC-specific tool. For REST API frameworks, documentation is commonly generated along with live examples using OpenAPI (formerly swagger). By using grpc-gateway it appears to be pretty straight forward to generate a REST/gRPC API combo from protocol buffers and then hook into the OpenAPI specification.</description>
    </item>
    
    <item>
      <title>Self Signed CA</title>
      <link>https://bbengfort.github.io/2020/12/self-signed-ca/</link>
      <pubDate>Wed, 30 Dec 2020 15:51:06 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2020/12/self-signed-ca/</guid>
      <description>I went on a brief adventure looking into creating a lightweight certificate authority (CA) in Go to issue certificates for mTLS connections between peers in a network. The CA was a simple command line program and the idea was that the certificate would initialize its own self-generated certs whose public key would be included in the code base of the peer-to-peer servers, then it could generate TLS x.509 key pairs signed by the CA.</description>
    </item>
    
    <item>
      <title>OS X Cleanup</title>
      <link>https://bbengfort.github.io/2020/11/mac-cleanup/</link>
      <pubDate>Tue, 24 Nov 2020 14:26:25 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2020/11/mac-cleanup/</guid>
      <description>Developer computers often get a lot of cruft built up in non-standard places because of compiled binaries, assets, packages, and other tools that we install over time then forget about as we move onto other projects. In general, I like to reinstall my OS and wipe my disk every year or so to prevent crud from accumulating. As an interemediate step, this post compiles several maintenance caommands that I run fairly routinely.</description>
    </item>
    
    <item>
      <title>Managing Multi-Errors in Go</title>
      <link>https://bbengfort.github.io/2020/10/go-multiple-errors/</link>
      <pubDate>Thu, 22 Oct 2020 11:45:41 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2020/10/go-multiple-errors/</guid>
      <description>This post is a response to Go: Multiple Errors Management. I&amp;rsquo;ve dealt with a multiple error contexts in a few places in my Go code but never created a subpackage for it in github.com/bbengfort/x and so I thought this post was a good motivation to explore it in slightly more detail. I&amp;rsquo;d also like to make error contexts for routine cancellation a part of my standard programming practice, so this post also investigates multiple error handling in a single routine or multiple routines like the original post.</description>
    </item>
    
    <item>
      <title>Writing JSON into a Zip file with Python</title>
      <link>https://bbengfort.github.io/2020/08/zipfiles-json/</link>
      <pubDate>Thu, 20 Aug 2020 11:41:14 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2020/08/zipfiles-json/</guid>
      <description>For scientific reproducibility, it has become common for me to output experimental results as zip files that contain both configurations and inputs as well as one or more output results files. This is similar to .epub or .docx formats which are just specialized zip files - and allows me to easily rerun experiments for comparison purposes. Recently I tried to dump some json data into a zip file using Python 3.</description>
    </item>
    
    <item>
      <title>Read mprofile Output into Pandas</title>
      <link>https://bbengfort.github.io/2020/07/read-mprofile-into-pandas/</link>
      <pubDate>Mon, 27 Jul 2020 18:16:50 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2020/07/read-mprofile-into-pandas/</guid>
      <description>When benchmarking Python programs, it is very common for me to use memory_profiler from the command line - e.g. mprof run python myscript.py. This creates a .dat file in the current working directory which you can view with mprof show. More often than not, though I want to compare two different runs for their memory profiles or do things like annotate the graphs with different timing benchmarks. This requires generating my own figures, which requires loading the memory profiler data myself.</description>
    </item>
    
    <item>
      <title>Basic Python Profiling</title>
      <link>https://bbengfort.github.io/2020/07/basic-python-profiling/</link>
      <pubDate>Tue, 14 Jul 2020 18:01:08 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2020/07/basic-python-profiling/</guid>
      <description>I&amp;rsquo;m getting started on some projects that will make use of extensive Python performance profiling, unfortunately Python doesn&amp;rsquo;t focus on performance and so doesn&amp;rsquo;t have benchmark tools like I might find in Go. I&amp;rsquo;ve noticed that the two most important usages I&amp;rsquo;m looking at when profiling are speed and memory usage. For the latter, I simply use memory_profiler from the command line - which is pretty straight forward. However for speed usage, I did find a snippet that I thought would be useful to include and update depending on how my usage changes.</description>
    </item>
    
    <item>
      <title>Mount an EBS volume</title>
      <link>https://bbengfort.github.io/2019/02/mount-ebs-volume/</link>
      <pubDate>Tue, 05 Feb 2019 12:48:18 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2019/02/mount-ebs-volume/</guid>
      <description>Once the EBS volume has been created and attached to the instance, ssh into the instance and list the available disks:
$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 86.9M 1 loop /snap/core/4917 loop1 7:1 0 12.6M 1 loop /snap/amazon-ssm-agent/295 loop2 7:2 0 91M 1 loop /snap/core/6350 loop3 7:3 0 18M 1 loop /snap/amazon-ssm-agent/930 nvme0n1 259:0 0 300G 0 disk nvme1n1 259:1 0 8G 0 disk └─nvme1n1p1 259:2 0 8G 0 part / In the above case we want to attach nvme0n1 - a 300GB gp2 EBS volume.</description>
    </item>
    
    <item>
      <title>Blast Throughput</title>
      <link>https://bbengfort.github.io/2018/09/blast-throughput/</link>
      <pubDate>Wed, 26 Sep 2018 17:06:24 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2018/09/blast-throughput/</guid>
      <description>Blast throughput is what we call a throughput measurement such that N requests are simultaneously sent to the server and the duration to receive responses for all N requests is recorded. The throughput is computed as N/duration where duration is in seconds. This is the typical and potentially correct way to measure throughput from a client to a server, however issues do arise in distributed systems land:
the requests must all originate from a single client high latency response outliers can skew results you must be confident that N is big enough to max out the server N mustn&amp;rsquo;t be so big as to create non-server related bottlenecks.</description>
    </item>
    
    <item>
      <title>Future Date Script</title>
      <link>https://bbengfort.github.io/2018/09/future-date/</link>
      <pubDate>Wed, 05 Sep 2018 17:42:50 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2018/09/future-date/</guid>
      <description>This is kind of a dumb post, but it&amp;rsquo;s something I&amp;rsquo;m sure I&amp;rsquo;ll look up in the future. I have a lot of emails where I have to send a date that&amp;rsquo;s sometime in the future, e.g. six weeks from the end of a class to specify a deadline … I&amp;rsquo;ve just been opening a Python terminal and importing datetime and timedelta but I figured this quick script on the command line would make my life a bit easier:</description>
    </item>
    
    <item>
      <title>Aggregating Reads from a Go Channel</title>
      <link>https://bbengfort.github.io/2018/08/aggregating-go-channels/</link>
      <pubDate>Sat, 25 Aug 2018 08:28:59 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2018/08/aggregating-go-channels/</guid>
      <description>Here&amp;rsquo;s the scenario: we have a buffered channel that&amp;rsquo;s being read by a single Go routine and is written to by multiple go routines. For simplicity, we&amp;rsquo;ll say that the channel accepts events and that the other routines generate events of specific types, A, B, and C. If there are more of one type of event generator (or some producers are faster than others) we may end up in the situation where there are a series of the same events on the buffered channel.</description>
    </item>
    
    <item>
      <title>The Actor Model</title>
      <link>https://bbengfort.github.io/2018/08/actor-model/</link>
      <pubDate>Fri, 03 Aug 2018 07:27:36 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2018/08/actor-model/</guid>
      <description>Building correct concurrent programs in a distributed system with multiple threads and processes can quickly become very complex to reason about. For performance, we want each thread in a single process to operate as independently as possible; however anytime the shared state of the system is modified synchronization is required. Primitives like mutexes can [ensure structs are thread-safe]({% post_url 2017-02-21-synchronizing-structs %}), however in Go, the strong preference for synchronization is communication.</description>
    </item>
    
    <item>
      <title>Syntax Parsing with CoreNLP and NLTK</title>
      <link>https://bbengfort.github.io/2018/06/corenlp-nltk-parses/</link>
      <pubDate>Fri, 22 Jun 2018 14:38:21 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2018/06/corenlp-nltk-parses/</guid>
      <description>Syntactic parsing is a technique by which segmented, tokenized, and part-of-speech tagged text is assigned a structure that reveals the relationships between tokens governed by syntax rules, e.g. by grammars. Consider the sentence:
The factory employs 12.8 percent of Bradford County.
A syntax parse produces a tree that might help us understand that the subject of the sentence is &amp;ldquo;the factory&amp;rdquo;, the predicate is &amp;ldquo;employs&amp;rdquo;, and the target is &amp;ldquo;12.8 percent&amp;rdquo;, which in turn is modified by &amp;ldquo;Bradford County&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Continuing Outer Loops with for/else</title>
      <link>https://bbengfort.github.io/2018/05/continuing-outer-loops-for-else/</link>
      <pubDate>Thu, 17 May 2018 09:02:43 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2018/05/continuing-outer-loops-for-else/</guid>
      <description>When you have an outer and an inner loop, how do you continue the outer loop from a condition inside the inner loop? Consider the following code:
for i in range(10): for j in range(9): if i &amp;lt;= j: # break out of inner loop # continue outer loop print(i,j) # don&amp;#39;t print unless inner loop completes, # e.g. outer loop is not continued print(&amp;#34;inner complete!&amp;#34;) Here, we want to print for all i ∈ [0,10) all numbers j ∈ [0,9) that are less than or equal to i and we want to print complete once we&amp;rsquo;ve found an entire list of j that meets the criteria.</description>
    </item>
    
    <item>
      <title>Predicted Class Balance</title>
      <link>https://bbengfort.github.io/2018/03/prediction-balance/</link>
      <pubDate>Thu, 08 Mar 2018 09:18:37 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2018/03/prediction-balance/</guid>
      <description>This is a follow on to the [prediction distribution]({{ site.base_url }}{% link _posts/2018-02-28-prediction-distribution.md %}) visualization presented in the last post. This visualization shows a bar chart with the number of predicted and number of actual values for each class, e.g. a class balance chart with predicted balance as well.
This visualization actually came before the prior visualization, but I was more excited about that one because it showed where error was occurring similar to a classification report or confusion matrix.</description>
    </item>
    
    <item>
      <title>Class Balance Prediction Distribution</title>
      <link>https://bbengfort.github.io/2018/02/prediction-distribution/</link>
      <pubDate>Wed, 28 Feb 2018 12:52:11 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2018/02/prediction-distribution/</guid>
      <description>In this quick snippet I present an alternative to the confusion matrix or classification report visualizations in order to judge the efficacy of multi-class classifiers:
The base of the visualization is a class balance chart, the x-axis is the actual (or true class) and the height of the bar chart is the number of instances that match that class in the dataset. The difference here is that each bar is a stacked chart representing the percentage of the predicted class given the actual value.</description>
    </item>
    
    <item>
      <title>Thread and Non-Thread Safe Go Set</title>
      <link>https://bbengfort.github.io/2018/01/go-set/</link>
      <pubDate>Fri, 26 Jan 2018 09:15:13 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2018/01/go-set/</guid>
      <description>I came across this now archived project that implements a set data structure in Go and was intrigued by the implementation of both thread-safe and non-thread-safe implementations of the same data structure. Recently I&amp;rsquo;ve been attempting to get rid of locks in my code in favor of one master data structure that does all of the synchronization, having multiple options for thread safety is useful. Previously I did this by having a lower-case method name (a private method) that was non-thread-safe and an upper-case method name (public) that did implement thread-safety.</description>
    </item>
    
    <item>
      <title>Git-Style File Editing in CLI</title>
      <link>https://bbengfort.github.io/2018/01/cli-editor-app/</link>
      <pubDate>Sat, 06 Jan 2018 09:30:58 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2018/01/cli-editor-app/</guid>
      <description>A recent application I was working on required the management of several configuration and list files that needed to be validated. Rather than have the user find and edit these files directly, I wanted to create an editing workflow similar to crontab -e or git commit — the user would call the application, which would redirect to a text editor like vim, then when editing was complete, the application would take over again.</description>
    </item>
    
    <item>
      <title>Lock Diagnostics in Go</title>
      <link>https://bbengfort.github.io/2017/09/lock-diagnostics/</link>
      <pubDate>Thu, 28 Sep 2017 10:44:30 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/09/lock-diagnostics/</guid>
      <description>By now it&amp;rsquo;s pretty clear that I&amp;rsquo;ve just had a bear of a time with locks and synchronization inside of multi-threaded environments with Go. Probably most gophers would simply tell me that I should share memory by communicating rather than to communication by sharing memory — and frankly I&amp;rsquo;m in that camp too. The issue is that:
Mutexes can be more expressive than channels Channels are fairly heavyweight So to be honest, there are situations where a mutex is a better choice than a channel.</description>
    </item>
    
    <item>
      <title>Lock Queuing in Go</title>
      <link>https://bbengfort.github.io/2017/09/lock-queueing/</link>
      <pubDate>Fri, 08 Sep 2017 11:31:19 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/09/lock-queueing/</guid>
      <description>In Go, you can use sync.Mutex and sync.RWMutex objects to create thread-safe data structures in memory as discussed in [“Synchronizing Structs for Safe Concurrency in Go”]({% post_url 2017-02-21-synchronizing-structs %}). When using the sync.RWMutex in Go, there are two kinds of locks: read locks and write locks. The basic difference is that many read locks can be acquired at the same time, but only one write lock can be acquired at at time.</description>
    </item>
    
    <item>
      <title>Online Distribution</title>
      <link>https://bbengfort.github.io/2017/08/online-distribution/</link>
      <pubDate>Mon, 28 Aug 2017 12:49:46 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/08/online-distribution/</guid>
      <description>This post started out as a discussion of a struct in Go that could keep track of online statistics without keeping an array of values. It ended up being a lesson on over-engineering for concurrency.
The spec of the routine was to build a data structure that could keep track of internal statistics of values over time in a space-saving fashion. The primary interface was a method, Update(sample float64), so that a new sample could be passed to the structure, updating internal parameters.</description>
    </item>
    
    <item>
      <title>Rapid FS Walks with ErrGroup</title>
      <link>https://bbengfort.github.io/2017/08/rapid-fs-walk/</link>
      <pubDate>Fri, 18 Aug 2017 15:33:35 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/08/rapid-fs-walk/</guid>
      <description>I&amp;rsquo;ve been looking for a way to quickly scan a file system and gather information about the files in directories contained within. I had been doing this with multiprocessing in Python, but figured Go could speed up my performance by a lot. What I discovered when I went down this path was the sync.ErrGroup, an extension of the sync.WaitGroup that helps manage the complexity of multiple go routines but also includes error handling!</description>
    </item>
    
    <item>
      <title>Event Dispatcher in Go</title>
      <link>https://bbengfort.github.io/2017/07/event-dispatcher/</link>
      <pubDate>Fri, 21 Jul 2017 06:28:45 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/07/event-dispatcher/</guid>
      <description>The event dispatcher pattern is extremely common in software design, particularly in languages like JavaScript that are primarily used for user interface work. The dispatcher is an object (usually a mixin to other objects) that can register callback functions for particular events. Then when a dispatch method is called with an event, the dispatcher calls each callback function in order of their registration and passes them a copy of the event.</description>
    </item>
    
    <item>
      <title>Lazy Pirate Client</title>
      <link>https://bbengfort.github.io/2017/07/lazy-pirate/</link>
      <pubDate>Fri, 14 Jul 2017 10:24:15 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/07/lazy-pirate/</guid>
      <description>In the [last post]({% post_url 2017-07-13-zmq-basic %}) I discussed a simple REQ/REP pattern for ZMQ. However, by itself REQ/REP is pretty fragile. First, every REQ requires a REP and a server can only handle one request at a time. Moreover, if the server fails in the middle of a reply, then everything is hung. We need more reliable REQ/REP, which is actually the subject of an entire chapter in the ZMQ book.</description>
    </item>
    
    <item>
      <title>Simple ZMQ Message Passing</title>
      <link>https://bbengfort.github.io/2017/07/zmq-basic/</link>
      <pubDate>Thu, 13 Jul 2017 11:00:27 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/07/zmq-basic/</guid>
      <description>There are many ways to create RPCs and send messages between nodes in a distributed system. Typically when we think about messaging, we think about a transport layer (TCP, IP) and a protocol layer (HTTP) along with some message serialization. Perhaps best known are RESTful APIs which allow us to GET, POST, PUT, and DELETE JSON data to a server. Other methods include gRPC which uses HTTP and protocol buffers for interprocess communication.</description>
    </item>
    
    <item>
      <title>PID File Management</title>
      <link>https://bbengfort.github.io/2017/07/pid-management/</link>
      <pubDate>Tue, 11 Jul 2017 09:10:44 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/07/pid-management/</guid>
      <description>In this discussion, I want to propose some code to perform PID file management in a Go program. When a program is backgrounded or daemonized we need some way to communicate with it in order to stop it. All active processes are assigned a unique process id by the operating system and that ID can be used to send signals to the program. Therefore a PID file:
The pid files contains the process id (a number) of a given program.</description>
    </item>
    
    <item>
      <title>Public IP Address Discovery</title>
      <link>https://bbengfort.github.io/2017/07/public-ip/</link>
      <pubDate>Sun, 09 Jul 2017 13:14:46 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/07/public-ip/</guid>
      <description>When doing research on peer-to-peer networks, addressing can become pretty complex pretty quickly. Not everyone has the resources to allocate static, public facing IP addresses to machines. A machine that is in a home network for example only has a single public-facing IP address, usually assigned to the router. The router then performs NAT (network address translation) forwarding requests to internal devices.
In order to get a service running on an internal network, you can port forward external requests to a specific port to a specific device.</description>
    </item>
    
    <item>
      <title>Concurrent Subprocesses and Fabric</title>
      <link>https://bbengfort.github.io/2017/06/concurrent-subprocesses-fabric/</link>
      <pubDate>Wed, 14 Jun 2017 15:56:24 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/06/concurrent-subprocesses-fabric/</guid>
      <description>I&amp;rsquo;ve ben using Fabric to concurrently start multiple processes on several machines. These processes have to run at the same time (since they are experimental processes and are interacting with each other) and shut down at more or less the same time so that I can collect results and immediately execute the next sample in the experiment. However, I was having a some difficulties directly using Fabric:
Fabric can parallelize one task across multiple hosts accordint to roles.</description>
    </item>
    
    <item>
      <title>Appending Results to a File</title>
      <link>https://bbengfort.github.io/2017/06/append-json-results/</link>
      <pubDate>Mon, 12 Jun 2017 16:04:24 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/06/append-json-results/</guid>
      <description>In my current experimental setup, each process is a single instance of sample, from start to finish. This means that I need to aggregate results across multiple process runs that are running concurrently. Moreover, I may need to aggregate those results between machines.
The most compact format to store results in is CSV. This was my first approach and it had some benefits including:
small file sizes readability CSV files can just be concatenated together The problems were:</description>
    </item>
    
    <item>
      <title>Decorating Nose Tests</title>
      <link>https://bbengfort.github.io/2017/05/test-decorators/</link>
      <pubDate>Mon, 22 May 2017 13:05:08 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/05/test-decorators/</guid>
      <description>Was introduced to an interesting problem today when decorating tests that need to be discovered by the nose runner. By default, nose explores a directory looking for things named test or tests and then executes those functions, classes, modules, etc. as tests. A standard test suite for me looks something like:
import unittest class MyTests(unittest.TestCase): def test_undecorated(self): &amp;#34;&amp;#34;&amp;#34; assert undecorated works &amp;#34;&amp;#34;&amp;#34; self.assertEqual(2+2, 4) The problem came up when we wanted to decorate a test with some extra functionality, for example loading a fixture:</description>
    </item>
    
    <item>
      <title>In Process Cacheing</title>
      <link>https://bbengfort.github.io/2017/05/in-process-caches/</link>
      <pubDate>Wed, 17 May 2017 08:16:34 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/05/in-process-caches/</guid>
      <description>I have had some recent discussions regarding cacheing to improve application performance that I wanted to share. Most of the time those conversations go something like this: “have you heard of Redis?” I&amp;rsquo;m fascinated by the fact that an independent, distributed key-value store has won the market to this degree. However, as I&amp;rsquo;ve pointed out in these conversations, cacheing is a hierarchy (heck, even the processor has varying levels of cacheing).</description>
    </item>
    
    <item>
      <title>OAuth Tokens on the Command Line</title>
      <link>https://bbengfort.github.io/2017/04/oauth-token-command-line/</link>
      <pubDate>Thu, 20 Apr 2017 10:26:32 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/04/oauth-token-command-line/</guid>
      <description>This week I discovered I had a problem with my Google Calendar — events accidentally got duplicated or deleted and I needed a way to verify that my primary calendar was correct. Rather than painstakingly go through the web interface and spot check every event, I instead wrote a Go console program using the Google Calendar API to retrieve events and save them in a CSV so I could inspect them all at once.</description>
    </item>
    
    <item>
      <title>Gmail Notifications with Python</title>
      <link>https://bbengfort.github.io/2017/04/gmail-notifications-python/</link>
      <pubDate>Mon, 17 Apr 2017 12:26:55 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/04/gmail-notifications-python/</guid>
      <description>I routinely have long-running scripts (e.g. for a data processing task) that I want to know when they&amp;rsquo;re complete. It seems like it should be simple for me to add in a little snippet of code that will send an email using Gmail to notify me, right? Unfortunately, it isn&amp;rsquo;t quite that simple for a lot of reasons, including security, attachment handling, configuration, etc. In this snippet, I&amp;rsquo;ve attached my constant copy and paste notify() function, written into a command line script for easy sending on the command line.</description>
    </item>
    
    <item>
      <title>Sanely gRPC Dial a Remote</title>
      <link>https://bbengfort.github.io/2017/03/sanely-grpc-dial-a-remote/</link>
      <pubDate>Tue, 21 Mar 2017 16:27:39 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/03/sanely-grpc-dial-a-remote/</guid>
      <description>In my systems I need to handle failure; so unlike in a typical client-server relationship, I&amp;rsquo;m prepared for the remote I&amp;rsquo;m dialing to not be available. Unfortunately when you do this with gRPC-Go there are a couple of annoyances you have to address. They are (in order of solutions):
Verbose connection logging Background and back-off for reconnection attempts Errors are not returned on demand. There is no ability to keep track of statistics So first the logging.</description>
    </item>
    
    <item>
      <title>Pseudo Merkle Tree</title>
      <link>https://bbengfort.github.io/2017/03/pseudo-merkle-tree/</link>
      <pubDate>Thu, 16 Mar 2017 12:23:21 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/03/pseudo-merkle-tree/</guid>
      <description>A Merkle tree is a data structure in which every non-leaf node is labeled with the hash of its child nodes. This makes them particular useful for comparing large data structures quickly and efficiently. Given trees a and b, if the root hash of either is different, it means that part of the tree below is different (if they are identical, they are probably also identical). You can then proceed in a a breadth first fashion, pruning nodes with identical hashes to directly identify the differences.</description>
    </item>
    
    <item>
      <title>Using Select in Go</title>
      <link>https://bbengfort.github.io/2017/03/channel-select/</link>
      <pubDate>Wed, 08 Mar 2017 10:52:39 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/03/channel-select/</guid>
      <description>Ask a Go programmer what makes Go special and they will immediately say “concurrency is baked into the language”. Go&amp;rsquo;s concurrency model is one of communication (as opposed to locks) and so concurrency primitives are implemented using channels. In order to synchronize across multiple channels, go provides the select statement.
A common pattern for me has become to use a select to manage broadcasted work (either in a publisher/subscriber model or a fanout model) by initializing go routines and passing them directional channels for synchronization and communication.</description>
    </item>
    
    <item>
      <title>Synchronizing Structs for Safe Concurrency in Go</title>
      <link>https://bbengfort.github.io/2017/02/synchronizing-structs/</link>
      <pubDate>Tue, 21 Feb 2017 10:48:24 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/02/synchronizing-structs/</guid>
      <description>Go is built for concurrency by providing language features that allow developers to embed complex concurrency patterns into their applications. These language features can be intuitive and a lot of safety is built in (for example a race detector) but developers still need to be aware of the interactions between various threads in their programs.
In any shared memory system the biggest concern is synchronization: ensuring that separate go routines operate in the correct order and that no race conditions occur.</description>
    </item>
    
    <item>
      <title>Extracting a TOC from Markup</title>
      <link>https://bbengfort.github.io/2017/02/extract-toc/</link>
      <pubDate>Sun, 05 Feb 2017 09:11:27 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/02/extract-toc/</guid>
      <description>In today&amp;rsquo;s addition of “really simple things that come in handy all the time” I present a simple script to extract the table of contents from markdown or asciidoc files:
So this is pretty simple, just use regular expressions to look for lines that start with one or more &amp;quot;#&amp;quot; or &amp;quot;=&amp;quot; (for markdown and asciidoc, respectively) and print them out with an indent according to their depth (e.g. indent ## heading 2 one block).</description>
    </item>
    
    <item>
      <title>Error Descriptions for System Calls</title>
      <link>https://bbengfort.github.io/2017/01/syscall-errno/</link>
      <pubDate>Mon, 23 Jan 2017 14:29:50 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/01/syscall-errno/</guid>
      <description>Working with FUSE to build file systems means inevitably you have to deal with (or return) system call errors. The Go FUSE implementation includes helpers and constants for returning these errors, but simply wraps them around the syscall error numbers. I needed descriptions to better understand what was doing what. Pete saved the day by pointing me towards the errno.h header file on my Macbook. Some Python later and we had the descriptions:</description>
    </item>
    
    <item>
      <title>Run Until Error with Go Channels</title>
      <link>https://bbengfort.github.io/2017/01/run-until-err/</link>
      <pubDate>Thu, 19 Jan 2017 11:00:40 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/01/run-until-err/</guid>
      <description>Writing systems means the heavy use of go routines to support concurrent operations. My current architecture employs several go routines to run a server for a simple web interface as well as command line app, file system servers, replica servers, consensus coordination, etc. Using multiple go routines (threads) instead of processes allows for easier development and shared resources, such as a database that can support transactions. However, management of all these threads can be tricky.</description>
    </item>
    
    <item>
      <title>Generic JSON Serialization with Go</title>
      <link>https://bbengfort.github.io/2017/01/generic-json-serialization-go/</link>
      <pubDate>Wed, 18 Jan 2017 11:31:06 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2017/01/generic-json-serialization-go/</guid>
      <description>This post is just a reminder as I work through handling JSON data with Go. Go provides first class JSON support through its standard library json package. The interface is simple, primarily through json.Marshal and json.Unmarshal functions which are analagous to typed versions of json.load and json.dump. Type safety is the trick, however, and generally speaking you define a struct to serialize and deserialize as follows:
type Person struct { Name string `json:&amp;#34;name,omitempty&amp;#34;` Age int `json:&amp;#34;age,omitempty&amp;#34;` Salary int `json:&amp;#34;-&amp;#34;` } op := &amp;amp;Person{&amp;#34;John Doe&amp;#34;, 42} data, _ := json.</description>
    </item>
    
    <item>
      <title>Yielding Functions for Iteration in Go</title>
      <link>https://bbengfort.github.io/2016/12/yielding-functions-for-iteration-golang/</link>
      <pubDate>Thu, 22 Dec 2016 06:54:26 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/12/yielding-functions-for-iteration-golang/</guid>
      <description>It is very common for me to design code that expects functions to return an iterable context, particularly because I have been developing in Python with the yield statement. The yield statement allows functions to “return” the execution context to the caller while still maintaining state such that the caller can return state to the function and continue to iterate. It does this by actually returning a generator, iterable object constructed from the local state of the closure.</description>
    </item>
    
    <item>
      <title>Modifying an Image&#39;s Aspect Ratio</title>
      <link>https://bbengfort.github.io/2016/09/image-aspect-ratio/</link>
      <pubDate>Tue, 13 Sep 2016 14:19:14 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/09/image-aspect-ratio/</guid>
      <description>When making slides, I generally like to use Flickr to search for images that are licensed via Creative Commons to use as backgrounds. My slide deck tools of choice are either Reveal.js or Google Slides. Both tools allow you to specify an image as a background for the slide, but for Google Slides in particular, if the aspect ratio of the image doesn&amp;rsquo;t match the aspect ratio of the slide deck, then weird things can happen.</description>
    </item>
    
    <item>
      <title>Serializing GraphML</title>
      <link>https://bbengfort.github.io/2016/09/serialize-graphml/</link>
      <pubDate>Fri, 09 Sep 2016 17:13:13 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/09/serialize-graphml/</guid>
      <description>This is mostly a post of annoyance. I&amp;rsquo;ve been working with graphs in Python via NetworkX and trying to serialize them to GraphML for use in Gephi and graph-tool. Unfortunately the following error is really starting to get on my nerves:
networkx.exception.NetworkXError: GraphML writer does not support &amp;lt;class &amp;#39;datetime.datetime&amp;#39;&amp;gt; as data values. Also it doesn&amp;rsquo;t support &amp;lt;type NoneType&amp;gt; or list or dict or &amp;hellip;
So I have to do something about it:</description>
    </item>
    
    <item>
      <title>Parallel Enqueue and Workers</title>
      <link>https://bbengfort.github.io/2016/09/parallel-enqueue-and-work/</link>
      <pubDate>Wed, 07 Sep 2016 14:29:51 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/09/parallel-enqueue-and-work/</guid>
      <description>I was recently asked about the parallelization of both the enqueuing of tasks and their processing. This is a tricky subject because there are a lot of factors that come into play. For example do you have two parallel phases, e.g. a map and a reduce phase that need to be synchronized, or is there some sort of data parallelism that requires multiple tasks to be applied to the data (e.</description>
    </item>
    
    <item>
      <title>Parallel NLP Preprocessing</title>
      <link>https://bbengfort.github.io/2016/08/parallel-nlp-preprocessing/</link>
      <pubDate>Fri, 12 Aug 2016 22:09:25 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/08/parallel-nlp-preprocessing/</guid>
      <description>A common source of natural language corpora comes from the web, usually in the form of HTML documents. However, in order to actually build models on the natural language, the structured HTML needs to be transformed into units of discourse that can then be used for learning. In particular, we need to strip away extraneous material such as navigation or advertisements, targeting exactly the content we&amp;rsquo;re looking for. Once done, we need to split paragraphs into sentences, sentences into tokens, and assign part-of-speech tags to each token.</description>
    </item>
    
    <item>
      <title>Pretty Print Directories</title>
      <link>https://bbengfort.github.io/2016/08/pretty-print-directories/</link>
      <pubDate>Mon, 01 Aug 2016 12:17:47 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/08/pretty-print-directories/</guid>
      <description>It feels like there are many questions like this one on Stack Overflow: Representing Directory &amp;amp; File Structure in Markdown Syntax, basically asking &amp;ldquo;how can we represent a directory structure in text in a pleasant way?&amp;rdquo; I too use these types of text representations in slides, blog posts, books, etc. It would be very helpful if I had an automatic way of doing this so I didn&amp;rsquo;t have to create it from scratch.</description>
    </item>
    
    <item>
      <title>Color Map Utility</title>
      <link>https://bbengfort.github.io/2016/07/color-mapper/</link>
      <pubDate>Fri, 15 Jul 2016 16:28:11 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/07/color-mapper/</guid>
      <description>Many of us are spoiled by the use of matplotlib&amp;rsquo;s colormaps which allow you to specify a string or object name of a color map (e.g. Blues) then simply pass in a range of nearly continuous values which are spread along the color map. However, using these color maps for categorical or discrete values (like the colors of nodes) can pose challenges as the colors may not be distinct enough for the representation you&amp;rsquo;re looking for.</description>
    </item>
    
    <item>
      <title>Visualizing Normal Distributions</title>
      <link>https://bbengfort.github.io/2016/06/normal-distribution-viz/</link>
      <pubDate>Mon, 27 Jun 2016 08:28:07 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/06/normal-distribution-viz/</guid>
      <description>Normal distributions are the backbone of random number generation for simulation. By selecting a mean (μ) and standard deviation (σ) you can generate simulated data representative of the types of models you&amp;rsquo;re trying to build (and certainly better than simple uniform random number generators). However, you might already be able to tell that selecting μ and σ is a little backward! Typically these metrics are computed from data, not used to describe data.</description>
    </item>
    
    <item>
      <title>Background Work with Goroutines on a Timer</title>
      <link>https://bbengfort.github.io/2016/06/background-work-goroutines-timer/</link>
      <pubDate>Sun, 26 Jun 2016 06:52:38 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/06/background-work-goroutines-timer/</guid>
      <description>As I&amp;rsquo;m moving deeper into my PhD, I&amp;rsquo;m getting into more Go programming for the systems that I&amp;rsquo;m building. One thing that I&amp;rsquo;m constantly doing is trying to create a background process that runs forever, and does some work at an interval. Concurrency in Go is native and therefore the use of threads and parallel processing is very simple, syntax-wise. However I am still solving problems that I wanted to make sure I recorded here.</description>
    </item>
    
    <item>
      <title>Converting NetworkX to Graph-Tool</title>
      <link>https://bbengfort.github.io/2016/06/graph-tool-from-networkx/</link>
      <pubDate>Thu, 23 Jun 2016 19:21:58 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/06/graph-tool-from-networkx/</guid>
      <description>This week I discovered graph-tool, a Python library for network analysis and visualization that is implemented in C++ with Boost. As a result, it can quickly and efficiently perform manipulations, statistical analyses of Graphs, and draw them in a visual pleasing style. It&amp;rsquo;s like using Python with the performance of C++, and I was rightly excited:
It&amp;#39;s a bear to get setup, but once you do things get pretty nice.</description>
    </item>
    
    <item>
      <title>Extracting Diffs from Git with Python</title>
      <link>https://bbengfort.github.io/2016/05/git-diff-extract/</link>
      <pubDate>Fri, 06 May 2016 08:43:29 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/05/git-diff-extract/</guid>
      <description>One of the first steps to performing analysis of Git repositories is extracting the changes over time, e.g. the Git log. This seems like it should be a very simple thing to do, as visualizations on GitHub and elsewhere show file change analyses through history on a commit by commit basis. Moreover, by using the GitPython library you have direct access to Git repositories that is scriptable. Unfortunately, things aren&amp;rsquo;t as simple as that, so I present a snippet for extracting change information from a Repository.</description>
    </item>
    
    <item>
      <title>NLTK Corpus Reader for Extracted Corpus</title>
      <link>https://bbengfort.github.io/2016/04/nltk-corpus-reader/</link>
      <pubDate>Mon, 11 Apr 2016 21:03:18 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/04/nltk-corpus-reader/</guid>
      <description>Yesterday I wrote a blog about [extracting a corpus]({% post_url 2016-04-10-extract-ddl-corpus %}) from a directory containing Markdown, such as for a blog that is deployed with Silvrback or Jekyll. In this post, I&amp;rsquo;ll briefly show how to use the built in CorpusReader objects in nltk for streaming the data to the segmentation and tokenization preprocessing functions that are built into NLTK for performing analytics.
The dataset that I&amp;rsquo;ll be working with is the District Data Labs Blog, in particular the state of the blog as of today.</description>
    </item>
    
    <item>
      <title>Extracting the DDL Blog Corpus</title>
      <link>https://bbengfort.github.io/2016/04/extract-ddl-corpus/</link>
      <pubDate>Sun, 10 Apr 2016 06:44:28 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/04/extract-ddl-corpus/</guid>
      <description>We have some simple text analyses coming up and as an example, I thought it might be nice to use the DDL blog corpus as a data set. There are relatively few DDL blogs, but they all are long with a lot of significant text and discourse. It might be interesting to try to do some lightweight analysis on them.
So, how to extract the corpus? The DDL blog is currently hosted on Silvrback which is designed for text-forward, distraction-free blogging.</description>
    </item>
    
    <item>
      <title>Dispatching Types to Handler Methods</title>
      <link>https://bbengfort.github.io/2016/04/dispatching-types-handler-methods/</link>
      <pubDate>Tue, 05 Apr 2016 08:58:32 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/04/dispatching-types-handler-methods/</guid>
      <description>A while I ago, I discussed the [observer pattern]({% post_url 2016-02-16-observer-pattern %}) for dispatching events based on a series of registered callbacks. In this post, I take a look at a similar, but very different methodology for dispatching based on type with pre-assigned handlers. For me, this is actually the more common pattern because the observer pattern is usually implemented as an API to outsider code. On the other hand, this type of dispatcher is usually a programmer&amp;rsquo;s pattern, used for development and decoupling.</description>
    </item>
    
    <item>
      <title>Class Variables</title>
      <link>https://bbengfort.github.io/2016/04/class-variables/</link>
      <pubDate>Mon, 04 Apr 2016 19:52:46 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/04/class-variables/</guid>
      <description>These snippets are just a short reminder of how class variables work in Python. I understand this topic a bit too well, I think; I always remember the gotchas and can&amp;rsquo;t remember which gotcha belongs to which important detail. I generally come up with the right answer then convince myself I&amp;rsquo;m wrong until I write a bit of code and experiment. Hopefully this snippet will shortcut that process.
Consider the following class hierarchy:</description>
    </item>
    
    <item>
      <title>Visualizing Pi with matplotlib</title>
      <link>https://bbengfort.github.io/2016/03/pi-day/</link>
      <pubDate>Mon, 14 Mar 2016 10:56:57 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/03/pi-day/</guid>
      <description>Happy Pi day! As is the tradition at the University of Maryland (and to a certain extent, in my family) we are celebrating March 14 with pie and Pi. A shoutout to @konstantinosx who, during last year&amp;rsquo;s Pi day, requested blueberry pie, which was the strangest pie request I&amp;rsquo;ve received for Pi day. Not that blueberry pie is strange, just that someone would want one so badly for Pi day (he got a mixed berry pie).</description>
    </item>
    
    <item>
      <title>Adding a Git Commit to Header Comments</title>
      <link>https://bbengfort.github.io/2016/03/git-version-id/</link>
      <pubDate>Tue, 08 Mar 2016 13:57:56 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/03/git-version-id/</guid>
      <description>You may have seen the following type of header at the top of my source code:
# main # short description # # Author: Benjamin Bengfort &amp;lt;benjamin@bengfort.com&amp;gt; # Created: Tue Mar 08 14:07:24 2016 -0500 # # Copyright (C) 2016 Bengfort.com # For license information, see LICENSE.txt # # ID: main.py [] benjamin@bengfort.com $ All of this is pretty self explanatory with the exception of the final line. This final line is a throw back to Subversion actually, when you could add a $Id$ tag to your code, and Subversion would automatically populate it with something that looks like:</description>
    </item>
    
    <item>
      <title>Implementing the Observer Pattern with an Event System</title>
      <link>https://bbengfort.github.io/2016/02/observer-pattern/</link>
      <pubDate>Tue, 16 Feb 2016 07:24:04 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/02/observer-pattern/</guid>
      <description>I was looking back through some old code (hoping to find a quick post before I got back to work) when I ran across a project I worked on called Mortar. Mortar was a simple daemon that ran in the background and watched a particular directory. When a file was added or removed from that directory, Mortar would notify other services or perform some other task (e.g. if it was integrated into a library).</description>
    </item>
    
    <item>
      <title>On Interval Calls with Threading</title>
      <link>https://bbengfort.github.io/2016/02/intervals-with-threads/</link>
      <pubDate>Tue, 02 Feb 2016 20:43:07 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/02/intervals-with-threads/</guid>
      <description>Event driven programming can be a wonderful thing, particularly when the execution of your code is dependent on user input. It is for this reason that JavaScript and other user facing languages implement very strong event based semantics. Many times event driven semantics depends on elapsed time (e.g. wait then execute). Python, however, does not provide a native setTimeout or setInterval that will allow you to call a function after a specific amount of time, or to call a function again and again at a specific interval.</description>
    </item>
    
    <item>
      <title>Timeline Visualization with Matplotlib</title>
      <link>https://bbengfort.github.io/2016/01/timeline-visualization/</link>
      <pubDate>Thu, 28 Jan 2016 22:24:47 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/01/timeline-visualization/</guid>
      <description>Several times it&amp;rsquo;s come up that I&amp;rsquo;ve needed to visualize a time sequence for a collection of events across multiple sources. Unlike a normal time series, events don&amp;rsquo;t necessarily have a magnitude, e.g. a stock market series is a graph with a time and a price. Events simply have times, and possibly types.
A one dimensional number line is still interesting in this case, because the frequency or density of events reveal patterns that might not easily be analyzed with non-visual methods.</description>
    </item>
    
    <item>
      <title>Freezing Package Requirements</title>
      <link>https://bbengfort.github.io/2016/01/freezing-requirements/</link>
      <pubDate>Thu, 21 Jan 2016 10:23:06 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/01/freezing-requirements/</guid>
      <description>I have a minor issue with freezing requirements, and so I put together a very complex solution. One that is documented here. Not 100% sure why this week is all about packaging, but there you go.
First up, what is a requirement file? Basically they are a list of items that can be installed with pip using the following command:
$ pip install -r requirements.txt The file therefore mostly serves as a list of arguments to the pip install command.</description>
    </item>
    
    <item>
      <title>Better JSON Encoding</title>
      <link>https://bbengfort.github.io/2016/01/better-json-encoding/</link>
      <pubDate>Tue, 19 Jan 2016 14:26:27 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/01/better-json-encoding/</guid>
      <description>The topic of the day is a simple one: JSON serialization. Here is my question, if you have a data structure like this:
import json import datetime data = { &amp;#34;now&amp;#34;: datetime.datetime.now(), &amp;#34;range&amp;#34;: xrange(42), } Why can&amp;rsquo;t you do something as simple as: print json.dumps(data)? These are simple Python datetypes from the standard library. Granted serializing a datetime might have some complications, but JSON does have a datetime specification. Moreover, a generator is just an iterable, which can be put into memory as a list, which is exactly the kind of thing that JSON likes to serialize.</description>
    </item>
    
    <item>
      <title>Simple SQL Query Wrapper</title>
      <link>https://bbengfort.github.io/2016/01/query-factory/</link>
      <pubDate>Mon, 18 Jan 2016 10:52:00 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/01/query-factory/</guid>
      <description>Programming with databases is a fact of life for any seasoned programmer (read, “worth their salt”). From embedded databases like SQLite and LevelDB to server databases like PostgreSQL, data management is a fundamental part of any significant project. The first thing I should say here is skip the ORM and learn SQL. SQL is such a powerful tool to query and manage a database, and is far more performant thanks to 40 years of research and development.</description>
    </item>
    
    <item>
      <title>The codetime and clock Commands</title>
      <link>https://bbengfort.github.io/2016/01/codetime-and-clock/</link>
      <pubDate>Tue, 12 Jan 2016 17:02:51 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/01/codetime-and-clock/</guid>
      <description>If you&amp;rsquo;ve pair programmed with me, you might have seen me type something to the following effect on my terminal, particularly if I have just created a new file:
$ codetime Then somehow I can magically paste a formatted timestamp into the file! Well it&amp;rsquo;s not a mystery, in fact, it&amp;rsquo;s just a simple alias:
alias codetime=&amp;#34;clock.py code | pbcopy&amp;#34; Oh, well that&amp;rsquo;s easy — why the blog post? Hey, what&amp;rsquo;s clock.</description>
    </item>
    
    <item>
      <title>Wrapping the Logging Module</title>
      <link>https://bbengfort.github.io/2016/01/logging-mixin/</link>
      <pubDate>Mon, 11 Jan 2016 08:15:05 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/01/logging-mixin/</guid>
      <description>The standard library logging module is excellent. It is also quite tedious if you want to use it in a production system. In particular you have to figure out the following:
configuration of the formatters, handlers, and loggers object management throughout the script (e.g. the logging.getLogger function) adding extra context to log messages for more complex formatters handling and logging warnings (and to a lesser extent, exceptions) The logging module actually does all of these things.</description>
    </item>
    
    <item>
      <title>Simple CLI Script with Argparse</title>
      <link>https://bbengfort.github.io/2016/01/simple-cli-argparse/</link>
      <pubDate>Sun, 10 Jan 2016 14:48:14 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/01/simple-cli-argparse/</guid>
      <description>Let&amp;rsquo;s face it, most of the Python programs we write are going to be used from the command line. There are tons of command line interface helper libraries out there. My preferred CLI method is the style of Django&amp;rsquo;s management utility. More on this later, when we hopefully publish a library that gives us that out of the box (we use it in many of our projects already).
Sometimes though, you just want a simple CLI script.</description>
    </item>
    
    <item>
      <title>Basic Python Project Files</title>
      <link>https://bbengfort.github.io/2016/01/project-start/</link>
      <pubDate>Sat, 09 Jan 2016 14:01:59 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2016/01/project-start/</guid>
      <description>I don&amp;rsquo;t use project templates like cookiecutter. I&amp;rsquo;m sure they&amp;rsquo;re fine, but when I start a new project I like to get a cup of coffee, go to my zen place and manually create the workspace. It gets me in the right place to code. Here&amp;rsquo;s the thing, there is a right way to set up a Python project. Plus, I have a particular style for my repositories — particularly how I use Creative Commons Flickr photos as the header for my README files.</description>
    </item>
    
    <item>
      <title>One Big Gift Selection Algorithm</title>
      <link>https://bbengfort.github.io/2015/12/one-big-gift/</link>
      <pubDate>Fri, 25 Dec 2015 11:54:12 +0000</pubDate>
      
      <guid>https://bbengfort.github.io/2015/12/one-big-gift/</guid>
      <description>My family does &amp;ldquo;one big gift&amp;rdquo; every Christmas; that is instead of everyone simply buying everyone else a smaller gift; every person is assigned to one other person to give them a single large gift. Selection of who gives what to who is a place of some (minor) conflict. Therefore we simply use a random algorithm. Unfortunately, apparently a uniform random sample of pairs is not enough, therefore we take 100 samples to vote for each combination to see who gets what as follows:</description>
    </item>
    
  </channel>
</rss>
