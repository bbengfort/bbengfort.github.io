<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Presentations on Libelli</title>
    <link>https://bbengfort.github.io/categories/presentations/</link>
    <description>Recent content in Presentations on Libelli</description>
    <image>
      <title>Libelli</title>
      <url>https://bbengfort.github.io/bear.png</url>
      <link>https://bbengfort.github.io/bear.png</link>
    </image>
    <generator>Hugo -- 0.135.0</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Apr 2025 14:06:23 -0400</lastBuildDate>
    <atom:link href="https://bbengfort.github.io/categories/presentations/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Implementing Function Calling LLMs without Fear</title>
      <link>https://bbengfort.github.io/2025/04/function-calling-without-fear/</link>
      <pubDate>Wed, 16 Apr 2025 14:06:23 -0400</pubDate>
      <guid>https://bbengfort.github.io/2025/04/function-calling-without-fear/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://info.umbctraining.com/effective-ai-systems&#34;&gt;Implementing Function Calling LLMs without Fear&lt;/a&gt; is a talk that I gave at a C4AI/RealmOne Happy Hour Tech Meetup in Columbia, Maryland. The slides of the talk are below:&lt;/p&gt;
&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/278029409?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Description: For an AI system to be an agent rather than a simple chatbot, it needs to be able to do work on behalf of its users, often accomplished through the use of Function Calling LLMs. Instruction-based models can identify external functions to call for additional input or context before creating a final response without the need for any additional training. However, giving an AI system access to databases, APIs, or even tools like our calendars is fraught with security concerns and task validation nightmares. In this talk, we&amp;rsquo;ll discuss the basics of how Function Calling works and think through the best practices and techniques to ensure that your agents work for you, not against you!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Privacy and Security in the Age of Generative AI</title>
      <link>https://bbengfort.github.io/2024/10/privacy-security-generative-ai/</link>
      <pubDate>Wed, 30 Oct 2024 11:35:00 -0700</pubDate>
      <guid>https://bbengfort.github.io/2024/10/privacy-security-generative-ai/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://odsc.com/speakers/privacy-and-security-in-the-age-of-generative-ai/&#34;&gt;Privacy and Security in the Age of Generative AI&lt;/a&gt; is a talk that I gave at ODSC West 2024 in Burlingame, California. The slides of the talk are below:&lt;/p&gt;
&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/272867721?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;p&gt;An updated presentation that I gave at C4AI on April 15, 2025 in Columbia, Maryland is below:&lt;/p&gt;
&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/278029566?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;From sensitive data leakage to prompt injection and zero-click worms, LLMs and generative models are the new cyber battleground for hackers. As more AI models are deployed in production, data scientists and ML engineers can&amp;rsquo;t ignore these problems. The good news is that we can influence privacy and security in the machine learning lifecycle using data specific techniques. In this talk, we&amp;rsquo;ll review some of the newest security concerns affecting LLMs and deep learning models and learn how to embed privacy into model training with ACLs and differential privacy, secure text generation and function-calling interfaces, and even leverage models to defend other models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Smart Global Replication Using Reinforcement Learning</title>
      <link>https://bbengfort.github.io/2023/11/smart-global-replication-using-reinforcement-learning/</link>
      <pubDate>Tue, 07 Nov 2023 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2023/11/smart-global-replication-using-reinforcement-learning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://kccncna2023.sched.com/event/21d4640540a0961019d201de8ec2fd5e&#34;&gt;Smart Global Replication using Reinforcement Learning&lt;/a&gt; is a talk that I gave at KubeCon + CloudNative North America 2023 in Chicago, IL. The video of the talk is below:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/YTF2dXNhFzI?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;There are many great reasons to replicate data across Kubernetes clusters in different geographic regions: e.g. for disaster recovery and to ensure the best possible user experiences. Unfortunately, global replication is not easy; not just because of the difficulty in consistency reasoning that it introduces, but also due to the increased cost of provisioning multiple volumes that exponentially duplicate ingress and egress. Wouldn&amp;rsquo;t it be great if our systems could learn the optimal placement of storage blocks so that total replication was not necessary? Wouldn&amp;rsquo;t it be even better if our replication messaging was reduced ensuring communication only between the minimally necessary set of storage nodes? We show a system that uses multi-armed bandits to perform such an optimization; dynamically adjusting how data is replicated based on usage. We demonstrate the savings achieved and system performance using a real world system: the TRISA Global Travel Rule Compliance Directory.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DIY Consensus: Crafting Your Own Distributed Code (with Benjamin Bengfort)</title>
      <link>https://bbengfort.github.io/2023/08/diy-consensus-crafting-your-own-distributed-code/</link>
      <pubDate>Wed, 30 Aug 2023 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2023/08/diy-consensus-crafting-your-own-distributed-code/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pod.link/developer-voices/episode/938292a4e4b2c1ca82a66d4674dd8d97&#34;&gt;DIY Consensus: Crafting Your Own Distributed Code (with Benjamin Bengfort)&lt;/a&gt;&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/Ij_PBvocf5c?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;How do distributed systems work? If you&amp;rsquo;ve got a database spread over three servers, how do they elect a leader? How does that change when we spread those machines out across data centers, situated around the globe? Do we even need to understand how it works, or can we relegate those problems to an off the shelf tool like Zookeeper? Joining me this week is Distributed Systems Doctor—Benjamin Bengfort—for a deep dive into consensus algorithms. We start off by discussing how much of &amp;ldquo;the clustering problem&amp;rdquo; is your problem, and how much can be handled by a library. We go through many of the constraints and tradeoffs that you need to understand either way. And we eventually reach Benjamin&amp;rsquo;s surprising message - maybe the time is ripe to roll your own. Should we be writing our own bespoke Raft implementations? And if so, how hard would that be? What guidance can he offer us?  Somewhere in the recording of this episode, I decided I want to sit down and try to implement a leader election protocol. Maybe you will too. And if not, you&amp;rsquo;ll at least have a better appreciation for what it takes. Distributed systems used to be rocket science, but they&amp;rsquo;re becoming deployment as usual. This episode should help us all to keep up!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Visual Diagnostics for More Effective Machine Learning</title>
      <link>https://bbengfort.github.io/2019/01/visual-diagnostics-for-more-effective-machine-learning/</link>
      <pubDate>Thu, 10 Jan 2019 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2019/01/visual-diagnostics-for-more-effective-machine-learning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pydata.org/miami2019/schedule/presentation/8/&#34;&gt;Visual Diagnostics for More Effective Machine Learning&lt;/a&gt;&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/2kZ38ysHDzM?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/127690054?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Modeling is often treated as a search activity: find some combination of features, algorithm, and hyperparameters that yields the best score after cross-validation. In this talk, we will explore how to steer the model selection process with visual diagnostics and the Yellowbrick library, leading to more effective and more interpretable results and faster experimental workflows.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding Machine Learning Through Visualizations with Benjamin Bengfort and Rebecca Bilbro - Episode 166</title>
      <link>https://bbengfort.github.io/2018/06/understanding-machine-learning-through-visualizations-with-benjamin-bengfort-and-rebecca-bilbro-episode-166/</link>
      <pubDate>Sun, 17 Jun 2018 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2018/06/understanding-machine-learning-through-visualizations-with-benjamin-bengfort-and-rebecca-bilbro-episode-166/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.pythonpodcast.com/yellowbrick-with-bejnamin-bengfort-and-rebecca-bilbro-episode-166/&#34;&gt;Understanding Machine Learning Through Visualizations with Benjamin Bengfort and Rebecca Bilbro - Episode 166&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Machine learning models are often inscrutable and it can be difficult to know whether you are making progress. To improve feedback and speed up iteration&lt;/p&gt;</description>
    </item>
    <item>
      <title>Visual Pipelines for Text Analysis</title>
      <link>https://bbengfort.github.io/2017/06/visual-pipelines-for-text-analysis/</link>
      <pubDate>Sat, 24 Jun 2017 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2017/06/visual-pipelines-for-text-analysis/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://data-intelligence.ai/presentations/13&#34;&gt;Visual Pipelines for Text Analysis&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Employing machine learning in practice is half search, half expertise, and half blind luck. In this talk we will explore how to make the luck half less blind by using visual pipelines to steer model selection from raw input to operational prediction. We will look specifically at extending transformer pipelines with visualizers for sentiment analysis and topic modeling text corpora.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Product Architectures: O&#39;Reilly Webinar</title>
      <link>https://bbengfort.github.io/2016/12/data-product-architectures-oreilly-webinar/</link>
      <pubDate>Wed, 07 Dec 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/12/data-product-architectures-oreilly-webinar/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.oreilly.com/pub/e/3800&#34;&gt;Data Product Architectures: O&amp;rsquo;Reilly Webinar&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Data products derive their value from data and generate new data in return. As a result, machine-learning techniques must be applied to their architecture and development. Machine learning fits models to make predictions on unknown inputs and must be generalizable and adaptable. As such, fitted models cannot exist in isolation; they must be operationalized and user facing so that applications can benefit from the new data, respond to it, and feed it back into the data product.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dynamics in Graph Analysis Adding Time as a Structure for Visual and Statistical Insight</title>
      <link>https://bbengfort.github.io/2016/10/dynamics-in-graph-analysis-adding-time-as-a-structure-for-visual-and-statistical-insight/</link>
      <pubDate>Mon, 24 Oct 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/10/dynamics-in-graph-analysis-adding-time-as-a-structure-for-visual-and-statistical-insight/</guid>
      <description>&lt;p&gt;I gave this talk twice, both at &lt;a href=&#34;http://pydata.org/dc2016/schedule/presentation/36/&#34;&gt;PyData DC&lt;/a&gt; on October 24, 2016 and at &lt;a href=&#34;http://pydata.org/carolinas2016/schedule/presentation/39/&#34;&gt;PyData Carolinas&lt;/a&gt; on September 15, 2016. Both videos are below if you feel like figuring out which presentation was better!&lt;/p&gt;
&lt;h3 id=&#34;pydata-dc&#34;&gt;PyData DC&lt;/h3&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/QhMZ1PmlJn4?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;pydata-carolinas&#34;&gt;PyData Carolinas&lt;/h3&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/RgixxVpfXDY?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;slides&#34;&gt;Slides&lt;/h3&gt;
&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/66065281?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Network analyses are powerful methods for both visual analytics and machine learning but can suffer as their complexity increases. By embedding time as a structural element rather than a property, we will explore how time series and interactive analysis can be improved on Graph structures. Primarily we will look at decomposition in NLP-extracted concept graphs using NetworkX and Graph Tool.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Interview - Ben Bengfort of District Data Labs</title>
      <link>https://bbengfort.github.io/2016/07/interview-ben-bengfort-of-district-data-labs/</link>
      <pubDate>Thu, 28 Jul 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/07/interview-ben-bengfort-of-district-data-labs/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/ZiY5tjgg7lU?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;We talk to Benjamin Bengfort about his Data Day Seattle talks, District Data Labs, and Ben&amp;rsquo;s popular O&amp;rsquo;Reilly books.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Visualizing the Model Selection Process</title>
      <link>https://bbengfort.github.io/2016/07/visualizing-the-model-selection-process/</link>
      <pubDate>Sat, 23 Jul 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/07/visualizing-the-model-selection-process/</guid>
      <description>&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/64311820?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Machine learning is the hacker art of describing the features of instances that we want to make predictions about, then fitting the data that describes those instances to a model form. Applied machine learning has come a long way from it&amp;rsquo;s beginnings in academia, and with tools like Scikit-Learn, it&amp;rsquo;s easier than ever to generate operational models for a wide variety of applications. Thanks to the ease and variety of the tools in Scikit-Learn, the primary job of the data scientist is &lt;em&gt;model selection&lt;/em&gt;. Model selection involves performing feature engineering, hyperparameter tuning, and algorithm selection. These dimensions of machine learning often lead computer scientists towards automatic model selection via optimization (maximization) of a model&amp;rsquo;s evaluation metric. However, the search space is large, and grid search approaches to machine learning can easily lead to failure and frustration. Human intuition is still essential to machine learning, and visual analysis in concert with automatic methods can allow data scientists to steer model selection towards better fitted models, faster. In this talk, we will discuss interactive visual methods for better understanding, steering, and tuning machine learning models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Product Architectures: Seattle Data Day</title>
      <link>https://bbengfort.github.io/2016/07/data-product-architectures-seattle-data-day/</link>
      <pubDate>Thu, 21 Jul 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/07/data-product-architectures-seattle-data-day/</guid>
      <description>&lt;iframe
  style=&#34;width: 100%; height: 500px;&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34;
  src=&#34;http://www.slideshare.net/slideshow/embed_code/64265501?rel=0&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Data products derive their value from data and generate new data in return; as a result, machine learning techniques must be applied to their architecture and their development. Machine learning fits models to make predictions on unknown inputs and must be &lt;em&gt;generalizable&lt;/em&gt; and &lt;em&gt;adaptable&lt;/em&gt;. As such, fitted models cannot exist in isolation; they must be operationalized and user facing so that applications can benefit from the new data, respond to it, and feed it back in to the data product. Data product architectures are therefore &lt;em&gt;life cycles&lt;/em&gt; and understanding the data product life cycle will enable architects to develop robust, failure free workflows and applications. In this talk we will discuss the data product life cycle, explore how to engage a model build, evaluation, and selection phase with an operation and interaction phase. Following the lambda architecture, we will investigate wrapping a central computational store for speed and querying, as well as incorporating a discussion of monitoring, management, and data exploration for hypothesis driven development. From web applications to big data appliances; this architecture serves as a blueprint for handling data services of all sizes!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Natural Language Processing with NLTK and Gensim</title>
      <link>https://bbengfort.github.io/2016/05/natural-language-processing-with-nltk-and-gensim/</link>
      <pubDate>Mon, 30 May 2016 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2016/05/natural-language-processing-with-nltk-and-gensim/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://us.pycon.org/2016/schedule/presentation/1597/&#34;&gt;Natural Language Processing with NLTK and Gensim&lt;/a&gt;&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/itKNpCPHq3I?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Natural Language Processing (NLP) is often taught at the academic level from the perspective of computational linguists. However, as data scientists, we have a richer view of the natural language world - unstructured data that by its very nature has latent information that is important to humans. NLP practitioners have benefited from machine learning techniques to unlock meaning from large corpora, and in this class we’ll explore how to do that particularly with Python, Gensim, and the Natural Language Toolkit (NLTK).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Natural Language Processing and Hadoop</title>
      <link>https://bbengfort.github.io/2013/11/natural-language-processing-and-hadoop/</link>
      <pubDate>Sat, 02 Nov 2013 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2013/11/natural-language-processing-and-hadoop/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://strataconf.com/stratany2013/public/schedule/detail/30806&#34;&gt;Natural Language Processing and Hadoop&lt;/a&gt;&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/2642kr9-cB0?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Benjamin Bengfort and Sean Murphy discuss how NLP can be integrated with Hadoop to gain insights in big data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unbound Concepts: Columbia TechBreakfast Dec. 2012</title>
      <link>https://bbengfort.github.io/2012/12/unbound-concepts-columbia-techbreakfast/</link>
      <pubDate>Wed, 12 Dec 2012 12:00:00 -0500</pubDate>
      <guid>https://bbengfort.github.io/2012/12/unbound-concepts-columbia-techbreakfast/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/N-Bi_MwvZiY?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;Presentation by the CTO of Unbound Concepts, Benjamin Bengfort, at the Columbia TechBreakfast 2012.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
